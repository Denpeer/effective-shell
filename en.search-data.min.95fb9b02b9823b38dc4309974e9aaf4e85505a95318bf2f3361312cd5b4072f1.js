'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/section6/the-future/','title':"The Future",'content':""});index.add({'id':1,'href':'/docs/getting-started/','title':"Getting Started",'content':"Chapter 1 - Getting Started This section is for those who are completely new to this topic. In this section we'll introduce just what the shell is, who this book is useful for, and what you can expect to learn.\nWe'll also look at how to set your computer up so that you can follow along with the examples. We'll finish by demonstrating a few basic skills so that you can learn to move around in the shell and get started with the rest of the book.\nIf you are already comfortable with running a shell, know what Bash is, and know how to run basic commands like ls and cd, then you can completely skip this section.\nIf you are already comfortable with running a shell, know what bash is, and know how to run basic commands like ls and cd, are familiar with terms like command and parameter then you can skip this section. You could also just review the Summary to make sure that you are comfortable with the material which has been introduced and then move onto the next section.\nWhat is the Shell? If you don't know what the shell is, then this is the place to start!\nWhen we talk about \u0026ldquo;The Shell\u0026rdquo;, we're normally referring to the simple, text-based interface which is used to control a computer or a program.\nHere's what the shell looks like on Windows:\nAnd here's what it looks like on a Mac:\nAnd here's what it looks like on Fedora, a popular Linux distribution:\nWhen we are talking about the shell in this book, we're talking about the simple program which can be used to operate the computer using this text based interface.\nWhy would you want to do this? There are a few reasons!\nFirstly, using the shell can help you learn more about the internals of how your computer can work. This can be really helpful if you are technology professional or work with computers.\nSecondly, there are some scenarios where you have to use a shell. Not every program or system can be operated with a Graphical User Interface, which is the visual point-and-click interface you are probably using now. Some lower-level programs do not have such interfaces, and many computers do not either.\nFinally, there are some scenarios where it can be more efficient to use the shell. Operations which might be time consuming or repetitive to perform using the user interface might be much faster to perform in a shell. You can also write shell scripts to automate these kinds of operations.\nIn the next section you'll learn how to startup the shell on your computer. Once this is done you are ready to continue with the book.\nOpening the Shell Now let's actually learn how to open the shell on your computer.\nOnce we've done this, we might need to make some configuration changes so that we get it to behave in a way which as consistent with other shells as possible - we'll get to that in the next chapter.\nMicrosoft Windows There are a number of shell programs on Microsoft Windows. We'll be using the basic shell which is pre-installed, which is called the \u0026ldquo;Command Prompt\u0026rdquo;.\nTo open the command prompt, start by clicking the start button on the bottom left hand side of the screen, and type command prompt. Open the Command Prompt program:\nOnce the program has opened, type whoami then hit the Return key. The whoami program will show the username of the logged in user:\nThat's it! We've still got some configuration to do to make this shell behave more like a Linux shell, which this book uses as the standard, but we'll come to that in the next section.\nMacOS If you are using a Mac, then you just need to run the \u0026ldquo;Terminal\u0026rdquo; program to open your shell. Hold down the Command Key and press Space, then type terminal. Open the terminal program which is shown:\nOnce the program has opened, type whoami then hit the Return key. The whoami program will show the username of the logged in user:\nThat's it! In the next section we'll make some minor configuration changes to keep things consistent with the samples in the book.\nLinux / Unix If you are using a Linux or Unix system, I'll assume that you are familiar enough with it to open a shell. Which terminal you use should not affect how you use this book, but for consistencies sake be aware that most of the examples are assuming that the user is using Bash version 5.\nConfiguring the Shell Shells can vary enormously between different systems. In general, Linux systems tend to use the \u0026ldquo;Bash\u0026rdquo; shell and require little configuration. Apple's MacOS operating system is actually based on BSD Unix, and under the hood is somewhat different to most Linux systems. Microsoft Windows is a completely unrelated operating system to either Linux or Unix and operates in a fundamentally different way both of them.\nIn this book, we assume that you are using a \u0026ldquo;Linux-like\u0026rdquo; system, something which operates like a modern Linux distribution. This is a deliberate choice. If you become comfortable using a Linux-like shell, you can generally apply the techniques we'll show to MacOS with no difficulties. For Windows, the techniques are not necessarily transferable immediately, but still valuable to know. Windows is actually being updated at the time of writing to provide a Linux-like shell interface as part of the core operating system (this is known as the Windows Subsystem Linux. As time progresses it will be easier to run commands using the techniques in this book natively, but for now we'll have to tweak a few things.\nIn this section we'll make sure that we are running with a setup which is close to Linux, and aim to set the latest version of our shell to the popular \u0026ldquo;Bash\u0026rdquo; program. If you are familiar with Bash but prefer to use another shell, that is fine, most of the book will work with any modern shell. However, if you are not sure what shell you should be using, I would recommend you follow the guides below to setup the most popular shell at its latest version.\nOnce this is done then we are ready to get into the book properly!\nMicrosoft Windows Windows is not anything like Linux under the hood. So to get a shell working, we have three options:\n Use a tool which provides common Linux tools which have been written to work with Windows Use a \u0026ldquo;virtual machine\u0026rdquo; running Linux Use the Windows Subsystem for Linux  The first option is the best if you want to actually be able to work with the files on your computer quickly and easily day to day.\nThe second option is best if you want to be able to experiment with the shell, but keep it completely separate from your main computer and its files.\nThe final option is best if you are a power user or expert who wants to use the latest WSL features and build the skills with the platform as soon as possible.\nWe'll go through all options here.\nOption 1: Install Linux Tools This is probably the easiest option and the one I would recommend for most user. It will let you run something like a Linux shell when you choose to, but not get in your way during day-to-day usage of your computer.\nTo get a Linux-like experience on a Windows machine, we'll install Cygwin. Cygwin provides a large set of programs which are generally available on Linux systems, which are designed to work on Windows.\nDownload the Cygwin installer and start the installation process. You should see something like this:\nStart the installation and tell it to install from the internet (the default option):\nInstall for all users in the default location. It is also fine to change the options if you prefer:\nCygwin will ask you where to install downloaded packages, whether a proxy is needed, and what download sites to use. Leave these options at their default unless you know what you are doing and why you'd need to change them. It will then start downloading. Once it has downloaded the list of available packages to install, it will ask which packages you want. Choose the default option \u0026ldquo;All\u0026rdquo;:\nThe installer will now start downloading and installing the programs:\nOnce Cygwin has finished installing, you will have a link to open Cygwin available on the desktop and start menu.\nYou can use this link to start using the \u0026ldquo;Bash\u0026rdquo; shell, or if you prefer you can open the \u0026ldquo;Command Prompt\u0026rdquo; as described in Opening the Shell and run the bash program:\nNote that you shouldn't use the --norc option. I have used it in the screenshot above just so that my Bash looks like it would after a clean install, without my own customisations added.\nAt this point you have a ready-to-go bash environment and can continue on to the Summary and Next Section.\nOption 2: Use a Virtual Machine We can run a virtual machine on Windows which will give us a complete Linux environment. This is an ideal way to create a safe sandbox for experimentation, without changing how the rest of the system is setup.\nThere are many ways to run a virtual machine on Windows. For this example we'll use the free \u0026lsquo;Oracle VirtualBox\u0026rsquo; tool. VirtualBox will run a virtual machine, and on that virtual machine we will install the popular Ubuntu distribution of Linux.\nFirst, start downloading Ubuntu, which might take some time as the download is quite large. You will want to install the latest Desktop Edition (which at the time of writing is version 18):\nWhile the Ubuntu software downloads, we can install VirtualBox. Go to the VirtualBox Website and download the VirtualBox installer. You will need the installer for \u0026lsquo;Windows Hosts\u0026rsquo;.\nOnce the installer has downloaded, run it to start the installation:\nNext you will be asked to configure the installation options. The defaults will be fine for most users:\nThen the installation will start:\nOnce the installation is complete and the Ubuntu installer has downloaded we can move onto the next step.\nOpen VirtualBox and choose \u0026lsquo;New\u0026rsquo; to create a new Virtual Machine. Ensure \u0026ldquo;Expert Mode\u0026rdquo; is selected. Provide a name for the machine and choose \u0026ldquo;Linux\u0026rdquo; as the type and \u0026ldquo;Ubuntu_64\u0026rdquo; as the version type. Everything else can be left as the default, unless you want to tweak the machine settings:\nYou will be asked to setup a virtual hard disk. I would recommend the default options for most users:\nOnce the machine has been created it will be shown in the main VirtualBox window. Select the machine and choose \u0026ldquo;Start\u0026rdquo;:\nWhen the machine starts up it will ask you for a \u0026ldquo;Startup Disk\u0026rdquo;. This is the disk which will be used to setup the operating system. Press the \u0026ldquo;browse\u0026rdquo; icon, then choose \u0026ldquo;open\u0026rdquo; and select the Ubuntu file which you downloaded, which should end in .iso:\nIf this step fails, you may need to disable \u0026ldquo;Hyper-V\u0026rdquo; and \u0026ldquo;Windows Sandbox\u0026rdquo; by going to \u0026ldquo;Add or Remove Windows features\u0026rdquo;:\nAfter a short while you will see the Ubuntu installer start up. Choose the \u0026ldquo;Install Ubuntu\u0026rdquo; option:\nYou can specify language settings, what components are installed and more. These options can be left at the default. On the final page, choose the \u0026ldquo;Erase disk and install Ubuntu\u0026rdquo; option:\nThe final step will be to choose a name for the computer, and a username and password to log in with. You can use any values you like here, just don't forget them!\nAfter this the installation will proceed. It might take a little while. After the installation is complete, you will need to restart. If you get an error saying \u0026ldquo;Please remove installation medium\u0026rdquo; just power off the machine and restart it. After restarting you can log into the machine with the credentials you specified earlier.\nWhen you have logged in, press the applications icon on the bottom-left of the screen and search for the \u0026ldquo;Terminal\u0026rdquo; application:\nYou are now running the \u0026ldquo;Bash\u0026rdquo; shell in the terminal. You can run the whoami command to show the current user, or bash --version to see the version of Bash which is installed:\nThat's it! You now have a virtual machine running Ubuntu and Bash which you can use to learn about the shell.\nOption 3: Setup the Windows Subsystem for Linux The Windows Subsystem for Linux is a relatively new set of features for Microsoft Windows. It allows users to install a Linux distribution on their Windows machine. This is a great way for us to be able to use the \u0026ldquo;Bash\u0026rdquo; shell without having to set up a virtual machine.\nFirst, open up the \u0026ldquo;Turn Windows Features on or off\u0026rdquo; option from the control panel:\nThen enable the \u0026ldquo;Windows Subsystem for Linux\u0026rdquo; feature:\nAfter your computer has restarted, open up the Windows App Store and search for \u0026ldquo;Ubuntu\u0026rdquo;:\nOnce Ubuntu has installed, open up the app. It will then initialise (which can take a little while):\nChoose a username and password to complete the setup:\nAnd that's it! You can now open the Ubuntu app at any time to use Ubuntu on Windows, interfacing using the Bash shell.\nMacOS If you are running a Mac, then you can probably run the standard Terminal program and follow the material in this book without making any changes. However, the version of Bash which comes installed by default on MacOS is version 3, which is a little out of date. I would strongly suggest that you upgrade the default installation. On MacOS Catalina, the default shell has changed to Z Shell - this should work fine for all of the examples in this book, but you might want to switch it to Bash to be on the safe side (you can always change back later).\nTo install the right software, we'll use a tool called Homebrew. Homebrew is a \u0026lsquo;package manager\u0026rsquo;, a tool used to install software on your computer, from the shell. It's kind of like the App Store but for shell users!\nFirst, follow the instructions online to install Homebrew:\nIn most cases, this will require opening the terminal programming and running a snippet which looks like this:\n/usr/bin/ruby -e \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\u0026#34; However, this might have changed since the time of writing so do check the website to see what the latest instructions are. You don't actually need to know what is going on with this command (but by the time you've worked through a bit more of this book it will make sense!), but in a nutshell it runs a basic installation script, using the Ruby programming language (which comes pre-installed on MacOS).\nOnce this has installed, install Bash by running the following command in the shell:\nbrew install bash This uses the brew command, which we have just installed, to install the bash program.\nFinally, update the Terminal preferences to use the version of Bash you have just installed, rather than the default, by setting the shell location to /usr/local/bin/bash:\nAgain, why we make these changes is not essential to know for now, we'll go into more details in a later section. Once you've made this change, whenever you open a new terminal window, it will run the latest version of Bash, which you can confirm by running echo $BASH_VERSION:\nThere is actually a more sophisticated way to change what shell is used in a system, which is the special chsh command (short for \u0026ldquo;change shell\u0026rdquo;). We'll see this in a later section. We'll also see what echo is in more detail shortly.\nLinux As before, if you are running Linux I will assume you are able to open a terminal and setup the appropriate shell. You can follow along with the content in this book with any recent Bash-like shell.\nThat's It! Later on we'll see a little more about the differences between different shell programs, what the difference between a shell and a terminal is and more. But for now, you are ready to go and move onto the Summary and then Section 1.\nA Quick Demo of the Shell If you have never used the shell before, then this is where we'll start. We're not going to go into lots of detail, there's plenty of that later on in book. Instead we'll do a quick crash course on the basics. If you have not used the shell before this'll give you a chance to see how it works.\nStart by opening your shell. This is covered in Opening the Shell. Your shell should be Bash - if this doesn't sound familiar, then make sure you have followed the instructions in Configuring the Shell.\nYou should see your terminal program running your shell. You can see what the version is of your shell by running:\nbash --version Let's quickly dissect this. We have run the bash command. A command can be a program on your computer, or it can be something built into the shell. We'll look at this in a lot more detail later, but for now it's important to understand that a lot of what you will be doing is running commands.\nThe --version text is a parameter. Parameters affect how commands work. This is actually easier to see with an example.\nLet's move to the home folder. On most computers your home folder is your personal space where things like documents, photos, music, downloads and so on are kept.\nLet's switch to the home folder by running the following command:\ncd ~ Once you've done that, run the pwd command:\npwd So what has happened here? The first command:\ncd ~ Is used to change directory - that's what cd stands for. The parameter we passed to cd was just the \u0026lsquo;tilde\u0026rsquo; character (~). This character has a special meaning in the shell - it means \u0026ldquo;the current user's home directory\u0026rdquo;.\nFinally, we ran the pwd command. This command is short for print working directory. It writes out to the screen where you currently are. On my Mac, my home directory is located at /Users/dwmkerr, which is what the command has shown me.\nLet's take another look at a command. Run the following in your shell:\nls The ls command is short for list directory contents - it shows you everything that is in the current directory. On my computer you can see things like the \u0026lsquo;Downloads\u0026rsquo;, \u0026lsquo;Music\u0026rsquo; and \u0026lsquo;Pictures\u0026rsquo; folders, which are set up by default on a Mac, as well as some of my own folders.\nWe can pass different parameters to ls. The main parameter is the location of the folder we'd like to list the contents of. So if we wanted to see what was in the Music folder, we'd just run:\nls Music Not much to see here:\nMany commands actually allow us to pass multiple parameters. For example, we could list the contents of my Movies and my personal applications:\nls Movies Applications There's not much in either. You might wonder why Applications is so empty - that's because we're looking at the applications only installed for the current user, because we are in the user's home directory. To see the applications for everyone we'd need to use the folder where applications are kept for all users.\nWe can do this by running ls /Appliciations:\nThe trick here is that we start with a leading forward slash - this means the Applications folder in the root of the computer, not the one in my current folder.\nOn Windows, applications are kept in different places, but we can see some of the installed applications by running ls \u0026quot;c:\\program files\\\u0026quot;:\nWhy do we have the extra quotation marks here? If we ran the command without the quotation marks, the shell would think we were giving it two parameters. It would think we wanted to see the contents of the c:\\program and files folders - and they don't exist!\nThe error above shows what happens when we miss the quotation marks.\nNow we can take a look at how a flag would work. A flag is a parameter which changes how a command works. Flags normally start with a hyphen. Let's say we wanted to know the size of the files in the folder. We do this by using the -lh pass the parameter, which is short for long list, human readable:\nls -lh Downloads/*.jpg Now I can see all of the jpg files (jpg files are images) in my Downloads folder. I can see it looks like I've got two pictures of \u0026ldquo;Mardi Himal\u0026rdquo; (a mountain in the Himalayas) which are both 384 Kilobytes in size, as well as some other images. Blow by blow, this is what we've got:\n ls - List the contents of a folder -lh - This is the long list in human-readable sizes parameter, which means we see how big the files are in a friendly format (like 911K for Kilobytes, rather than showing something like 911012 which would be the number of bytes - and harder to read!) Downloads/*.jpg - Show the contents of the Downloads folder, including any files which end with .jpg - the * is a wildcard which means that we don't mind what the filename is  The -lh parameter is shorthand. Many commands offer longhand parameters (such as --version) as well as shorthand (such as -v as an alternative for --version). Longhand is easier to read, shorthand is faster to type.\nDon't worry - in the next section we'll see how to look up the available parameters for a command. You don't need to remember all of these details, only understand which part is the command and which parts are the parameters. This is just an introduction for now!\nNow let's look at one more command.\nThe Echo Command The \u0026lsquo;echo\u0026rsquo; command is used to write out a message in the shell. Here's an example of how it works:\necho \u0026#34;Hello Shell!\u0026#34; This command writes out the text Hello Shell!:\nWhy would we do this? One of the most common reasons would be to see what the shell thinks a certain value is. For example, try this command:\necho \u0026#34;My home directory is at: $HOME\u0026#34; You'll see something like this:\nThe $HOME part of the text is called a variable. We can recognise variables because they start with a dollar symbol. $HOME is a built-in variable which holds the location of the current user's home directory.\nWe're going to see all sorts of cool things we can do with echo as we continue in the book!\nMove Around One common thing we can do in a visual file explorer is move around. We can open folders, and go \u0026lsquo;up\u0026rsquo; from the current folder. We often also see visually where we are in the folder structure with an \u0026lsquo;address bar\u0026rsquo;.\nA useful reference might be the picture below:\nHere we map the shell commands to the visual interface's equivalents:\n pwd shows the current working directory - where you currently are in the file system ls lists the files in the current directory (or any directory you tell it) cd .. changes the directory to another location - if you use the special .. directory, you are telling it to change to the parent directory, i.e. \u0026lsquo;go up\u0026rsquo; in the file system  As a final trick, lets see how we open a file or folder. Let's say I want to open one of the photos in my Downloads folder. Here's how I can do it:\ncd ~/Downloads open himalayas.jpg We can see the result here:\nRunning open himalayas.jpg has opened the photo in the application which is used for photos by default in the operating system.\nBe aware - this command is different on different operating systems (but we're going to see later on how to fix that and make it consistent everywhere!). The open command will open a file on MacOS. On Windows you can use start, and on Linux you can generally use xdg-open.\nAs a nifty trick, trying running open .1:\n. This will open the current folder. Every folder contains two \u0026lsquo;special\u0026rsquo; folders. The first is .., which we've seen means \u0026lsquo;my parent folder\u0026rsquo; and the second is ., which means \u0026lsquo;myself\u0026rsquo;. Having this . folder is convenient, as it means we can do things like this - run a command to open the current folder.\nWe're going to go into a lot more detail on how to work with files and folders, move around, but hopefully this has provided a crash course for the basics. They key concepts to remember, which are much more important than the individual commands we've see are:\n In the shell we run commands We can change how commands work by using parameters Some parameters just go at the end of the command - like ls Downloads Some parameters start with a hyphen, and change how the command behaves - these are often called \u0026lsquo;flags\u0026rsquo;. An example is ls -lh, which lists the files in the current folder with a human-readable file size  We've also learned:\n cd changes the current directory pwd prints the current directory ls lists the files in a directory echo can be used to write out text to the screen open, start and xdg-open can be used to open a file or folder on MacOS, Windows and Linux respectively  Now we can start to get into more detail!\nSummary In this section we learnt:\n That this book is for IT professionals, hobbyists or anyone who wants to learn more about how to work with computers What the shell is, and why we might want to use it How to open the shell programs for Windows, Mac and Linux which are installed by default How to configure the shells for Windows or Mac to behave in a Linux-like way to allow us to follow on with the rest of the book  We introduced the following commands:\n cd - which changes directory pwd - which prints the current working directory ls - which lists the contents of a directory echo - which writes text to the screen open - which will open a file or folder  We also briefly introduced variables, which are special values which start with the dollar symbol, such as $HOME which stores the user's home directory. We saw that each directory contains two special directories - .. which represents the parent directory, and . which represents the current directory.\nWith these tasks complete we can now move onto the next section.\n Footnotes\n  On Windows you might need to run start . and on Linux, xdg-open .. \u0026#x21a9;\u0026#xfe0e;\n   "});index.add({'id':2,'href':'/docs/part-1-transitioning-to-the-shell/navigating-your-system/','title':"Navigating Your System",'content':"Chapter 2 - Navigating Your System Switching from a graphical user interface to the shell can take some getting used to. First we'll take a look at how to navigate your system using the shell, and get information on files and folders in the system.\nThis section will introduce the pwd, ls, cd, pushd and popd commands, as well as the concepts of the \u0026ldquo;working directory\u0026rdquo; and \u0026ldquo;environment variables\u0026rdquo;. We'll also take a bit of a look into how \u0026ldquo;Paths\u0026rdquo; work.\nIf these commands far familiar to you then feel free to jump to the next chapter! Otherwise, let's get started.\nThe Working Directory Perhaps the easiest way to start to understand how to navigate your system using the shell is to use a graphical interface as an illustration of how we often navigate. Open your shell, and enter the following command:\npwd You should see something like this:\nWhen we open a folder in a graphical user interface, we are always viewing the contents of a folder, or directory. When you open the shell, the same applies - we are always sitting in a specific directory.\nThe pwd command is the Print Working Directory command. It shows the full path of the directory that you are in. You might not use this command very often, as in many shells you can see the directory you are in (if you cannot see this in your shell, you'll find out how to do this in Chapter 18).\nThere's one more way to find the working directory. It is stored in an Environment Variable called PWD.\nAn environment variable is just a bit of data that you can access from your shell. You can create them, you can change them, and there are some which are set for you by the system or the shell to help you out.\nTry the following command:\necho \u0026#34;My current working directory is: $PWD\u0026#34; You should see something like this:\nThe dollar symbol is used to tell the shell we want to use the PWD variable, not write out the text PWD. We'll see a lot more about environment variables as we continue through the book.\nListing the Contents of the Working Directory In the graphical user interface, we can also see the files and folders in the current directory. In the shell, we don't see this content. But we can show the contents of the current working directory with the following command:\nls You should see something like this:\nThe ls command is the List Directory Contents command. It will show the contents of a directory. If we don't give it any parameters, it will show the contents of the current directory.\nThere are a lot of options for the ls command. In Chapter 5 we'll see how to find out the options for commands. For now, let's look at one of the most common options -l. This shows the contents as a list:\nls -l A little like the \u0026lsquo;details\u0026rsquo; view in a graphical user interface, this list view shows us more details, such as who owns the file or folder, when it was modified, and more. Again, we'll see more details on this later.\nChanging the Directory In a graphical user interface, you move to a different directory by clicking on it.\nIn the shell, you run the cd command. Try it out with:\n# Move to the pictures directory... cd Pictures # ...then list the contents of the directory. # Note that the \u0026#39;-al\u0026#39; flags mean show *all* files, as a *list*. ls -al Note that when you see shell commands, everything which starts with a hash symbol is a comment. These comments are just for readability, you don't need to include them. But if you are saving your own shell snippets (or \u0026ldquo;scripts\u0026rdquo;), then you might find comments a useful way to remind yourself of what you are hoping to achieve with the commands, or to make the script more readable.\nOn my system, we'll see the following output:\nThe cd command is the Change Directory command. You might see a pattern here - shell commands often are very short (to make it easier to type them quickly) and are often made up of the first letters of the description of the command (pwd for Print Working Directory, cd for Change Directory).\nNow that you know how the cd command works, you will be able to move around to different folders. At this stage, it's important to talk a little bit about how paths work in systems.\nUnderstanding Paths In Linux, Windows and MacOS (and most other operating systems), paths are the \u0026lsquo;addresses\u0026rsquo; of files or folders.\nThere are two types of paths - Absolute Paths and Relative Paths. An absolute path is one which gives the exact location of a file. For example, on my computer, the absolute path to the folder I am writing this book in is:\n/Users/dwmkerr/repos/github/dwmkerr/effective-shell Absolute paths always start with a slash. That's how the system knows it is an absolute path. The / is the root of the file system - basically it's the folder which everything else lives in.\nIf I have an absolute path, I know exactly where the file or folder is. Let's compare this to a relative path. Below is the relative path in my shell for the file I'm writing right now:\nwebsite/content/docs/part-1-transitioning-to-the-shell This path is relative to my current working directory in the shell. This means that this path only makes sense if you use it from a specific directory. If I am in my Pictures folder, and I want to move to the 2020-photos folder, I could do it in two ways. The first is with an absolute path:\ncd /Users/dwmkerr/Pictures/2020-photos The second is with a relative path:\ncd 2020-photos In short - relative paths are often useful if you want to move to something within the current directory and absolute paths are useful if you need to move to somewhere completely different.\nThe Special Dot and Dot Dot Folders As you experiment with these commands, you might have noticed that every folder contains two other folders, one with the name . and one with the name ... Run ls -al on the pictures folder to see an example:\nls -al pictures You should see something like this:\nThis picture highlights two special folders - . and ... These are special folders which exist in every folder in the system.\nThe first folder, ., represents the folder it is in. Why would this be useful? Well, sometimes we just want a quick way to say the equivalent of \u0026ldquo;right here\u0026rdquo; in a command. For example, if I wanted to copy the current folder to a backup folder, I could do this:\ncp . /backup The cp command is the Copy command, and we'll see it in the next chapter. But the key thing to note is that we can use . to tell the command to copy the folder we are in right now.\nThe .. folder means the parent folder. You can use this to \u0026ldquo;go up\u0026rdquo; to the parent folder, for example:\ncd .. ls . Would give:\nNote that we've used cd .. to change directory to the parent folder then ls to list the contents of the current folder. We could also just have used ls on its own as it defaults to the current folder.\nThe .. folder can be helpful if you need to navigate to a location which is outside of your current folder. For example, if I am in the pictures folder and I want to move to the scripts folder, I can just use:\ncd ../scripts ls And we'll see this:\nThe Home Directory There is one more special part of the file system we have to know about. That is the Home Directory. In Linux-like systems every user has their own personal directory where they can keep their files and folders.\nThis directory can always be accessed through the ~ character. For example, no matter where I am in the system, I can run the following command to move to my home directory and show the contents:\ncd ~ ls This would show something like this:\nThis makes moving around your home directory very easy. For example, on a Mac, to go to your pictures folder from anywhere, you can always just run:\ncd ~/Pictures Your home directory on most computers will be where you keep your documents, pictures, videos and so on. Normally this directory is not accessible to other users of the system. Each user in a system gets their own home directory.\nYou can also see the home directory by using the special HOME environment variable:\necho \u0026#34;My home directory is: $HOME\u0026#34; This would show something like this:\nOne useful trick - running cd without any parameters will always take you home! So to go home, just run:\ncd Now that we know about relative paths, absolute paths, and the special dot and dot dot folders, and the home directory we can continue learning how to navigate the shell!\nPushing and Popping the Working Directory One thing we might want to do is quickly move from one location to another, then go back again. Let's say for example I am working in on this chapter, but I want to check my downloads. One way to do this is with this pushd command:\npushd ~/Downloads ls popd After I've checked my downloads, I can run popd to go back to where I was:\nThe pushd command \u0026lsquo;pushes\u0026rsquo; a new working directory onto a stack - moving you there. The popd command \u0026lsquo;pops\u0026rsquo; the working directory off the top of the stack. A stack is a structure often used in computers; we can actually push lots of different files to the working directory stack.\nWhy is it called a stack? Well, the reason is that if we were to visualise the structure, it might look like a stack of plates or similar. Here's how pushd and popd would look if we were to visualise it:\nThese commands can be useful if you need to move to other locations but want to be able to quickly go back to where you were before afterwards.\nGoing Back One last trick which can save time is the following command:\ncd - This is a special parameter for cd which tells it to go back to the last location you moved to. Here's how it might look if you use it:\nThis can only be used to go back to the last directory. If you need to be able to go backwards multiple times or through a history of directories, you might need to use pushd and popd instead.\nSummary In this chapter we introduced the following:\n The pwd (print working directory) command shows the current working directory The $PWD environment variable holds the current working directory The ls (list) command shows the contents of the current directory or a given directory The ls -l command shows the contents of the current directory as list The cd (change directory) changes the current working directory Absolute paths are paths which specify the exact location of a file or folder\u0026hellip; \u0026hellip;Relative paths are paths which are relative to the current directory The . special folder means \u0026lsquo;this folder\u0026rsquo; The .. special folder means \u0026lsquo;the parent folder\u0026rsquo; The ~ special folder is the \u0026lsquo;home directory\u0026rsquo; The $HOME environment variable holds the user's home directory You can run cd at any time to quickly go to your home directory You can use pushd and popd to push and pop the working directory stack You can use the cd - command to go back to the last location  "});index.add({'id':3,'href':'/docs/part-1-transitioning-to-the-shell/managing-your-files/','title':"Managing Your Files",'content':"Chapter 3 - Managing Your Files Downloading, unzipping, copying, moving, renaming and deleting files in a graphical user interface is normally fairly intuitive. Now we'll learn how to perform the same operations in a shell. Once you can organise your files, you are well on your way to being able to use the shell more effectively for day to day tasks.\nNow that we know how to organise the files in our computer, we'll take a look at how to download files, create new files, preview the contents of files, open files, copy, move and delete files.\nThis chapter will introduce the wget, unzip, cp, mv, rm, mkdir, rmdir, cat and zip commands. We'll also briefly look at wildcards and redirection.\nCreating a Playground Before we start copying, deleting, moving and renaming files, we should create a \u0026lsquo;playground\u0026rsquo; area we can work in. We don't want to test all of this on our own personal files until we know exactly what we're doing!\nTo help with this, I've created a zipped up folder which has a lot of files in it which we can use to play with. Now the file itself is available on the effective-shell.com website, right here:\neffective-shell.com/downloads/effective-shell-playground.zip\nWe could open up a web browser, download the file, unzip it and then start from there, but this book is all about how to deal with every day tasks in your shell, so let's skip the browser and do it in the shell instead!\nOpen your shell - if you've not yet got set up with your shell, that's OK, just check Chapter 1 - Getting Started.\nNow that you have your shell open, we can run the wget command (Web Get) to download the zip file. Let's download it to our Home folder. If you are not sure what the Home folder is, check Chapter 2- Navigating Your System.\nFirst, we'll move to our home directory, then download the file.\ncd wget https://effective-shell.com/downloads/effective-shell-playground.zip You'll see something like this:\nWhen you call the wget command, you can give it any web address and it'll download it to your current folder. It also shows the progress of the download interactively (particularly useful if it's a big file!).\nAs an aside, if we were not in our home folder when we called the wget command, we'd download the file to wherever we are currently working in. If we wanted to be explicit about where we download the file, we can use the -O (Output File) flag to say explicitly where we want to download the file.\nAs an example, if were not in the home folder, but wanted to download there, we'd just call:\ncd wget -O ~/playground.zip https://effective-shell.com/downloads/effective-shell-playground.zip Now that we've downloaded the file, let's look at our home directory now, with a quick call to ls ~:\nCool - we have the zip file downloaded! Now we need to work out how to unzip it so we can get to the files in the zip archive.\nFinding out about files One of the interesting things you can do in a shell is ask it to tell you more about a file. This can be useful if we've got a file, and we're not sure what it might be. Let's try it out now:\nfile ~/effective-shell-playground.zip The file command is showing us we have a zip file - now it's time to unzip it!\nExtracting the Zip Right now we have a zip file. We need to extract it, unpack the files so that we can play with them. Again, in a system with a graphical user interface, this is easy, generally you just double click on it. But we're going to use the shell for this!\nRun the command:\nunzip ~/effective-shell-playground.zip Now let's look at what we've got with the ls command:\nExcellent - we've now got a folder which contains all of the files in the zip archive.\nDeleting Files Now that we've downloaded and unzipped the file, we don't need the zipped version any more. So let's delete this file.\nThe rm (Remove) command can be used to delete a file. If we run:\nrm ~/effective-shell-playground.zip ls | grep playground Then we'll see the following:\nNotice that the zip file is gone - just the folder is left.\nBy the way - be really careful with the rm command. Unlike in a graphical interface, it won't put files you delete into a recycle bin, they are blatted forever! In a later chapter we'll see some ways to change this behaviour for your local machine, but always remember rm is a little risky!\nHowever one thing it will do to try and help you not make mistakes is let you know if you are trying to delete a folder, not a file.\nRun the following command to try and delete the unzipped folder:\nrm ~/effective-shell-playground The rm command has not succeeded in this case - it's warning us that we're not deleting a file, but a whole directory.\nNow we can get around this by adding the -r flag, which means \u0026lsquo;recursive\u0026rsquo; - i.e. not just the folder but everything in it. But use this with caution!\nExamining the Contents of a Folder Let's take a look at what is in the playground. By the way, the output you see on your computer might have a few more files in it as I might have added some after writing this article!\nIn a graphical user interface, we'd open the folders and look at the files. In the shell, we can use the tree command to show the contents of a folder.\nNow the tree command is not installed by default on all systems. So if you are on a Mac, run:\nbrew install tree If you are on Linux, you will likely already have it. If you don't, use your distributions package manager to get it (e.g. apt-get install -y tree).\nUsing a non-universal command is generally not our goal in this book, but in these early stages while we are transitioning from the graphical user interface, the tree command can be really helpful. Later on we'll see how to use the more universal find command to give a similar output.\nTry it out with:\ntree ~/effective-shell-playground The tree command shows you all of the folders and files in a location. If we are unsure what one of the files is, we can ask the shell to give us more info. For example, I could find out more about the loas-gch.JPG file by running:\nfile ~/effective-shell-playground/pictures/loas-gch.JPG Note that the file command is already showing it is a bit more clever. It knows that the file is a JPEG file (a picture), but is giving other details as well.\nCopying a File Let's say we really love that photo, and we want to make a copy of it. We can do that easily by using the cp (_Copy) command:\ncp ~/effective-shell-playground/pictures/laos-gch.JPG ~/effective-shell-playground/pictures/laos-gch-copy.JPG This makes a copy of the file - if you are not sure if it has worked, just run:\ntree ~/effective-shell-playground We can see we've made a copy.\nSaving Some Keystrokes Wow, it's painful putting ~/effective-shell-playground before everything! From Chapter 2- Navigating Your System we already know how to change directory, so let's do that now:\ncd ~/effective-shell-playground Remember - cd is change directory. Excellent - until we tell our shell otherwise, this our new working directory.\nRenaming or Moving Files You might have noticed that the photos have different endings - one of them ends in .JPG. Let's rename it so that it has the ending .jpeg to be consistent with the others.\nTo do this, we use the mv (Move) command. When it comes down to it, moving a file or renaming a file amount to the same kind of operation, so one command can do both.\nRename the copy we made of the photo by running:\nmv pictures/loas-gch-copy.JPG pictures/loas-gch-copy.jpeg Let's run tree to see what happened. Remember - now that our working folder is the playground, we don't even need to tell tree where to look, if we give it no arguments it'll assume we're looking at the working directory:\nMuch nicer! Now our copied file has been moved to have a new name. It's in the same folder still, but you can use mv to also change what folder a file is in.\nCreating a New Folder Perhaps we're not happy with the name pictures for our folder we've been playing with, maybe we'd prefer to have them all in a folder called photos?\nProbably the first thing we'd do in a graphical environment is create a new folder - so let's do thee same here!\nRun the commands:\nmkdir photos tree And we should see:\nWe've use the mkdir command, which is short for Make Directory. This is how we create a new folder in the shell.\nNow let's say we wanted to be really organised, and create a photos folder by year and topic, perhaps 2019/outdoors/pictures. In a graphical user interface, we'd have to create each folder one at a time. In the shell, it's easy!\nmkdir -p 2019/outdoors/pictures tree Let's see how it looks:\nAll we had to do was add the -p flag (which means \u0026ldquo;make the parent folder if it doesn't already exist) and we can create a whole set of subfolders. Now we're starting to see why knowing the shell can be powerful - if you know you have this trick up your sleeve you can be doing things like re-organising files more effectively in a shell than in your graphical user interface!\nCopying or Moving Multiple Files with Wildcards Let's copy the photos that we have in the pictures folder into the photos/2019/outdoor/climbing folder.\nWhen we run the cp or mv command, we can use a wildcard to specify the files we are copying and moving. A wildcard is a simple pattern which can be used to select multiple files. Here's how we can copy the photos over:\ncp pictures/* photos/2019/outdoor/climbing Here's how it works for Now we need to copy over our files from the pictures folder to the 2019/outdoor/photos folder. We'll use exactly the command we used before to copy a file - cp:\n$ cp pictures/* photos/2019/outdoors/climbing/ $ tree photos photos ├── 2019 │ └── outdoors │ └── climbing │ ├── laos-gch-copy.jpeg │ ├── laos-gch.JPG │ └── nepal-mardi-himal.jpeg └── 2020 └── outdoors └── climbing 6 directories, 3 files Here we've used the wildcard symbol, which is *, to say \u0026ldquo;everything in the folder\u0026rdquo;. Many commands can take wildcards as inputs. We'll see much more about them later!\nDeleting a Folder Now that we have our more organise 2019/outdoors/photos folder, we don't need the photos folder we created. So let's delete it! Remember how rm removes a file, and mkdir creates a folder? Well rmdir will remove a folder!\nrmdir photos tree As an important sidenote, just how rm doesn't move files to your recycle bin, so you cannot undo the operation, rmdir works the same way. So if we try to remove a directory which has things in it, such as the pictures directory, it will fail:\nrmdir pictures In this case, it is actually easier to just call rm -r pictures. Why is that? Well it's just like we saw in the earlier example - rm can delete files or directories. And if the directory is not empty, we just add the -r (Recursive) flag to tell it to delete the directory and everything it contains.\nLooking at Text Files Run tree and you'll see we have a quotes folder:\ntree We're going to use the cat (Concatenate) command to look at the Ursula Le Guin quote. Run the following command:\ncat quotes/ursula-le-guin.txt In the screenshot we snuck in a quick file call to see what the shell thinks the file is.\nWhy Concatenate? We're just showing the text in the terminal, not concatenating (i.e. joining) anything! Well the reason is that the cat command does concatenate files (i.e. puts them together), it's just that we only gave it one file, so it had nothing to join it to. By default, cat writes the output to the screen, so this is one of the most common ways you'll see to quickly look at the contents of a file.\nWe'll see a lot more about how this works later, and how you can then take that output and put it somewhere else. But for now, let's finish with a couple of tricks.\nFirst, let's just cat the whole folder:\ncat quotes/* There we see the * wildcard again. We could be more specific and use something like cat quotes/*.txt to only show files ending in .txt.\nNotice how the output from all of the files has been concatenated together into a single output? That's where the cat name comes from - it concatenates, i.e. joins files.\nAs one last trick, let's use this output but instead of showing it on the screen, put it into a single all-quotes.txt file:\ncat quotes/* \u0026gt; quotes/all-quotes.txt tree cat quotes/all-quotes.txt The \u0026gt; part of this is called a redirect operator - in short it's telling the shell not to write to the screen, but to write to a file. We've concatenated all of the individual quotes and made a single file from them.\nWe'll look at wildcards and redirection in a lot more detail as we continue through the book!\nZipping up Files Let\u0026rsquo; say that we want to zip up the new 2019/outdoors/pictures folder. We've already seen the unzip command, let's see how to use the zip command to zip up a folder:\nRun the command below:\nzip -r 2019-outdoor-pictures.zip 2019 This is how it will look - there's a tree and ls command before and after so we can see what's happening!\nGreat! We've created a zip. Let's dissect the command a bit:\n zip just means call the zip executable -r means recursive we don't just want to zip the 2019 folder, we want to zip everything inside it as well! 2019-outdoor-pictures.zip is the name of the file we want to create, we put this first\u0026hellip; \u0026hellip;because everything which follows (e.g. 2019) is going to be zipped, and we can specify many files and folders if we want  Summary In this chapter we introduced the following:\n The wget (web get) command can download a file from the web. If we use the -O (output location) flag, we can specify where we want to download the file to. The file command can be used to ask the shell what it thinks a file is (this is quite useful because unlike on some systems, not all files in Linux have a file ending). The unzip command can unzip a file for us. The rm (remove) command can delete a file. The rm command won't delete a folder which has files in it, unless you tell it to by adding the -r (recursive) flag. The tree command can show the files and folders in a given directory, or the current directory by default. The cp (copy) command can copy a file. The cp can also be given wildcards like * to copy many files. The mv (move) command can move or rename a file. The mkdir command can create a folder - it can even create a whole tree of folders if you pass the -p (_create parent directories) flag. The rmdir command can delete a folder - but just like rm it will fail if the folder is not empty! When we delete files in the shell with rm or rmdir they are gone forever, no recycle bin! The cat command (concatenated) can be used to write the contents of a file to the screen. We can pass multiple files to commands like cat if we use wildcards, such as quotes/*. We can write the output to a file instead of the screen, if we use the \u0026gt; (redirect to file) operator.  "});index.add({'id':4,'href':'/docs/part-1-transitioning-to-the-shell/clipboard-gymnastics/','title':"Becoming a Clipboard Gymnast",'content':"Chapter 4 - Becoming a Clipboard Gymnast For those who are new to the shell, we've covered a lot. In this chapter we'll slow down the pace of new commands a bit and instead focus on a core skill which you will already be familiar with from Graphical User Interfaces - using the clipboard.\nYou have probably already been using the clipboard with the shell, copying and pasting commands and their outputs. However, there's a lot more we can do with the clipboard. Now we'll look at how to take this to the next level.\nWe'll also briefly introduce introduce aliases and pipelines, which will be covered in a lot more detail in later chapters.\nThe Clipboard Essentials I wouldn't be surprised if the keyboard shortcuts to access the clipboard are already firmly locked into muscle memory for almost all readers, but just in case, here's a reminder of the shortcuts across different systems:\n   Command Windows Shortcut Linux Shortcut MacOS Shortcut     Cut Ctrl + X Ctrl + X ⌘ + X   Copy Ctrl + C Ctrl + C ⌘ + C   Paste Ctrl + V Ctrl + V ⌘ + V    In the shell, you may find that these commands don't run as expected. For example, in the screenshot below I have tried to use Ctrl + V a few times to paste into terminal on Ubuntu:\nInstead of the contents of the clipboard being dropped into the shell, we see the text ^V. Why is this?\nWell, some of this is historical (the shell has been around for a long time so we'll see this answer a lot!). The reason the Ctrl key is called the Control Key is that it is used to send control sequences to the computer. When we're using the Control Key, the characters we send are not plain text, they're used to perform actions. This is something that is probably pretty familiar. For example, Ctrl + P is almost universally used as a shortcut for the \u0026lsquo;Print\u0026rsquo; command.\nWe tend to think of these commands as shortcuts to save us from finding the appropriate command in a menu or on a toolbar. But of course most shells and command-line interfaces pre-date graphical user interfaces. They needed a way to differentiate between a user entering plain old text, and a user wanting to execute a certain command.\nEven modern shells tend to follow the conventions around control sequences which were established by earlier ones to ensure a consistent experience for users who are used to working with shells. Shells have a whole bunch of control sequences which actually pre-date the graphical user interface, the clipboard itself, and even screens!\nSome of the control sequences used in the shell you might already be familiar with. For example, if you have a program running and want to cancel it, you might be used to using Ctrl + C. This actually sends a signal to the program and typically the program responds by closing. We'll see signals again and again as we go through the book.\nThe Ctrl + C combination terminates the current program. What about Ctrl + V? This is the grand-sounding \u0026ldquo;Verbatim Insert\u0026rdquo; command. It tells the shell to write out the next keystroke you give it. This allows you to write out \u0026lsquo;special\u0026rsquo; characters like the escape key, left or right keys, or even the Ctrl + V combination itself.\nSo if you type Ctrl + V twice, the shell writes out the text ^V. The hat symbol ^ represents Ctrl. The first command tells the shell to write out the following command, the second is then written out directly. You can try writing out some different sequences. You'll see various odd looking symbols drawn, which represent things like the Alt key and other special keys.\nSo why do we need to care? Well the shell already has a command for Ctrl + C and Ctrl + V, so we're going to need to work around this to use our familiar \u0026lsquo;copy\u0026rsquo; and \u0026lsquo;paste\u0026rsquo; commands.\nHow this works varies across platforms. Follow the instructions below for the platform you are using.\nWindows\nIf you are using a Command Prompt, then the usual shortcuts will work fine. However, most of the time we will be using Bash. In this case the shortcuts will not work. Instead, select the Use Ctrl+Shift+C/V as Copy/Paste option from the properties menu:\nYou can now use Ctrl+Shift+C for copy and Ctrl+Shift+V for paste. You can also copy text by just dragging the cursor over it with the right mouse button.\nLinux\nOn most Linux systems you'll be using the Gnome terminal, which means that you can use Ctrl+Shift+C for copy and Ctrl+Shift+V for paste. You can also right click on text with the cursor to select it.\nMacOS\nMac users can just use ⌘ + C for copy and ⌘ + V for paste. The shell doesn't use the special Mac Command character ⌘, which means the default keyboard mappings on MacOS work fine in a shell as they do not clash with anything.\nNow that we've got the basics out of the way, and learnt far more than we probably wanted to about control keys, we can look at more ways to use the clipboard.\nPreparing the Clipboard Commands Copying and pasting text to and from the clipboard is useful, but there's a lot more we can do. With a couple of basic commands we can hugely expand what we can do with the shell and make a whole set of everyday tasks far easier to accomplish.\nThere is one small complexity we'll need to work through before we continue. The complexity is that the clipboard is accessed in different ways on Windows, Linux and MacOS. I'll first show you how to deal with this, just follow the instructions for the platform you are working on.\nTo make things easier for the reader I'm going to assume you have created the pbcopy and pbpaste commands by following the instructions below. I am creating these commands so that regardless of the platform you are using the tutorials will work in the same way!\nWindows\nAssuming you are using WSL, you will need to run the following two commands. By the time this book is published there may be a cleaner way, but for now this is a workaround for some limitations on the WSL system:\nalias pbcopy=\u0026#39;clip.exe\u0026#39; alias pbpaste=\u0026#34;powershell.exe -command \u0026#39;Get-Clipboard\u0026#39; | tr -d \u0026#39;\\r\u0026#39; | head -n -1\u0026#34; Don't worry about how these commands work - by the time you've gone through the book it should make perfect sense. For now you just need to know we're adding two new commands to our toolkit - pbcopy and pbpaste, which will work in Bash on Windows.\nLinux\nHopefully if you are Linux user the commands below will seem familiar. They install the xclip program and create shortcuts to copy and paste. You absolutely don't need to do this if you prefer to call xclip directly, these commands are just setup so that across all platforms the tutorial looks the same.\nsudo apt install -y xclip alias pbcopy=\u0026#34;xclip -selection c\u0026#34; alias pbpaste=\u0026#34;xclip -selection c -o\u0026#34; MacOS\nNothing is required on MacOS - pbcopy and pbpaste are built in.\nMaking these changes permanent\nWe've used the alias command to create pbcopy and pbpaste. In Bash (and most shells) an alias is something you can configure as a shortcut to avoid having to type longer commands. There's a whole chapter on commands in Section 2.\nThese instructions will need to be repeated when you re-open your terminal. In a later chapter we'll see how to make permanent customisations to our shells so that we don't have to repeat this setup.\nWe'll also see later on how to create configuration which works across many different platforms, so that you can use the same configuration regardless of what platform you are working on. This is very useful if you work across multiple machines or operating systems!\nCopy and Paste Basics Now that we've created these commands, we can use them to access the clipboard. For example, if I copy the following text:\nKirk Van Houten Timothy Lovejoy Artie Ziff Then I can paste it into the shell with the following command:\npbpaste And we'll see something like this:\nCopying is just as straightforward. If you have downloaded the Effective Shell \u0026lsquo;playground\u0026rsquo; folder you can see we have a list of characters from \u0026ldquo;The Simpsons\u0026rdquo; in the file playground/text/simpsons-characters.txt. Now we could use the cat command to show the contents of the file, and then manually select the text and copy it. Even easier though is to just pipe the contents of the file to the pbcopy command:\ncat ~/playground/text/simpsons-characters.txt | pbcopy The output will look similar to the below (I've included the output of cat for reference as well):\nThe vertical bar | is the pipe operator. It tells the shell to take the output from the command on the left and send it straight to the input of the program on the right. We're going to see a lot more of the pipeline operator as we continue. For now it's enough to know you can use it to \u0026lsquo;chain\u0026rsquo; commands together.\nThis might not seem super useful so far - but if the text file was a lot larger then it would be much harder to cat it out, use the mouse to select all of the text (scrolling up through the window) and then copy it. And if you didn't have a mouse, it would be even more tricky. We're aiming to be as effective as possible when using the shell so being able to use the keyboard quickly for common tasks is critical.\nNow we can see some real world examples of how these commands can be useful in daily tasks!\nRemoving Formatting Don't you hate it when you have to copy formatted text and don't have an easy way to paste it as unformatted text? Here's an example, I want to copy this Wikipedia page on \u0026lsquo;bash\u0026rsquo;, and paste it into a Word document:\nMany programs have a shortcut to paste the contents of the clipboard (such as \u0026lsquo;command + shift + v\u0026rsquo;) but if you are like me you might find yourself pasting into a plain text editor just to copy out the plain text.\nIf you just run the command pbpaste | pbcopy, you can easily strip the formatting:\nWe're just piping out the clipboard (which ends up as plain text, cause we're in a terminal!) and then piping that plain text back into the clipboard, replacing the formatted text which was there before.\nThis little trick can be very useful. But we can use the same pattern to quickly manipulate the contents of the clipboard in more sophisticated ways.\nSorting Text Because we can pipe the contents of the clipboard to other programs, that means we can easily use the huge number of tools available to us to work with text.\nLet's take another look at the list of characters we have in the ~/plaground/text/simpsons-characters.txt file:\n$ cat ~/playground/text/simpsons-characters.txt Artie Ziff Kirk Van Houten Timothy Lovejoy Artie Ziff Nick Riviera Seymore Skinner Hank Scorpio Timothy Lovejoy John Frink Cletus Spuckler Ruth Powers Artie Ziff Agnes Skinner Helen Lovejoy We can easily take this text, sort it and then directly copy the results:\n$ cat ~/playground/text/simpsons-characters.txt | sort | pbcopy The contents of the clipboard will now contain:\nAgnes Skinner Artie Ziff Artie Ziff Artie Ziff Cletus Spuckler Hank Scorpio Helen Lovejoy John Frink Kirk Van Houten Nick Riviera Ruth Powers Seymore Skinner Timothy Lovejoy Timothy Lovejoy The sort command has lots of different options but the defaults work fine for this case. We can see we've got quite a few duplicates - now we can move onto how we'd handle that.\nManipulating Text Let's say someone has emailed me a list of people I need to invite to an event:\nThe problem is:\n The list is in Excel and is formatted The list has duplicates I need to turn each name into an email address like 'Artie_Ziff@simpsons.com\u0026rsquo;  I want to email get the email addresses on my clipboard ready to paste into my email client quickly. We can quickly handle this task without leaving the shell.\nIf you want to try out the same commands and follow along you can copy the raw text below (don't worry if the commands are unfamiliar, we'll be seeing them again and again and breaking down each one in later chapters):\nArtie Ziff Kirk Van Houten Timothy Lovejoy Artie Ziff Nick Riviera Seymore Skinner Hank Scorpio Timothy Lovejoy John Frink Cletus Spuckler Ruth Powers Artie Ziff Agnes Skinner Helen Lovejoy First, we copy the text to the clipboard.\nNow we can paste and sort:\n$ pbpaste | sort Agnes Skinner Artie Ziff Artie Ziff Artie Ziff Cletus Spuckler Hank Scorpio Helen Lovejoy John Frink Kirk Van Houten Nick Riviera Ruth Powers Seymore Skinner Timothy Lovejoy Timothy Lovejoy Then remove the duplicates:\n$ pbpaste | sort | uniq Agnes Skinner Artie Ziff Cletus Spuckler Hank Scorpio Helen Lovejoy John Frink Kirk Van Houten Nick Riviera Ruth Powers Seymore Skinner Timothy Lovejoy Replace the space with an underscore:\n$ pbpaste | sort | uniq | tr \u0026quot; \u0026quot; \u0026quot;_\u0026quot; Agnes_Skinner Artie_Ziff Cletus_Spuckler Hank_Scorpio Helen_Lovejoy John_Frink Kirk_Van_Houten Nick_Riviera Ruth_Powers Seymore_Skinner Timothy_Lovejoy Then add the final part of the email address:\n$ pbpaste | sort | uniq | tr \u0026quot; \u0026quot; \u0026quot;_\u0026quot; | sed 's/$/@simpsons.com/' Agnes_Skinner@simpsons.com Artie_Ziff@simpsons.com Cletus_Spuckler@simpsons.com Hank_Scorpio@simpsons.com Helen_Lovejoy@simpsons.com John_Frink@simpsons.com Kirk_Van_Houten@simpsons.com Nick_Riviera@simpsons.com Ruth_Powers@simpsons.com Seymore_Skinner@simpsons.com Timothy_Lovejoy@simpsons.com This looks perfect! We can now put the transformed text back onto the clipboard:\n$ pbpaste | sort | uniq | tr ' ' '_' | sed 's/$/@simpsons.com' | pbcopy All in all we have the following pipeline:\n pbpaste - output the clipboard sort - order the output uniq - deduplicate the rows tr ' ' '_' - replace spaces with underscores sed /$/@simpsons.com - add the email domain to the end of the row  Now you don't need to remember all of these commands. We'll be going into them in detail as the book continues, and in the next chapter we'll be looking into how you can get help directly in the shell to discover how commands work. The key concept is that you can treat the clipboard just like a file - reading from it, manipulating it, and writing back to it, without ever leaving the shell.\nIn fact - if you are on a Linux system, try running:\ncat /dev/clipboard You'll see the contents of the clipboard written out. In Linux almost everything can be represented as a file - the clipboard included! Like a lot of the other topics this is something we'll visit again in detail later.\nWe're also going to spend a lot of time later on looking at pipelines in detail, so don't worry too much if this seems overwhelming at this stage!\nAs you go through the book you'll be able to apply every technique you learn to the clipboard itself - hopefully you'll find this can save you a lot of time and make you even faster with your day to day work.\nSummary In this chapter we learnt:\n You can copy and paste into the shell with keyboard commands which are the same, or at least very similar, to the commands you normally use. Different operating systems access the clipboard in different ways, but we can work around this by creating an alias command (which we'll see in detail later) We can use pbcopy to copy and pbpaste to paste. We can \u0026lsquo;chain\u0026rsquo; commands together with the | (pipe) operator. We can turn formatted text on the clipboard into plain text by just running pbpaste | pbcopy. We can sort lines of text with the sort command. There is clearly a lot more we can do with text as we save hints of with the uniq, tr and sed commands - which we'll introduce in detail later. You can treat the clipboard a bit like a file in the shell. On Linux, lots of things can be represented as files - including the clipboard (which is accessed via the /dev/clipboard file).  "});index.add({'id':5,'href':'/docs/part-1-transitioning-to-the-shell/getting-help/','title':"Getting Help",'content':"Chapter 5 - Getting Help In the earlier chapters I've introduced quite a few commands. Having to remember all of these commands and their parameters would be very hard. Fortunately there are built-in capabilities in the shell to help.\nIn this chapter I'll show you how to quickly get help when working with tools in the shell, without disrupting your flow!\nGetting Help is Important! If you are trying to be more effective when using the shell, it is crucial to know how to quickly look things up.\nThere'll be many circumstances where you'll need to open a browser to search for help. But there's also a wealth of information only a few keystrokes away. Looking up parameters, checking how to run commands, C library documentation, or even useful information like ASCII charts are available directly in the shell.\nBeing able to access this information quickly, without jumping into a browser or interrupting your flow is going to be one of the most crucial things you can do to become an effective shell user.\nFirst we're going to look at the standard help system which is available on all Unix-like systems, which is man (short for \u0026lsquo;manual\u0026rsquo;). Then we'll see a useful tool you can installed called tldr, which might be more helpful for day-to-day use. Finally we'll take a look at the cht.sh site as an alternative source for help.\nUnderstanding \u0026lsquo;man\u0026rsquo; Most tools you encounter in the shell have manual pages available. Many people will be familiar with the man command to get help on a tool, but there is a lot more help available than people often realise.\nGetting help on a command The most basic way to get help on a command is with man. Here's an example:\n$ man cp CP(1) BSD General Commands Manual CP(1) NAME cp -- copy files SYNOPSIS cp [-R [-H | -L | -P]] [-fi | -n] [-apvX] source_file target_file cp [-R [-H | -L | -P]] [-fi | -n] [-apvX] source_file ... target_directory DESCRIPTION In the first synopsis form, the cp utility copies the contents of the source_file to the target_file. In the second synopsis form, the con- tents of each named source_file is copied to the destination target_directory. The names of the files themselves are not changed. If cp detects an attempt to copy a file to itself, the copy will fail. ... The man command opens the manual for the given tool. These manuals should contain all command line options and details of how to use the tool.\nYou can scroll up and down through the content with the arrow keys. This scrolling capability actually is not part of man - it is available because the information is presented in the shell pager. A pager is a tool for looking through content which might not easily fit on a screen.\nUsing the pager The first thing you might notice is that you can move through the manual pages with the arrow keys.\nThe man command finds the appropriate manual page (often shortened to \u0026lsquo;manpages\u0026rsquo;) and then opens the page in a pager tool. The pager is what is providing the keyboard interface to look through the file.\nOn most systems, the pager will be the less program. There are lots of commands you can use to navigate through files with less, but the bare essentials are:\n d - Scroll down half a page u - Scroll up half a page j / k - Scroll down or up a line. You can also use the arrow keys for this q - Quit /\u0026lt;search\u0026gt; - Search for text n - When searching, find the next occurrence N - When searching, find the previous occurrence  There are many other commands, but the set above is normally what I find myself using the most.\nIf you are interested, you can actually see what your pager is with the command below:\n$ echo $PAGER less The $PAGER environment variable is used to tell the shell what program to use for paging. A few more details can be found with the man man command.\nYou can put any text content into your pager - try this:\nls -al /usr/bin | less This lists the contents of the /usr/bin folder, piping the output to less so we can easily scroll through it.\nThere are alternative pagers available (on many Unix-y systems you'll have less, more and most) but in general you'll normally get what you need with less.\nThe Alternative - Help Sometimes you'll look something up in the manual and get the \u0026lsquo;builtins\u0026rsquo; page. For example:\n$ man cd BUILTIN(1) BSD General Commands Manual BUILTIN(1) NAME builtin, !, %, ., :, @, {, }, alias, alloc, bg, bind, bindkey, break, breaksw, builtins, case, cd, chdir, command, complete, continue, # (I\u0026#39;ve skipped the bulk of the output to save space!) This happens when the command you are looking up is not actually a program with a manual page, but a built-in shell command. Most shells have a way get help on such commands - bash for example has help:\n$ help cd cd: cd [-L|[-P [-e]] [-@]] [dir] Change the shell working directory. Change the current directory to DIR. The default DIR is the value of the HOME shell variable. # (I\u0026#39;ve skipped the bulk of the output to save space!) This is all I'll say about help for now. We visit it again in [Chapter 10 - Understanding Commands](/docs/part-2-core-skills/understanding-commands/, where we talk more about built-in commands. For now we'll go back to the man command, which works across all shells as it is a Linux feature rather than a shell specific feature!\nManual Sections You'll often see tools referred to in manpages with numbers after them. Take a look at man less:\nThe number is the manual Section Number. The different sections of the manual are documented be found on most Unix-like systems in man's documentation, which you can check by running man man1. Here's what you'd get on Ubuntu 16:\n Section 1 - Executable programs or shell commands Section 2 - System calls (functions provided by the kernel) Section 3 - Library calls (functions within program libraries) Section 4 - Special files (usually found in /dev) Section 5 - File formats and conventions (e.g. /etc/passwd) Section 6 - Games Section 7 - Miscellaneous (including macro packages and conventions), e.g. man(7), groff(7) Section 8 - System administration commands (usually only for root) Section 9 - Kernel routines (Non standard)  Not all of these explanations will be entirely clear to everyone, so we'll go through the sections in detail shortly.\nIf you want to, you can specifically choose which section of the manual you are looking in by using:\nman \u0026lt;section\u0026gt; \u0026lt;search\u0026gt; You can also get more information about the sections themselves by opening up the intro page. For example:\n$ man 1 intro INTRO(1) BSD General Commands Manual INTRO(1) NAME intro -- introduction to general commands (tools and utilities) DESCRIPTION Section one of the manual contains most of the commands which comprise... Why would you do this, and why would you care? In general you won't need to worry about the sections unless you are looking for something which has an entry in multiple sections and you want to specify which one you use.\nAnother reason it is useful to know about the sections is that a lot of documentation (online and offline) includes a section number after the name of a command or file. Knowing what the section is can be useful in this case.\nHere are a few examples of entries from each section, which illustrate what each section is for.\nSection 1: Programs and Shell Commands These are programs - probably what you are going to be looking up most regularly! For example, man 1 time shows:\nTIME(1) BSD General Commands Manual TIME(1) NAME time -- time command execution SYNOPSIS time [-lp] utility DESCRIPTION The time utility executes and times utility. After the utility finishes, time writes the total time elapsed, the time consumed by system overhead, and the time used to execute utility to the standard error stream. Times are reported in seconds. ... Section 2: System Calls You'll probably not use this section unless you are doing systems programming2. This section contains info on the available Linux Kernel system calls. For example, running man 2 chown gives:\nCHOWN(2) BSD System Calls Manual CHOWN(2) NAME chown, fchown, lchown, fchownat -- change owner and group of a file SYNOPSIS #include \u0026lt;unistd.h\u0026gt; int chown(const char *path, uid_t owner, gid_t group); ... This entry shows you how you would call the function if you were programming for the Kernel.\nSection 3: Library Calls These are the manpages for the C standard library functions. For example, man 3 time:\nTIME(3) BSD Library Functions Manual TIME(3) NAME time -- get time of day LIBRARY Standard C Library (libc, -lc) SYNOPSIS #include \u0026lt;time.h\u0026gt; time_t time(time_t *tloc); ... You would use this information if you were writing programs to run on the system.\nHere we can see why the sections are important to know about. There are multiple entries for time. We need to use the sections to differentiate between them.\nRunning man time would not open the page above, because man searches the library in ascending section order, meaning that it actually finds time(1) and shows the pages for the time program, not the time C library call.\nBecause of the potential ambiguity of names if no section number is included, in lots of Linux documentation you'll see the man section number written next to library calls, system calls, programs and so on (things will refer to sed(1) or time(3) for example.\nSection 4: Devices This section deals with the special devices which live in the /dev/* folder. For example, running man 4 random shows:\nRANDOM(4) BSD Kernel Interfaces Manual RANDOM(4) NAME random , urandom -- random data source devices. SYNOPSIS pseudo-device random DESCRIPTION The random device produces uniformly distributed random byte values of potentially high quality. ... Again, we see that section numbers can be important. If you just run man random, you'll see:\nRANDOM(3) BSD Library Functions Manual RANDOM(3) NAME initstate, random, setstate, srandom, srandomdev -- better random num- ber generator; routines for changing generators LIBRARY Standard C Library (libc, -lc) SYNOPSIS #include \u0026lt;stdlib.h\u0026gt; char * initstate(unsigned seed, char *state, size_t size); long random(void); ... Which is the manpage for random(3), which is C library function, not the /dev/random file!\nWe'll see more of these special files later in the book.\nSection 5: File Formats This section details special files in the system. For example, man 5 crontab shows:\nCRONTAB(5) BSD File Formats Manual CRONTAB(5) NAME crontab -- tables for driving cron DESCRIPTION A crontab file contains instructions to the cron(8) daemon of the gen- eral form: ``run this command at this time on this date''. Each user has their own crontab, and commands in any given crontab will be exe- cuted as the user who owns the crontab. Uucp and News will usually have their own crontabs, eliminating the need for explicitly running su(1) as part of a cron command. ... Which describes the crontab file used to define scheduled tasks. Again, this is different to man crontab which would document crontab(1). Similarly, man 5 passwd is going to show something quite different to man passwd.\nYou'll potentially use this section if you are performing system administration.\nSection 6: Games Nothing says it better than man 6 intro itself (this'll not work on a Mac sadly, but try it on another Linux system):\n... DESCRIPTION Section 6 of the manual describes all the games and funny little programs available on the system. ... There are probably a few silly programs available on your system, here you'll find their manuals. For example, man 6 banner on a Mac shows:\nBANNER(6) BSD Games Manual BANNER(6) NAME banner -- print large banner on printer SYNOPSIS banner [-d] [-t] [-w width] message ... DESCRIPTION Banner prints a large, high quality banner on the standard output. If the message is omitted, it prompts for and reads one line of its stan- dard input. ... This section is going to be highly dependent on your operating system!\nSection 7: Miscellaneous This is where you'll find additional assorted documentation. For example, man 7 ascii shows:\nASCII(7) BSD Miscellaneous Information Manual ASCII(7) NAME ascii -- octal, hexadecimal and decimal ASCII character sets DESCRIPTION The octal set: 000 nul 001 soh 002 stx 003 etx 004 eot 005 enq 006 ack 007 bel ... Section 8: System Commands We've actually already seen one of these commands mentioned, in the manpage for crontab(5) it mentions cron(8). Let's see, with man 8 cron:\nCRON(8) BSD System Manager's Manual CRON(8) NAME cron -- daemon to execute scheduled commands (Vixie Cron) SYNOPSIS cron [-s] [-o] [-x debugflag[,...]] These are commands which system administrators would normally run. You might open section eight unexpectedly, for example man chmod will open chmod(1), but man chown will open chown(8), as it is a system command.\nSome distributions might vary for section nine. On my Mac it contains information about the kernel interfaces, a C style guide and some more.\nGetting the Index of Manual Section Manpages are just files on the filesystem, so you can get the index of a section just by looking in the appropriate folder.\nFor example, to index the available system calls, try ls /usr/share/man/man2:\nEV_SET.2 FD_CLR.2 FD_COPY.2 FD_ISSET.2 FD_SET.2 FD_ZERO.2 _exit.2 accept.2 access.2 acct.2 ... This is quick and easy way to see what sort of entries you have on your system. If you want to work out where an entry lives, use the -w flag:\n$ man -w printf /usr/share/man/man1/printf.1 There are other ways to show the index of each section, but they vary a lot from system to system so showing the actual files is probably easier.\nSearching the Manual You can search the manpage titles and summaries with man -k. For example, man -k cpu shows:\ncpuwalk.d(1m) - Measure which CPUs a process runs on. Uses DTrace dispqlen.d(1m) - dispatcher queue length by CPU. Uses DTrace gasm(n), grammar::me::cpu::gasm(n) - ME assembler You can find more advanced options for searching by using your newfound man skills on man itself.\nYou can also use the apropos or whatis commands to search through the manuals. However, for simplicity I suggest just remember man -k!\nIntroducing tl;dr In general for this book I'm trying to avoid suggesting too many non-standard tools which don't come pre-installed on systems. However, this one is just too good to miss!\nLet's say I need to find and replace some text in a file. I know I can do this with the sed command, but have forgotten the syntax. So I run man sed:\nWow, that's a lot of detail! And this is just page one of six!\nNow let's compare this to the output from tldr (which is short for \u0026ldquo;Too Long, Didn't Read\u0026rdquo;). All I need to do is run tldr sed:\nThe first example is exactly what I'm looking for. Now for any more detail than a few basic examples, I'm going to have to go to the manual, but for the basics this is great.\nYou can install the tldr tool with npm install -g tldr. It's open source and community maintained. You will need Node.js installed to install the tool, the instructions are available online.\nI'd recommend tldr as a first-call for checking to see how to use a command.\nThe Online Cheatsheet One final resource which I think is worth sharing is the website www.cheat.sh. This is a fantastic online collection of \u0026lsquo;cheat sheets\u0026rsquo;.\nThese sheets cover almost all of the tools you will encounter, programming languages and more. But the real beauty of the tool is how it integrates into the shell. To see what I mean, just run the following command:\n$ curl cht.sh You will see something like this:\nThe curl command we'll see again and again. It is a tool which lets you download content from the web. If we load the cheat.sh website (or its shortened version, cht.sh) from the shell, we get a text version of the website. We can now look at all sorts of content by following the guide shown.\nThe Cheat.sh site aggregates many data sources - including tldr! This means we can get information on tools without even having to install a tool like tldr locally.\nThis online cheatsheet is a wonderful resource. As well as guides for specific tools, there are entire courses on programming languages. You can even use it to search for the answers to questions, these features are powered by Stack Overflow. For example:\n$ curl cht.sh/\u0026#34;How do I copy a folder in bash?\u0026#34; You'll see something like this:\nNow that can be a real time saver!\nSummary In this chapter we looked at some of the ways we can get help. To quickly summarise:\n The man tool can be used to look at the manual page for a topic The man pages are grouped into sections, we can see them with man man The tldr tool shows a very short description of a tool, which covers the most common use cases only The cht.sh website can be used directly from the shell to get help on tools or even ask specific questions  Footnotes   Weirdly satisfying to run. \u0026#x21a9;\u0026#xfe0e;\n Which it is always fun to try if you get the chance, and a great way to learn more about the fundamentals of the operating system. \u0026#x21a9;\u0026#xfe0e;\n   "});index.add({'id':6,'href':'/docs/part-1-transitioning-to-the-shell/the-renaissance-of-the-shell/','title':"The Renaissance of the Shell",'content':"Chapter 6 - Interlude - The Renaissance of the Shell This is the first of the \u0026ldquo;interludes\u0026rdquo; which end each section of the book. They don't teach any specific skills but instead give a little flavour and background about the world of the shell, Linux and modern computing.\nIn this first interlude we'll look at just why the shell is experiencing something of a renaissance in the modern age of IT.\nIs there a Renaissance of the Shell? To be honest, it is hard to know whether there is an increase in popularity of the use of the shell and command-line tooling in general. There are data sources which indicate there is more widespread usage amongst the technical community - Stack Overflow tag popularity is one. LinkedIn data on desired skillsets is another. However, disassociating whether there is a general increase in the need for diverse technical skillsets and whether there is a specific increase in the popularity of keyboard and script operated systems is a challenge.\nFor the purposes of this chapter, we'll instead examine changes in the technology landscape over the last few decades and consider what those changes might mean for the shell, the command-line and similar tools.\nWe'll look at three specific developments in technology:\n Diversity of programming languages Convergence of operating platforms DevOps  Each of these developments has a potentially profound impact on how we work with computers, and might hint at the long term need for shell skills.\nThe Changing Technology Landscape So let's look at some of the key changes in the technology landscape over recent years and consider how they might affect the popularity and importance of the shell.\nThe Diversity of Programming Languages There have been many programming languages and platforms over the years. But in recent years it is possible that the diversity has increased at a greater rate than ever before.\nWith the advent of the internet and the increase in the size of the online technical community, programming has in a sense become more democratised (which we will discuss a little more in the \u0026lsquo;citizen coder\u0026rsquo; section). When in the past it was necessary to find physical books or teachers and tutors to learn a programming language, students can now find a wealth of resources online.\nIt is perhaps this democratisation which has led to a startlingly diverse world of programming languages. For many years, there were a small number of \u0026lsquo;general purpose\u0026rsquo; languages, and a larger number of highly specialised languages (and associated platforms).\n\u0026ldquo;C\u0026rdquo;, and later, \u0026ldquo;C++\u0026rdquo; were the go-to languages for systems programming (sometimes backed up by assembly language). This was the language which kernels and compilers were written in.\n\u0026ldquo;Java\u0026rdquo; become the \u0026lsquo;general purpose\u0026rsquo; language of choice for applications which had to run on many systems. \u0026ldquo;Basic\u0026rdquo; and later \u0026ldquo;C#\u0026rdquo; were the standards for Windows platform development. PHP was a staple for web development.\nAlongside these giants were the workhorses for specific use cases. Erlang was (and is) a language which is highly popular in the telecommunications industry, where high availability and reliability were paramount. COBOL was the language for the financial industry, where mission critical systems ran on mainframes (and many still do).\nOf course there were many other languages, but many of these other languages were highly specific, in a sense C, Java, PHP and later C# dominated the landscape.\nTransition to the time of writing. In the Stack Overflow 2020 Technology Survey1, the top ten languages most wanted by employers are:\n Python JavaScript Go TypeScript Rust Kotlin Java C++ SQL C#  Some of our old friends are there, but there are many new languages, languages which are evolving quickly. Later on in the list we will see Swift, Dart, Ruby, Haskell, Scala. There are many programming languages which are extremely popular today.\nWhy does this matter for the shell? The answer is that for many new languages, developer tooling is not as mature (some might say bloated) as it is for the \u0026lsquo;Workhorse\u0026rsquo; languages. Java developers are likely very familiar with the Eclipse IDE, Microsoft shops will be familiar with Visual Studio. These are products which have been evolving for years (or decades) to support developers with rich integrated development environments.\nFor server-side JavaScript, Golang, Rust, Python and other languages, the development environment really is the shell. Modern editors like Visual Studio Code, Atom and so on provide a vast amount of support and tooling, encompassing the features of a full fledged IDE if the user wants. But for modern programming languages, users often have had to rely on the shell to compile, transpile, manage packages, bundle and so on. The average developer today is perhaps much more likely to have to use the shell - to manage Python virtual environments one day, to run Node.js another, to install packages for Golang another.\nIn time tooling will likely catch up and provide a \u0026lsquo;friendly\u0026rsquo; interface on top of these operations, but many engineers have realised (or always known) that direct access to simple command line tools can be extremely efficient when working, and that overly featured IDEs can get in the way and hide complexity.\nThe modern programming is often polyglot - having to be at least familiar in a number of languages. The shell provides a common environment and interface for tooling, which is accessible by all, without installing many complex components, for both development and runtime environments.\nConvergence of Operating Platforms Whilst the variety in programming languages and developer tooling may have increased, in many ways the operating platforms engineers use have become more homogeneous.\nIn the early days of computing, each operating environment was highly diverse. There were many systems which were used for production and many of them were highly proprietary. Even popular application servers were often closed source and highly specialised.\nThe modern execution environment however is often fairly uniform. A Linux-like system, with few customisations, which the developer or operator can tweak to suit their needs.\nMore and more enterprise users have moved away from proprietary Unix platforms to Linux platforms (whether commercial or non-commercial). The earliest cloud environments were using open-source Linux distributions as the available operating systems.\nEven Windows has increasing support for Linux-like operation, in the form of the Windows Subsystem for Linux.\nPerhaps the greatest movement in this area has been the rapid adoption of Docker as a common container technology. Containers, or container-like systems have been around for a long time, but Docker brought containers to the masses. With Docker, engineers expect operating environments to be even more uniform and Linux-like.\nThis has made knowledge of the shell extremely valuable. For any containerised workloads, Linux and shell skills are crucial. Kubernetes (as an execution environment) has standardised things even more.\nWhilst there are still many workloads which run on proprietary systems, modern solutions are often built to run in containers on Linux. The shell has historically been the most common way to manage Linux systems, and the standardisation of operating environments around Linux, or Linux-like systems has made shell skills even more critical.\nDevOps Love it or hate it, DevOps has exploded in popularity. DevOps engineers, site-reliability engineers, these kinds of roles may have been unheard of in companies not that long ago and are now becoming ubiquitous.\nIn attempting to unify the goals of development and operation of software, DevOps represents an organisational and cultural change. Rather than having one group focus on feature development and another group focus on reliable software operations, a single group is responsible for both. The theory is that this encourages software engineers to also consider security, reliability, maintainability etc, and operators to also consider speed of delivery.\nRegardless of whether teams are genuinely combined, or specialised roles are added to teams, or even if teams are still separated, the lines between development and operations blur somewhat. Software developers are expected to build and plan with knowledge of the execution environment, operators are expected to work with developers to build features which support reliability.\nThe intersection of these two roles often is in the realm of automation. Automated deployments after testing, automated failover in case of errors, automated alerting when potential issues are discovered, automated provisioning of environments, automated scaling of systems when load increases.\nThe world of automation is intimately linked to the world of the shell and in particular shell scripting. Many tasks which require automation can be easily achieved using shell scripts. Many aspects of modern environments (such as cloud environments) support provisioning and management of services via scripting. In fact, services which cannot be managed via shell scripts or simple interfaces are increasingly becoming obsolete. If it cannot be scripted, it cannot be automated, and the increasingly complex systems we build require automation.\nIn practice, this means software engineers are far more likely to have to build shell scripts (or at least understand how to interface with systems via the shell) than they perhaps might have been. Similarly, operators are far more likely to have to program automated routines to manage high availability and so on. Again, the shell and shell scripts are a common way to manage this (even if they are simply entrypoints to more complex systems, such as scripts which execute programs).\nThe rise in popularity of DevOps as a set of practices and beliefs has perhaps made the shell more popular, and more important, than any other recent developments in software engineering.\nAnd for these reasons and many more, learning how to use the shell effectively has never been more relevant or practical.\n Footnotes\n  https://insights.stackoverflow.com/survey/2020 \u0026#x21a9;\u0026#xfe0e;\n   "});index.add({'id':7,'href':'/docs/part-2-core-skills/thinking-in-pipelines/','title':"Thinking in Pipelines",'content':"Chapter 7 - Thinking in Pipelines Understanding the concept of pipelines in the shell, as well as how input and output work for command line programs is critical to be able to use the shell effectively.\nIn this chapter, we'll look at the ways programs handle input and output, then we'll look at how we can chain multiple commands together with pipelines. We'll also look at some really common ways to use pipelines which should hopefully make your life easier!\nWhen you understand these concepts, it will open up a new world in terms of what you can do with in the shell. We'll briefly touch on the \u0026lsquo;The Unix Philosophy\u0026rsquo;, which is a concept which allows us to perform highly complex tasks by composing together small, simple components.\nInput and Output Many of the programs we have seen so far follow a very similar pattern:\nProgram - Output\" width=\"480px\" /\nIn fact, when you get down to the details, there are very few programs which don't do something like this! As a more concrete example, we can look at the sort program - which sorts the input in alphabetic order:\nWe can easily see this in action by just running sort in a shell. Start the sort program, enter some text, then press Ctrl+D. Ctrl+D (which is normally written as ^D is a special control character which means \u0026lsquo;end of transmission\u0026rsquo; - in this case we use it to tell sort we've finished writing text. If you were to use ^C (which is the interrupt command) it would closes the sort program instead).\n$ sort ogs chase cats and cats chase mice Once you've entered the text you want to sort, hit ^D and you'll see the sorted output:\nDogs and cats cats chase chase mice So by default, the sort command is reading input from the keyboard (until we send it a special message saying we're done), then writing the output to the terminal.\nIn fact, sort is using two special files - stdin and stdout - but what does this mean?\nStanding Input, Output and Error Every program has access to three \u0026lsquo;special\u0026rsquo; files, stdin, stdout and stderr:\n stdin is short for \u0026lsquo;standard input\u0026rsquo; - it's where many programs read their input from stdout is short for \u0026lsquo;standard output\u0026rsquo; - it's where many programs write their output to stderr is short for \u0026lsquo;standard error\u0026rsquo; - it's where some programs write error messages to  Why do I say \u0026lsquo;many\u0026rsquo; and \u0026lsquo;some\u0026rsquo;? Well the reason is that while this is convention, it is not adhered to universally. Anyone who writes a program is free to choose how they read input and write output, so some programs might not follow these conventions. In Chapter 29 we'll look at how to write tools which follow these conventions, as well as others which are useful.\nEach of these files has a special number which is shown in grey in the diagram. This is known as the file descriptor and we'll see it later on. Each of these files also has a special location in the system which you can access directly - you can see these files by running ls -al /dev/std*:\n$ ls -al /dev/std* lr-xr-xr-x 1 root wheel 0 Jan 1 1970 /dev/stderr -\u0026gt; fd/2 lr-xr-xr-x 1 root wheel 0 Jan 1 1970 /dev/stdin -\u0026gt; fd/0 lr-xr-xr-x 1 root wheel 0 Jan 1 1970 /dev/stdout -\u0026gt; fd/1 If you are not familiar with ls (the list directory contents command) then check Chapter 2 - Navigating Your System. The first part of the output isn't too important - but we can see we have three files in the special /dev/ (short for device folder). We can also see the associated file descriptors.\nAs an aside - this is a really fundamental thing we'll see again and again in Unix and Linux - almost everything can be represented as a file. This is a core concept and one we'll touch on regularly.\nWhen you are running programs in a shell, the shell attaches your keyboard to the program's standard input1, and attaches the standard output and standard error to the terminal display:\nThis means when we're in a shell, we can type on the keyboard, which goes to the input of the program and then as the program outputs information and errors they show up on the screen.\nWe can already see the beginnings of a pipeline here. There's a clear flow of data from the keyboard, through the stdin file, through the program, then through the output files, then to the display.\nLooking at some real programs in action will hopefully make this clearer!\nA Pipeline in Action Do you remember the cat command? It's the one which writes the contents of a file to the screen. For example:\n$ cat ~/playground/text/simpsons-characters.txt Artie Ziff Kirk Van Houten Timothy Lovejoy Artie Ziff Nick Riviera Seymore Skinner Hank Scorpio Timothy Lovejoy John Frink Cletus Spuckler Ruth Powers Artie Ziff Agnes Skinner Helen Lovejoy We saw in Chapter 4 - Becoming a Clipboard Gymnast that we could pipe the output of this command into the sort command to order it and then into the uniq command to remove duplicates, like this:\n$ cat ~/playground/text/simpsons-characters.txt | sort | uniq Agnes Skinner Artie Ziff Cletus Spuckler Hank Scorpio Helen Lovejoy John Frink Kirk Van Houten Nick Riviera Ruth Powers Seymore Skinner Timothy Lovejoy The pipe operator (which is the vertical pipe symbol or |) has a very specific meaning in the shell - it attaches the stdout of the first program to the stdin of the second. This means we can now visualise the entire pipeline and see exactly what is going on:\nThat's it! If you can follow what is going on here then you have the key information you need to know to understand how pipelines work. The pipe operator just connects the output of one program to the input of another. A pipeline is just a set of connected programs. Easy!\nWe could do the same thing by writing the output of each step as a file, then reading that file with the next step, but that would mean we'd have a lot of intermediate files to clean up (and if we're processing a big file, it also uses a lot of space). Pipelines let us create complex sequences of operations which work well even on very large files.\nNow we'll look at stdin, stdout and stderr in a little more detail. We'll be seeing these special streams a lot as we go through the book. Knowing more about them is really going to help you when working in the shell or with Linux-like systems.\nCommon Patterns - Standard Input Let's have a quick look at some of the common things we might see as sources of inputs for other programs. Each one illustrates an interesting point about how the shell or the standard input stream works.\nThis list is by no means exhaustive, in fact with a bit of tinkering you can make almost anything the input to anything else, but let's check each example.\nThese examples will use some new programs to transform the output - don't worry about the details of them, each will be described as we go through the book!\nThe Shell\nYou might just use code in the shell as input, for example:\n$ echo \u0026quot;I am in the $PWD folder\u0026quot; | sed 's/folder/directory/' I am in the /Users/dwmkerr/repos/github/dwmkerr/effective-shell directory Here we've just used echo to write out message including a variable and then used the sed (stream editor) program to replace the word folder with directory. We'll get a lot of practice with sd as we go through this book!\nFiles\nWe've already seen a few examples of using cat to write a file to stdout.\nA lot of the time we don't need to use cat many programs accept the path of a file as a parameter, meaning we can just tell the program to open the file directly. For example, we could count the number of lines in a file like this:\n$ cat ~/playground/text/simpsons-characters.txt | wc -l 14 Or more simply, like this:\n$ wc -l ~/playground/text/simpsons-characters.txt 14 /Users/dwmkerr/playground/text/simpsons-characters.txt In this case, we've passed the file path as an argument to the wc (word, line, character and byte count) program. But be aware - not all programs use the same convention or parameter names!\nNow here's a cool trick. Type rev \u0026lt; /dev/stdin, then enter some text and hit ^D or ^C when done. You should see something like this:\n$ rev \u0026lt; /dev/stdin Red Rum muR deR What's going on here? Remember we mentioned that stdin is a special stream which represents input and that it lives at /dev/stdin? This little trick uses redirection to redirect the stdin file to the rev (reverse) command.\nThe \u0026lt; operator redirects the standard input of a program to come from the given file. We could also have written cat /dev/stdin | rev. Or just enter rev and type in the input we want to reverse!\nThe Clipboard\nIn Chapter 4 - Becoming a Clipboard Gymnast we saw a trick to remove formatting from text in the clipboard. Here's a similar trick to reverse the contents of the clipboard:\n$ pbpaste | rev | pbcopy This pipeline pastes the contents of the clipboard to stdout, which is piped to rev (reversing the text) and then pipes the output to pbcopy, which copies the results to the clipboard2.\nFiltered Input\nThis is a trick a friend shared with me. He works with data scientists and whenever he shows them this command they love it!\n$ head -n 100 100GBFile.csv \u0026gt; 100linefile.csv The head (display first lines of a file) command in this case just grabs the first 100 lines of a file and puts it straight into a smaller, more manageable file. We'll see what the \u0026gt; symbol (the redirection symbol) means in the section lower down on Standard Output.\nYou can also use tail in the same way to get the last lines from a file. And if you are a more advanced user, you might use something like this:\n$ grep -C 5 error /var/log/mylogfile.txt | less We'll see all of these commands as we go through the book, but this very cool trick uses the grep (file pattern searcher) command to search for the text error in the file /var/log/mylogfile.txt, shows five lines of context (-C 5), which are the lines before and after the match, then puts the result into your pager! We'll see the pager just below. We'll do a lot of grep-ing as we go through the book so don't worry if this looks a little confusing for now.\nMany More!\nWe've only scratched the surface - almost any program will write to the standard output, meaning it can be the input for any pipeline you can imagine!\nCommon Patterns - Standard Output Now let's look at some of the things we can do with the standard output:\nSome of these outputs are things we've seen before, but let's do a quick revision.\nDisplay\nThis is what we've been doing a lot of so far. When you are working with the shell interactively this makes a lot of sense.\nIf you have jobs which run in the background (or on a timer, such as backup jobs which run nightly), you might not actually have a terminal attached to the program to see the output, in which case you'll likely write to a file.\nWhat about if you have a lot of output? It can be quite inconvenient to have to scroll through the terminal (or impossible, depending on the system you are on). In this case use a pager. A pager is a program which makes it possible to interactively page through output in the shell, scrolling up and down, searching and so on.\nTry this out as an example:\nls /usr/bin /usr/local/bin /usr/sbin | less You'll see something like this:\nThis long list of files would be hard to search through if it was printed directly to the shell, but in the pager we can use the d and u keys to go down and up, or the / and ? keys to search forwards or backwards.\nPiping into your pager is a really useful trick - you can read more about pagers in Chapter 5 - Getting Help.\nFile\nThe shell has a built in operator which will pipe the standard output of a program and write it to a file. It is the \u0026gt; or redirection operator:\n$ echo \u0026quot;Here's some data\u0026quot; \u0026gt; some_file.txt It's as easy as that! Note that this will overwrite anything already in the file.\nAppend\nWhat if you don't want to overwrite a file, but instead just add a new line? The \u0026gt;\u0026gt; or append redirection operator:\n$ echo \u0026quot;Tuesday was good\u0026quot; \u0026gt;\u0026gt; diary.txt $ echo \u0026quot;Wednesday was better!\u0026quot; \u0026gt;\u0026gt; diary.txt $ echo \u0026quot;Thursday suuucks\u0026quot; \u0026gt;\u0026gt; diary.txt $ cat diary.txt Tuesday was good Wednesday was better! Thursday suuucks This example writes each line in turn to the diary.txt file, appending the text to the end of the file (and creating it if it doesn't already exist).\nAppending to a file is extremely useful for circumstances where you might want to build or update a log of events over time.\nPipe\nThis is what we've spent most of this chapter looking at - to simply pipe the standard output to the standard input of another program!\nIn this case, the output of our program becomes the input of the next one in the pipeline.\nCommon Patterns - Standard Error We haven't actually seen stderr in action yet. Let's see how it works.\n$ mkdir ~/playground/new-folder $ mkdir ~/playground/new-folder mkdir: /Users/dwmkerr/playground/new-folder: File exists In the first call to mkdir, the folder is created successfully. In the second call, we get an error. Now let's try and use this output and make it louder - making all of the text uppercase.\nThere are lots of ways to make text uppercase in the shell, let's use the tr (translate characters) program. Here's an example of how it works:\n$ echo 'Be quiet, this is a library!' | tr '[:lower:]' '[:upper:]' BE QUIET, THIS IS A LIBRARY! Now let's use it to shout out our error message:\n$ mkdir ~/playground/new-folder | tr '[:lower:]' '[:upper:]' mkdir: /Users/dwmkerr/playground/new-folder: File exists In this case the output has not been made uppercase. What's going on?\nTo understand, let's quickly review the three streams:\nWhen we are in the shell, the shell automatically writes the stderr stream to the screen. But the shell's pipe operator pipes stdout only - it is not piping our error output. And the mkdir command is writing this error message to stderr.\nThe command we ran before:\nmkdir ~/playground/new-folder | tr '[:lower:]' '[:upper:]' Actually looks like this:\nThe pipe has piped the standard output to the tr program. But there is no standard output - the error message was written to standard error instead. The shell has still written it to the screen for us, but has not piped it to the tr program.\nSo how do we deal with stderr? Here are some common options:\nNow this might be the ah-ha! moment if you have done some shell scripting before - some of these obscure sequences like 2\u0026gt;\u0026amp;1 might look familiar (even if it is just the thing you know you you always have to Google to get right!).\nLet's take a quick look at some of these options.\nTo Standard Output\nIf we want to be able to pipe the error message to another command, we can use another redirection trick - we can redirect stderr to stdout.\nThe characters 2\u0026gt;\u0026amp;1 look really obscure - let's break it down:\n Take the file with descriptor 2 - which is standard error Redirect it with the redirect symbol \u0026gt; - we saw this in the earlier section Redirect it into the file with descriptor (\u0026amp;) 1 - which is standard output  Remember, there are three \u0026lsquo;magic\u0026rsquo; files each process has access to:\n stdin, the standard input, which has the file descriptor 0 stdout, the standard output, which has the file descriptor 1 stderr, the standard error, which has the file descriptor 2  File descriptors are just numbers the operating system uses to keep track of files. When a program opens a \u0026lsquo;normal\u0026rsquo; file, it'll get a new file descriptor. Here's a little example:\npython \u0026lt;\u0026lt;EOF import os for r in range(3): print(os.open(\u0026#39;/dev/random\u0026#39;, os.O_RDONLY)) EOF This code uses redirection (see how useful it is?) to pipe a small Python script into the Python program, which writes the results to stdout. You will probably see the following output:\n3 4 5 It doesn't really matter whether you know Python or not (and there weird looking EOF is a heredoc which we have a whole chapter on later). The script is just a way of showing the file descriptors that the operating system gives me when I try to open three files (each time I open the same file, the magic /dev/random file which just contains random data).\nThe interesting thing is that the descriptors in my program start from 3 and go upwards - that's because 0, 1 and 2 are already in use, for stdin, stdout and stderr!\nSo to make our error message go through the tr command, we can redirect stderr to stdout, which means the error message will go to stdout and then be piped to tr:\n$ mkdir ~/playground/new-folder 2\u0026gt;\u0026amp;1 | tr '[:lower:]' '[:upper:]' MKDIR: /USERS/DWMKERR/PLAYGROUND/NEW-FOLDER: FILE EXISTS Visually, what is happening is this:\nIf you can wrap your head around this, the other options we showed for stderr might start to make a little more sense.\nA nice trick to remember the slightly obscure ampersand \u0026amp; which references a file descriptor - if you were to write this:\ncat some-file-that-might-not-exist 2\u0026gt;1 What would happen is that the shell would write stderr to a new file with the name 1! Why don't we need an ampersand before the \u0026gt; symbol, only for the file descriptor afterwards? This is just because the shell only supports redirecting file descriptors, so an additional ampersand would be superfluous.\nTo a File\nBefore, we redirected to \u0026amp;2, which is \u0026lsquo;the file with descriptor 2. We can also use a similar trick to redirect to any arbitrary file:\nmkdir ~/playground/new-folder 2\u0026gt;./errors.txt This command just redirects all of the errors (remember, 2 is stderr) to a file called ./errors.txt.\nThis is quite a common trick - run the program, but log the errors to a file for later review.\nTo Nowhere\nWhat if we just don't want to see the errors at all? Well there's a special file called /dev/null which we can use for this. When we write to this file, the operating system just discards the input. In fact, it exists for just this kind of purpose!\nmkdir ~/playground/new-folder 2\u0026gt;/dev/null This just redirects all errors to the black hole of /dev/null - we won't see them on the screen or anywhere else. This is a common way to \u0026lsquo;silence\u0026rsquo; errors3 in shell commands.\nNotice how we're starting to see patterns? This is just redirection, the same tricks we saw for stdout, but we're explicitly redirecting stderr (file descriptor 2). If we don't tell the shell what to redirect, it assumes stdout by default.\nSo if we can redirect, can we append too?\nAppend\nYes! Just like we did with stdout, there's nothing stopping us appending to a file:\nmkdir ~/playground/new-folder 2\u0026gt;\u0026gt;./all-errors.log Just like before, we use \u0026gt;\u0026gt; which means append (rather than overwrite or create).\nAll to a File\nThis is a really important subtlety. If you want to write both stdout and stderr to a file, you might try this:\nls /usr/bin /nothing 2\u0026gt;\u0026amp;1 \u0026gt; all-output.txt If you run this command, you'll get stdout written to all-output.txt, but the error message cannot access '/nothing' is written to the screen, not the file. Why is this?\nBash (and most bash-like shells) process redirections from left to right, and when we redirect we duplicate the source. So breaking this down:\n 2\u0026gt;\u0026amp;1 - duplicate file descriptor 2 (stderr) and write it to 1 - which is currently the terminal! \u0026gt; all-output.txt - duplicate file descriptor 1 (stdout) and write it to a file called all-output.txt  To write everything to the file, try do this:\nls /usr/bin /nothing \u0026gt; all-output.txt 2\u0026gt;\u0026amp;1 This will work. Breaking it down:\n Redirect stdout to the file all-output.txt Now redirect stderr to stdout - which by this point has already been redirected to a file  This can be tough to remember so it's worth trying it out4. There are many variations you can play with and we'll see more as we go through the book.\nOne Last Trick - The T Pipe This is a long chapter, but I can't talk about pipelines without briefly mentioning the T pipe. Check out this command:\ncat ~/playground/text/simpsons-characters.txt | sort | tee sorted.txt | uniq | grep \u0026#39;^A\u0026#39; This command sorts the list of Simpsons characters, removes duplicates and filters down to ones which start with the letter A. And it has the tee command in the middle. What does this do?\nWell the tee command is like a T-pipe in plumbing - it lets the stream of data go in two directions! The sorted.txt file contains the sets of characters after the sort operation, but before the unique and filter operation. Visually, it does this:\nAs soon as you visualise a T-pipe it's easy to remember this useful command! You might use it in more complex pipelines or other scenarios to write things to a file which would otherwise go straight to another program or just the display.\nThinking in Pipelines Once you get comfortable with pipelines, a whole world of possibilities open up.\nJust the day before I wrote this chapter, I had to find out how many unique data points were in a data file, which also included empty lines and comments, it took less than a minute to quickly build this:\ncat data.dat | sort | uniq | grep -v \u0026#39;^#\u0026#39; | wc -l I didn't have to find a special program which does exactly what I needed5 - I just incrementally built a pipeline. Each section I added one by one, writing to the screen each time, until I had it working. The thought process was:\n cat data.dat - OK, first I need to write out the file sort - now I can sort it, that'll put all the blank lines together uniq - this'll remove all of those duplicate blank lines, although it still leaves one blank one at the top! grep -v '^#' - this should get rid of all the lines which start with # wc -l - this'll count the number of lines I'm left with  Now there's probably better ways, and this has an oddity which is that if there are blank lines it'll remove all but one of them (although that would be quick to fix), but it gave me my quick and dirty answer in less than a minute.\nOf course, as things get more complex you might want to build scripts, or use a programming language, or other methods, but this Unix Philosophy (which we'll talk about more as we continue) of having lots of small, simple programs which we can chain together can be immensely powerful.\nSummary We'll see pipelines again and again. The standard streams, redirection, pipelines and all of the tricks we've introduced in this chapter are fundamental not only to using the shell effectively, but really understanding how computer programs work.\nDon't be worried if this feels like a lot to take in - we'll see more and more examples in later chapters which will help reinforce these concepts. If you find yourself struggling later you might want to quickly review this chapter, because we introduced a lot!\nIn this chapter we looked at:\n How each program has access to three \u0026lsquo;standard\u0026rsquo; streams - one for input, one for output and one for reporting errors The standard input stream is available as a file at /dev/stdin, is often called stdin in programming languages, and always has the special file descriptor 0 The standard output stream is available as a file at /dev/stdout, is often called stdout in programming languages, and always has the special file descriptor 1 The standard error stream is available as a file at /dev/stderr, is often called stderr in programming languages, and always has the special file descriptor 2 The Ctrl+D sequence means \u0026lsquo;end of transmission\u0026rsquo; - we can use it to signal that we have completed putting our input into stdin\u0026hellip; \u0026hellip;but the Ctrl+C sequence means \u0026lsquo;interrupt\u0026rsquo; and is normally used to force a program to close We can pipe the output of one program to the input of another with the pipe | symbol We can redirect a file to the standard input of a program with the \u0026lt; operator We can redirect the standard output of a program to create or overwrite a file with the \u0026gt; operator We can redirect the standard output of a program to create or append to a file with the \u0026gt;\u0026gt; operator We can redirect the standard error of a program to its standard output with 2\u0026gt;\u0026amp;1 We can redirect the standard error of a program to another file (such as the \u0026lsquo;null\u0026rsquo; file) with 2\u0026gt;/dev/null We can redirect the standard error of a program to create or append to a file, just like with standard output, using the \u0026gt;\u0026gt; operator  We also briefly saw some commands:\n sort sorts text sed can replace content in text tr can replace parts of text wc can count words or lines of text tee takes the input stream and sends it straight to the output, but also to a file (like a T-pipe in plumbing) grep can filter lines  These programs can do a lot more and are workhorses we'll see in more detail through the book.\nThere are a few chapters which are planned to come later which go into detail on some of the concepts we only briefly touched on:\n Writing Good Programs - How to write programs which use stdin, stdout and stderr sensibly The Unix Philosophy - Why we have so many small simple programs which we can pipe together Streams in Detail - How streams like stdin actually work, especially with things like line endings, command sequences like ^D and so on Signals - A little more on Signals (such as ^C and ^D)  When these chapters are published I'll update the links here. If you want to be updated when new chapters are published, you can Join the Mailing Lits on the Homepage.\nFootnotes\n  Technically there is another layer here, which is the tty. You can see this by running tty in the shell. We'll more about this in the Interlude - What is a Shell section. \u0026#x21a9;\u0026#xfe0e;\n Check Chapter 4 - Becoming a Clipboard Gymnast for how to do this on a Linux or Windows machine. \u0026#x21a9;\u0026#xfe0e;\n Although always use tricks like this with caution! If we had a different error, perhaps one we really do want to know about, we would lose the message in this case. \u0026#x21a9;\u0026#xfe0e;\n There is a very detailed explanation of this behaviour at https://linuxnewbieguide.org/21-and-understanding-other-shell-scripts-idioms/. \u0026#x21a9;\u0026#xfe0e;\n With the correct options, sed could likely do this in a single operation, but I'd probably spend a lot longer Googling the right options for it! \u0026#x21a9;\u0026#xfe0e;\n   "});index.add({'id':8,'href':'/docs/part-2-core-skills/fly-on-the-command-line/','title':"Fly on the Command Line",'content':"Chapter 8 - Fly on the Command Line This is my favourite chapter of the book! The tricks I picked up on rapidly moving around in the command line have saved me an enormous amount of time over the years.\nIn this chapter, we'll look at the ways you can rapidly move your cursor around on the command line, as well as how to easily open up the current command in an editor (which is incredibly useful if you realise you are building a more complex sequence of commands).\nBelow is a quick reference which you can use - we'll see each operation in more detail as we go through the chapter!\n\n Basic Navigation  Go to beginning / end Move backwards / forwards one word Delete a word or undo a mistake Delete the next word Delete to beginning / clear line Delete to end   Searching  Search Backwards / Forwards Run the command found in a search Edit the command found Stop Searching   Editing In-Place Clear the Screen See the History and Execute a Recent Command Pro Tip: All The Keys! Pro Tip: Transposing! The Power of Readline  Basic Navigation Let's assume we have a very simple command we are writing, which is going to write a quote to a text file:\necho \u0026#34;The trouble with writing fiction is that it has to make sense, whereas real life doesn\u0026#39;t. -- Iain M. Banks\u0026#34; \u0026gt;\u0026gt; quote.txt Navigating around long lines of text is a slow process if you are only relying on the arrow keys.\nLet's see how we can quickly move around and manipulate text!\nGo to beginning / end Quickly jump to the beginning or end of the text:\n Ctrl + a - Go to beginning Ctrl + e - Go to end  Move backwards / forwards one word For a little more fine-grained movement, you can jump backwards or forwards one word at a time:\n Alt + b - Go back one word Alt + f - Go forward one word  Delete a word or undo a mistake As this is the first operation we're seeing which changes the text, it is useful to remember how to undo any changes you make!\n Ctrl + w - Delete a word Ctrl + - - Undo most recent change  Delete the next word We've seen how to delete the word on or behind the cursor, now let's see how to delete the next word:\n Alt + d - Delete next word  Remember, just like any edit you can undo these changes with the Ctrl + - command.\nDelete to beginning / clear line In the Bash shell, you can delete all the way to the beginning of the line with Ctrl + u. However - if you are using the Z-Shell this will delete the entire line!\n Ctrl + u - Delete to beginning of line OR delete line  Delete to end You can also delete all of the way to the end of the line.\n Ctrl + k - Delete to end  When you find yourself repeatedly using the arrow or delete keys, refer back to this section to remind yourself of the shortcut - it will save a lot of time in the long run!\nSearching Once you have the basic navigation commands down, the next essential is searching. Let's assume we've run the following three commands:\n$ command1 param1 param2 param3 $ command2 param4 param5 param6 $ command3 param7 param8 param9 You can search backwards or forwards with Ctrl + r and Ctrl + s. This will search in the current line and then iteratively through previous lines:\nPeople often remember this as searching through the history - but remember that it actually searches the current line as well. So this is often the fastest way to move to the desired location in the current line.\nThis is useful for searching in the current command, but can be also used to quickly search backwards and forwards through the command history:\nAs you type, your command history is searched, the most recent commands coming first. Use the arrow keys to edit the command, press enter to execute it, or Ctrl + g to cancel the search.\nI think it's a little easier to see these commands in action with a more realistic example, so here's how they look with the text we used earlier.\nSearch Backwards / Forwards Search backwards or forwards through the current line and also the history:\n Ctrl + r - Search backwards (reverse search) Ctrl + s - Search forwards  Run the command found in a search This one is easy! Just hit Enter: Edit the command found When you have found the command or positioned the cursor where you want it, use the Left or Right arrow keys to stop searching and to go back into the normal editing mode:\nStop Searching Cancel the search and return back to the text as it was before you started with the Ctrl + g command:\nEditing In-Place These tips and tricks are helpful, but if you are working with a really long or complex command, you might find it useful just to jump into your favourite editor. This is one of those tricks that when you know it, you'll wonder how you lived without it!\nUse Ctrl + x , Ctrl + e to edit-in place, opening the current command line in your default editor:\nNow it's important to explain that this is the shell's default editor. This might not be the same as the default editor for your operating system. You can see what the shell is using as its default editor by printing the contents of the EDITOR environment variable. For example, my shell will show this:\n$ echo $EDITOR vim This means vim will be used to edit the command line. Your shell might use emacs or nano as a default editor. Unless you are familiar with vim or emacs, you might not find them particularly user friendly as an editor. You can change your default editor by setting the EDITOR variable. For example, below I set the editor to code (with the -w flag which tells the code program not to return control immediately back to the shell but instead wait until I've finished editing the file):\nNow this works (just about), but I wouldn't recommend using a Graphical Editor like Visual Studio Code for this. The reason is that because the editor runs in a separate window, it is actually easy to lose track of it (or the shell). You pause to take a short break, come back, close the editor and the contents are either lost or written to the shell (and if you see in the example above, the shell actually executed the command, rather than just putting it in the command line ready for me to execute).\nThe other reason to avoid a graphical editor is that if you are using a shell on another user's machine, the editor might not be present (or might be configured differently). In general however, the main reason to avoid a graphical editor is that it moves the context of the command away from where you are in the shell to another place, which can be confusing. If you see the screenshot below, I have two editors open:\nThe top right pane has my git commit command running (which is asking me to write a description of my changes) and the bottom right pane has the command line editor running (where I am testing out the commands for this chapter).\nIn this example, each editor has taken the place of the contents of the shell, so there's no ambiguity about which command I am editing. If I was to open a graphical editor, it would open multiple tabs for this operation and I'd have to track which tab was which.\nIt can be daunting to learn an editor like vim or emacs. Chapter 27 goes into more detail on the terminal based text editors - for now if you are not familiar with these programs I recommend you use the nano editor. Nano is small, simple and shows the shortcuts in a convenient menu at the bottom of the screen:\nIn Chapter 18 we'll see how to make permanent customisations to the shell, configuring things like the default editor.\nClear the Screen Probably the shortcut I use the most is Ctrl + l, which clears the screen without trashing your current command. Here's how it looks:\nThis is very helpful if you have a lot of noise and output on the screen and are ready to start with a fresh command.\nSee the History and Execute a Recent Command Just a few days ago a friend showed me a fantastic trick. If you run the history command, the shell will print the recent history of commands you have entered. But as an added bonus, you can execute any of these commands by entering an exclamation mark and the number next to the command:\nThe number is actually just the line number in the history file. Most shells maintain a history of commands which have been entered (to allow for things like searching through old commands). Where this history file is kept will depend on your shell, configuration and operating system, but in most cases you can find the file by running:\necho $HISTFILE There are many configuration options for the shell history. But the main thing to remember is that you can see recent history with the history command and quickly execute the command at line n by running !n.\nPro Tip: All The Keys! You can use the bindkey command to see a list of all keyboard shortcuts:\n$ bindkey \u0026quot;^@\u0026quot; set-mark-command \u0026quot;^A\u0026quot; beginning-of-line \u0026quot;^B\u0026quot; backward-char \u0026quot;^D\u0026quot; delete-char-or-list \u0026quot;^E\u0026quot; end-of-line \u0026quot;^F\u0026quot; forward-char \u0026quot;^G\u0026quot; send-break \u0026quot;^H\u0026quot; backward-delete-char \u0026quot;^I\u0026quot; expand-or-complete \u0026quot;^J\u0026quot; accept-line \u0026quot;^K\u0026quot; kill-line \u0026quot;^L\u0026quot; clear-screen ... This is an extremely useful command to use if you forget the specific keyboard shortcuts, or just want to see the shortcuts which are available.\nPro Tip: Transposing! If you've mastered all of the commands here and feel like adding something else to your repertoire, try this:\nThe Alt + t shortcut will transpose the last two words. Use Ctrl + t to transpose the last two letters:\nThese two commands were new to me when I was researching this chapter. I can't see myself ever being able to remember the commands more quickly than just deleting the last two words (Ctrl+w twice!) or characters and re-typing them, but perhaps you'll find them useful!\nThe Power of Readline All of the movement commands you've learned in this chapter apply to:\n Bash zsh The Python REPL The Node.js REPL  And many more! The reason is that all of these programs use the same library under the hood to control reading command line input. This library is called GNU Readline.\nIf you are ever looking to go deeper on this topic then search the web for GNU Readline. You can actually configure lower level details of how all programs which use readline work, with the .inputrc configuration file.\nThis configuration file can be used to configure things like the shortcuts used to move around. All of these shortcuts should be familiar to Emacs users. There is in fact also \u0026lsquo;Vi Mode\u0026rsquo; option for readline, which allows you to use vi commands to work with text. You can enter this mode with set -o vi.\nThere's a great cheat sheet on emacs readline commands at readline.kablamo.org/emacs, which is a very useful reference if you want to dig deeper.\nWe'll also see GNU Readline later on when we talk about writing programs which work well in the shell.\nI Hope that was useful! Being able to rapidly move around the command line will hopefully save you time and make you a more confident user of not just the shell, but many command line programs.\n Footnotes\nGIFs were made with LICEcap.\n"});index.add({'id':9,'href':'/docs/part-2-core-skills/job-control/','title':"Job Control",'content':"Chapter 9 - Job Control Job control is a feature of most shells which can often be somewhat complicated to work with. However, knowing the basics can help prevent you from getting yourself into a tangle and can from time to time make certain tasks a little easier.\nWhat Is Job Control? Let's start with an example. I am building a simple web page. It has one index.html file, one styles.css file, and one code.js file. The index.html file looks like this:\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;My New Project\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; href=\u0026#34;styles.css\u0026#34;\u0026gt; \u0026lt;script src=\u0026#34;code.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!--Snip... --\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Opening the file in a browser doesn't quite work, as it won't load the code or the styles. We need a web server to serve styles and code.\nA super-useful one-liner to run a web server on any machine with Python installed is:\npython -m SimpleHTTPServer 3000 In fact, this is so useful that I normally alias this command, so that I can just type serve. We'll see aliases in a later chapter.\nMake sure you have the playground folder downloaded, then run the following commands to open the webpage:\n$ cd ~/effective-shell/websites/simple $ python -m SimpleHTTPServer 3000 For now, if we run this command, then we can open the webpage in a browser, with the styles and code loaded:\nWe can also see that the server has served the HTML, JavaScript, and CSS files, this is clear from the output of the Python command we ran:\n$ python -m SimpleHTTPServer 3000 Serving HTTP on 0.0.0.0 port 3000 ... 127.0.0.1 - - [08/Jan/2021 16:33:40] \u0026quot;GET / HTTP/1.1\u0026quot; 200 - 127.0.0.1 - - [08/Jan/2021 16:33:40] \u0026quot;GET /styles.css HTTP/1.1\u0026quot; 200 - 127.0.0.1 - - [08/Jan/2021 16:33:40] \u0026quot;GET /code.js HTTP/1.1\u0026quot; 200 - 127.0.0.1 - - [08/Jan/2021 16:33:40] code 404, message File not found 127.0.0.1 - - [08/Jan/2021 16:33:40] \u0026quot;GET /favicon.ico HTTP/1.1\u0026quot; 404 - All well and good so far. But if you try and use the shell to do something else, you will encounter a problem, let’s take a look.\nThe Problem Let's say we want to now continue using our shell, maybe to edit the website with a terminal editor like Vim or Emacs, or we want to zip up the site, or just run any shell command1.\nWe have a problem. The python process is still running - it's serving the website. Our shell is essentially useless, until we stop the server. See what happens when I try to edit a file:\nIn the example above, I try to run vi, but nothing is happening. Standard input is not being read by the server and not being interpreted by the shell.\nI have to kill the server by hitting Ctrl+C. This sends a SIGINT signal (which tells the command to stop). We saw signals briefly in Chapter 4 - Becoming a Clipboard Gymnast and we'll see more of them in as we continue. Now I need to clear my screen to get rid of all of the error messages, then start again.\nThis is obviously not optimal. Let's look at some solutions.\nSolution 1: Start the Server in the Background In most shells, you can run a command and instruct the shell to run it in the background. To do this, you end the line with an ampersand. Here's how the example would look in this case:\n$ python -m SimpleHTTPServer 3000 \u0026amp; [1] 7025 $ Serving HTTP on 0.0.0.0 port 3000 ... By ending the command with an \u0026amp; ampersand symbol, we instruct the shell to run the command as a background job. This means that our shell is still functional. The shell has also notified us that this command is running as a background job with a specific job number:\n$ python -m SimpleHTTPServer 3000 \u0026amp; [1] 19372 In slightly obtuse language, the shell has informed us that it has started a job in the background, with job number 1 and that this job is currently handling the process with ID 19372.\nThe ampersand solution is a fairly common pattern used in day-to-day work. The process is in the background and our shell is available for us to use as normal, the web server will continue to run in the background.\nSolution 2: Move the Server to the Background Let's say you forgot to start the command in the background. Most likely in this case you'd kill the server with Ctrl+C and then start it again with the \u0026amp; option. However, what if this was a large file download or a task you didn't want to abort?\nIn the example below, we'll move the job to the background:\n$ python -m SimpleHTTPServer 3000 Serving HTTP on 0.0.0.0 port 3000 ... ^Z [1] + 7657 suspended python -m SimpleHTTPServer 3000 The process is currently in the foreground, so my shell is inactive. Hitting Ctrl+Z sends a \u0026lsquo;suspend\u0026rsquo; signal to the process2, pausing it and moving it to the background.\nLet's dissect this:\n$ python -m SimpleHTTPServer 3000 Serving HTTP on 0.0.0.0 port 3000 ... 127.0.0.1 - - [03/Jun/2019 13:38:45] \u0026quot;GET / HTTP/1.1\u0026quot; 200 - ^Z [1] + 21268 suspended python -m SimpleHTTPServer 3000 The shell echos as I type, so we see ^Z (i.e., the Ctrl+Z chord I entered). The shell responds by moving the process into a background job and suspending it.\nThe key here is that it is suspended. The process is paused. So the web server is no longer serving. If you are following with the sample, reload your browser. The webpage fails to load, as the server process is not able to respond to requests.\nTo continue the job, in the background, we use the bg (\u0026lsquo;background\u0026rsquo;) command, with a job identifier (which always starts with a % symbol - we'll see why soon) to tell the shell to continue the job:\n$ bg %1 [1] + 21268 continued python -m SimpleHTTPServer 3000 The shell lets us know the job is being continued, and if we load the webpage again, the content is shown as expected.\nAs a final check, we run the jobs command to see what jobs the shell is running:\n$ jobs [1] + running python -m SimpleHTTPServer 3000 And there you have it - our server is running as a background job. This is exactly what we would see if we run jobs after starting the server with an \u0026amp; at the end. In fact, using an \u0026amp; is perhaps an easier way to remember how to continue a suspended job:\n$ %1 \u0026amp; [1] + 21268 continued python -m SimpleHTTPServer 3000 In the same way ending a command with \u0026amp; runs it in the background, ending a job identifier with \u0026amp; continues it in the background.\nThere is at least one more way to move a job to the background3, but I have not yet found it useful in any scenarios, and it is overly complex to explain. See the footnote for details if you are interested.\nMoving Background Jobs to the Foreground If you have a job in the background, you can bring it back to the foreground with the fg (\u0026lsquo;foreground\u0026rsquo;) command. Let's show the jobs, with the jobs command:\n$ jobs [1] + running python -m SimpleHTTPServer 3000 Here I have a background job running a server. Any one of the following commands will bring it back to the foreground:\nfg %1 # Explicitly bring Job 1 into the foreground %1 # ...or in shorthand, just enter the job id... fg # ...if not given an id, fg and bg assume the most recent job. Now the job is in the foreground, and you can interact with the process again however you like.\nCleaning Up Jobs You might realise you cannot continue what you are doing because an old job is still running. Here's an example:\nI tried to run my web server, but there was still one running as a background job. The server failed to start because the port is in use.\nTo clean it up, I run the jobs command to list the jobs:\n$ jobs [1] + suspended python -m SimpleHTTPServer 3000 There's my old web server. Note that even though it is suspended, it'll still be blocking the port it is serving on4. The process is paused, but it is still holding onto all of the resources it is using.\nNow that I know the job identifier (%1 in this case), I can kill the job:\n$ kill %1 [1] + 22843 terminated python -m SimpleHTTPServer 3000 This is why job identifiers start with a percentage sign! The kill command I have used is not a special job control command (like bg or fg). It is the normal kill command, which terminates a process. But shells that support job control can normally use a job identifier in place of a process identifier. So rather than working out what the process identifier is that I need to kill, I can just use the job identifier5.\nWhy You Shouldn't Use Jobs Avoid jobs. They are not intuitive to interface with and they suffer from some challenges.\nThe most obvious one is that all jobs write to the same output, meaning you can quickly get garbled output like this:\nThis is what happens when I run a job, which just outputs text every second. It's in the background, but it's printing all over my commands. Even running the jobs command to try and find the job to stop it is difficult.\nInput is even more complex. If a job is running in the background, but requires input, it will be silently suspended. This can cause confusion.\nJobs can be used in scripts but must be done so with caution and could easily confuse a consumer of the script if they leave background jobs hanging around, which cannot be easily cleaned up6.\nHandling errors and exit codes for jobs can be problematic, causing confusion, poor error handling, or overly complex code.\nIf jobs should be avoided, why discuss them at all? Well sometimes you move things into the background by mistake, sometimes it can be useful to quickly shift a download or slow command into the background, and also if you are going to avoid something it's good to know why! And the challenge of managing multiple units of work on a computer has been around for a long, long time, with jobs as one of the tools in the toolkit to deal with the challenge.\nBut given I'd suggest to avoid jobs, let's summarise with the most key takeaways and some alternatives.\nThe Most Key Takeaways If there are two things to take away, they would be:\n If you have started running a command in the foreground, and you don't want to stop it and would rather move it to the background, hit Ctrl+Z. Then Google \u0026ldquo;job control\u0026rdquo;.\n And:\n If you think there is a job running in the background, and it is messing with your screen, type fg to bring it to the front and kill it with Ctrl+C. Repeat as needed!\n In either case, if you need to do something more subtle, you can return to this reference. But the first command should allow you to get your shell back while you work out how to continue the job, and the second should kill a background job that is messing with your screen.\nAlternatives to Jobs If you are using any kind of modern terminal such as iTerm, Terminal or the GNOME Terminal, just open a new tab or split! Much easier.\nThe benefit to this is that each tab gets its own standard input and output, so there's no risk of overwriting. And of course you can hide/reveal/rearrange the tabs however you like.\nThe traditional alternative to a job for an operator who simply wants more than one thing going on at once would be a terminal multiplexer, such as screen or tmux:\nMultiplexers work in a very similar way to a modern graphical terminal - they manage many shell instances. But there are some differences.\nModern terminals, such as iTerm, tend to have more intuitive GUIs and a lot of features. Multiplexers can be stateful - and manage work even when you close the shell (allowing you to \u0026lsquo;re-attach\u0026rsquo; later. We can also run them over SSH sessions to manage complex operations on remote machines. They run a client-server model, meaning many people can work with many multiplexed processes (and they can persist beyond sessions).\nMy personal preference is both - I use a modern terminal and run everything inside it in tmux, which is a very common multiplexer (and in some ways the spiritual successor to screen, an older multiplexer). We'll look at both of these options in later chapters.\nQuick Reference You might find that jobs are useful, or you might find that they are not. Either way, here's a quick reference of some common commands:\n   Command Usage     command \u0026amp; Run the command as a background job.   \u0026lt;Ctrl+Z\u0026gt; Move the current process into a background job, suspended.   jobs List all jobs.   fg %1 Move background job number 1 into the foreground.   bg %1 Continue background job number 1.   kill %1 Terminate job number 1.   wait %1 Block until job number 1 exits.    If you want to find out more about the gory details of jobs, the best place to start is the Bash Manual - Job Control Section, or the \u0026lsquo;Job Control\u0026rsquo; section of your preferred shell's manual. On Bash you can find this by using man bash and searching for the text JOB CONTROL. You can find out more about how to get help in Chapter 5 - Getting Help\n Footnotes   If you are not a heavy shell user, this might seem unlikely. But if you do a lot of work in shells, such as sysadmin, devops, or do your coding from a terminal, this happens all the time! \u0026#x21a9;\u0026#xfe0e;\n Technically, SIGTSTP signal - which is \u0026lsquo;TTY stop\u0026rsquo;. If you have always wondered about the \u0026lsquo;TTY\u0026rsquo; acronym, check the chapter, Interlude: Understanding the Shell. \u0026#x21a9;\u0026#xfe0e;\n The alternative method is to use Ctrl+Y, which will send a delayed interrupt, which will continue to run the process until it tries to read from stdin. At this point, the job is suspended and the control given to the shell. The operator can then use bg or kill or fg to either move to the background, stop the process, or keep in the foreground as preferred. See: https://www.gnu.org/savannah-checkouts/gnu/bash/manual/bash.html#Job-Control \u0026#x21a9;\u0026#xfe0e;\n Another super-useful snippet: lsof -i -P -n | grep 8000 to find any process that has a given port open. Another one for the aliases chapter! \u0026#x21a9;\u0026#xfe0e;\n There are times this is needed. If a job runs many processes - for example, by running a pipeline - the process identifier will change as the command moves from one stage of the pipeline to the next. The job identifier will remain constant. Remember, a job is a shell command, so could run many processes. \u0026#x21a9;\u0026#xfe0e;\n To see how bad this can be, create a script that starts jobs, then run it. Then run the jobs command to see what is running. The output might surprise you! \u0026#x21a9;\u0026#xfe0e;\n   "});index.add({'id':10,'href':'/docs/part-2-core-skills/understanding-commands/','title':"Understanding Commands",'content':"Chapter 10 - Understanding Commands In this chapter, we'll take a look at the various different types of shell commands that exist and how this can affect your work. Commands are far more subtle than you might think and in this chapter we'll look at some of the nuances of commands and the practical consequences for your work.\nBy the end of this chapter, you might even be able to make sense of the horrifying and perfectly syntactically valid code below:\nwhich $(where $(what $(whence $(whereis who)))) What Are Commands? This is really important to understand! A command in a shell is something you execute. It might take parameters. Generally it'll have a form like this:\ncommand param1 param2 We've already seen many commands during this series:\nls # Show the contents of the current directory cd ~ # Move to the user\u0026#39;s home cat file.txt # Output the contents of \u0026#39;file.txt\u0026#39; to stdout But to be an effective shell user, you must understand that not all commands are created equal. The differences between the types of commands will affect how you use them.\nThe Different Types of Commands There are four types of commands in most shells:\n Executables \u0026ldquo;Built-Ins\u0026rdquo; (which we'll just call builtins from now on) Functions Aliases  Each is different and has its own quirks. Let's quickly dig in and see a bit more.\nExecutables - Programs Executables are just files with the \u0026lsquo;executable\u0026rsquo; bit set1. If I execute the cat command, the shell will search for an executable named cat in my $PATH. If it finds it, it will run the program.\n$ cat file.txt This is a simple text file What is $PATH? $PATH is the standard environment variable used to define where the shell should search for programs. If we temporarily empty this variable, the shell won't find the command:\n$ PATH=\u0026#34;\u0026#34; cat file.txt bash: cat: No such file or directory Normally your $PATH variable will include the standard locations for Linux programs - folders such as /bin, /sbin, /usr/bin and so on2.\nIf you were to print the variable, you'd see a bunch of paths (they are separated by colons; I've put them on separate lines for readability):\n/usr/local/bin /usr/bin /bin /usr/sbin /sbin The shell will start with the earlier locations and move to the later ones. This allows local flavours of tools to be installed for users, which will take precedence over general versions of tools.\nThere will likely be other locations too - you might see Java folders, package manager folders and so on.\nExecutables - Scripts Imagine we create a text file called dog in the local folder that looks like this:\n#!/bin/sh echo \u0026#34;🐶 woof 🐶\u0026#34; This is a shell script (you've heard this before, but we'll see a lot more of these as we go through the book!). We mentioned that executables are any files which have the executable bit set. Let's actually do this, using the chmod (change file modes) command:\n$ ls -l dog -rw-r--r-- 1 dwmkerr staff 32 Oct 8 22:44 dog $ chmod +x dog $ ls -l dog -rwxr-xr-x 1 dwmkerr staff 32 Oct 8 22:44 dog I've used ls -l dog to show the file permissions of dog before and after the chmod +x dog3 command. We can see that there are some new x's in the first section. These are saying that the file is now executable by all users.\nNow that we have made the file executable we can run this just like any other program - as long as we tell the shell to look for programs in the current directory:\n$ PATH=\u0026#34;.\u0026#34; dog 🐶 woof 🐶 By the way - don't do this! Adding the special . directory to the path is generally not a safe or sensible thing to do, this is just a demonstration of how it works. More common would be to run the program by specifying the path to the file, like so:\n$ ./dog 🐶 woof 🐶 Another option would just be to move it to a standard location that the shell already checks for programs:\n$ mv dog /usr/local/bin $ dog 🐶 woof 🐶 Executables don't have to be compiled program code, they can be scripts. If a file starts with #! (the \u0026lsquo;shebang\u0026rsquo;), then the system will try to run the contents of the file with the program specified in the shebang.\nWe will look at shebangs in greater detail in a later chapter. But the key takeaway here is that we can also have executable scripts as commands.\nBuiltins OK, so we've seen executables. What about a command like this?\nlocal V=\u0026#34;hello\u0026#34; echo $V You will not find the local executable anywhere on your system. It is a builtin - a special command built directly into the shell program.\nBuiltins are often highly specific to your shell. They might be used for programming (local for example is used to declare a locally scoped variable), or they might be for very shell-specific features.\nThis is where we need to take note. As soon as you are running a builtin, you are potentially using a feature that is specific to your shell, rather than a program that is shared across the system and can be run by any shell.\nTrying to programmatically execute local as a process will fail - there is no executable with that name; it is purely a shell construct.\nSo how do we know if a command is a builtin? The preferred method is to use the type command:\n$ type local local is a shell builtin The type command (which is itself a builtin!) can tell you the exact type of shell command.\nInterestingly, you might be using more builtins than you think. echo is a program, but most of the time you are not executing it when you are in a shell:\n$ type -a echo echo is a shell builtin echo is /bin/echo By using the -a flag on type to show all commands that match the name, we see that echo is actually both a builtin and a program.\nMany simple programs have builtin versions. The shell can execute them much faster.\nSome commands are a builtin so that they can function in a sensible manner. For example, cd command changes the current directory - if we executed it as a process, it would change only the directory for the cd process itself, not the shell, making it much less useful.\nEcho is builtin because the shell can run much more quickly by not actually running a program if it has its own built in implementation.\nBuiltins will vary from shell to shell, but many shells are \u0026lsquo;Bash-like\u0026rsquo; - meaning they will have a set very similar to the Bash builtins, which you can see here:\nhttps://www.gnu.org/software/bash/manual/html_node/Bash-Builtins.html\nAs should be familiar from Chapter 5 - Getting Help, you can get help for builtins:\n$ man source # source is a builtin BUILTIN(1) BSD General Commands Manual BUILTIN(1) NAME builtin, !, %, # ...snip... SYNOPSIS builtin [-options] [args ...] However, the manual will not show information on specific builtins, which is a pain. Your shell might have an option to show more details - for example, in Bash you can use help:\n$ help source source: source filename [arguments] Read and execute commands from FILENAME and return. The pathnames in $PATH are used to find the directory containing FILENAME. If any ARGUMENTS are supplied, they become the positional parameters when FILENAME is executed. But remember: help is a builtin; you might not find it in all shells (you won't find it in zsh, for example). This highlights again the challenges of builtins.\nFunctions You can define your own shell functions. We will see a lot more of this later, but let's show a quick example for now:\n$ restart-shell () { exec -l $SHELL } This snippet creates a function that restarts the shell (quite useful if you are messing with shell configuration files or think you might have irreversibly goofed up your current session).\nWe can execute this function just like any command:\n$ restart-shell And running type will show us that this is a function:\n$ type restart-shell restart-shell is a function restart-shell () { exec -l $SHELL } Functions are one of the most powerful shell constructs we will see; they are extremely useful for building sophisticated logic. We're going to see them in a lot more detail later, but for now it is enough to know that they exist, and can run logic, and are run as commands.\nAliases An alias is just a shortcut. Type in a certain set of characters, and the shell will replace them with the value defined in the alias.\nSome common commands are actually already aliases - for example, in my zsh shell, the ls command is an alias:\n% type -a ls ls is an alias for ls -G ls is /bin/ls I make sure that when I use the ls command, the shell always expands it to ls -G, which colours the output.\nWe can quickly define aliases to save on keystrokes. For example:\n$ alias k=\u0026#39;kubectl\u0026#39; From this point on, I can use the k alias as shorthand for the kubectl command.\nAliases are far less sophisticated than functions. Think of them as keystroke savers and nothing more, and you won't go far wrong.\nThe Key Takeaways So we now hopefully have a greater understanding of the variety of shell commands. Not all commands are executables, not all of the commands we think are executables necessarily are, and some commands might be more sophisticated.\nAs a shell user, the key things to remember are:\n Executables are programs your system can use; your shell just calls out to them. Builtins are very shell-specific and usually control the shell itself Functions are powerful ways to write logic but will normally be shell-specific. Aliases are conveniences for human operators, but only in the context of an interactive shell.  To find out how a command is implemented, just use the type -a command:\n$ type -a cat cat is /bin/cat More than You Need to Know OK, for the masochistic few, you might be wondering about all of the other commands and utilities you may have seen that can tell you about programs and commands:\n what whatis which whence where whereis command type  A lot of these are legacy and should be avoided, but for completeness sake, we'll go through them.\nwhat what reads out special metadata embedded in a program, generally used to identify the version of source code it was built from:\n$ what /bin/ls /bin/ls Copyright (c) 1989, 1993, 1994 PROGRAM:ls PROJECT:file_cmds-272.220.1 There should be almost no circumstance in which you need to use it in your day-to-day work, but you might come across it if you meant to type whatis.\nwhatis whatis searches a local help database for text. This can be useful in tracking down manual pages:\n$ whatis bash bash(1) - GNU Bourne-Again SHell bashbug(1) - report a bug in bash But I can't imagine it will be a regularly used tool by most users.\nwhich which will search your $PATH to see whether an executable can be found. With the -a flag, it will show all results.\n$ which -a vi /usr/local/bin/vi /usr/bin/vi which originated in csh. It remains on many systems for compatibility but in general should be avoided due to potentially odd behaviour4.\nwhence whence was added to the Korn shell. You are unlikely to use it unless you are on systems using ksh. zsh also has this command, but it should be avoided and considered non-standard.\n% whence brew /usr/local/bin/brew where This is a shell builtin that can provide information on commands, similar to type:\n% where ls ls: aliased to ls -G /bin/ls However, type should be preferred, as it is more standard.\nwhereis whereis is available on some systems and generally operates the same as which, searching paths for an executable:\n% whereis ls /bin/ls Again, type should be preferred for compatibility.\ncommand command is defined in the POSIX standard, so should be expected to be present on most modern systems. Without arguments, it simply executes a command. With the -v argument, you get a fairly machine-readable or processable response; with the -V argument, you get a more human readable response:\n% command -v ls alias ls=\u0026#39;ls -G\u0026#39; % command -V ls ls is an alias for ls -G command can be useful in scripts, as we will see in later chapters.\ntype type is part of the Unix standard and will be present in most modern systems. As we've already seen, it will identify the type of command as well as the location for an executable:\n% type -a ls ls is an alias for ls -G ls is /bin/ls This command can also be used to only search for paths:\n% type -p ls ls is /bin/ls Summary\nIn summary, avoid anything that starts with \u0026lsquo;w\u0026rsquo;! These are legacy commands, generally needed only when working on older Unix machines. type or command should be used instead.\n Footnotes\n  We will cover permissions and modes in later chapters. \u0026#x21a9;\u0026#xfe0e;\n Why these names and locations? It's a long story. The best place to start if you are interested is the Filesystem Hierarchy Standard. \u0026#x21a9;\u0026#xfe0e;\n chmod changes the mode of a file; +x means \u0026lsquo;add the executable bit\u0026rsquo;. This tells the operating system the file can be executed. \u0026#x21a9;\u0026#xfe0e;\n Stack Exchange: Why not use “which”? What to use then? \u0026#x21a9;\u0026#xfe0e;\n   "});index.add({'id':11,'href':'/docs/part-2-core-skills/finding-files/','title':"Finding Files",'content':"Chapter 11 - Finding Files Searching through a system to find files or folders can be complex and time consuming, even with a graphical user interface. In this chapter we'll look at how to use the shell to search for files and folders and some quick ways to accomplish common tasks.\nIntroducing the Find Command The find (search for files) command is used to search for files and folders and to perform operations on the results. Let's see it in action by running it in the ~/effective-shell folder.\nRunning the Samples\nEach of these samples assumes you are in the effective-shell samples folder. If you don't have this folder, just run:\nmkdir -p ~/effective-shell curl -s -L https://effective-shell.com/downloads/effective-shell-playground.tar.gz | tar -xzf - -C ~/effective-shell cd ~/effective-shell This command will download and extract the samples folder in your home directory and then set it as your working directory.\n Let's set the current working directory to the effective-shell folder and run the find command:\n$ cd ~/effective-shell $ find . ./text ./text/simpsons-characters.txt ./scripts ./scripts/show-info.sh ./websites ./websites/simple ./websites/simple/index.html ./websites/simple/styles.css ./websites/simple/code.js ... By default, find will list all of the files and folders which are present in the current working directory. It will also show the children of any folders it finds, meaning that it shows the full hierarchy of files and folders.\nFind not working on MacOS\nIf you are running these samples on MacOS, you will probably see the following output:\n$ find usage: find [-H | -L | -P] [-EXdsx] [-f path] path ... [expression] This is the find command telling you what parameters can be used. On MacOS the default find command does not assume the current working directory.\nThis is because there is a difference between the MacOS and GNU versions of find and in this book I will use GNU wherever possible as it will be more compatible (MacOS is based on the BSD operating system, most Linux distributions use a set of tools which are part of the GNU project - there are sometimes differences).\nTo run the equivalent command on MacOS, just provide the current directory as a parameter:\n$ find . A better solution is to install the findtools package, which will install the GNU versions of the tools we'll be using:\n$ brew install findtools $ gfind If you do install findtools, remember that all of the GNU versions of the tools start with g - so when reading this chapter substitute find with gfind.\nFor more details on what BSD and GNU are, you can check Chapter - Unix, Linux, GNU and POSIX, which covers these concepts in detail.\n So this is the find command - you can provide it a directory (or let it use the current directory) and the command will list all of files and folders in the given directory, including all children.\nYou can also provide multiple directories:\n$ find /usr/bin /usr/sbin /usr/bin /usr/bin/fwupdtool /usr/bin/gnome-keyring ... /usr/sbin /usr/sbin/cupsd /usr/sbin/pppdump ... This is the most basic use of find - showing a file and folder hierarchy. Now let's look at how to search using this command.\nSearching with Find Perhaps the most common use for find is to search for files. There are a number of options which can be used to filter the results shown, which allow us to search for files. Let's look at some common ways to refine our searches, using the ~/effective-shell folder as a playground to search in.\nSearching for Files or Folders only The -type parameter can be used to search either for files or folders. Let's see both in action. First, we'll search for files only, using -type f:\n$ find . -type f ./text/simpsons-characters.txt ./scripts/show-info.sh ./websites/simple/index.html ./websites/simple/styles.css ./websites/simple/code.js ... And for folders, using -type d (remember, d is for directory!):\n$ find . -type d . ./text ./scripts ./websites ./websites/simple ... It's important to note that when searching for folders, the find command shows folders which are normally hidden, such as the special \u0026lsquo;dot\u0026rsquo; folder1.\nIn both commands, I specified the \u0026lsquo;dot\u0026rsquo; folder as the place to search. I could omit this parameter; I just think it makes it a little more readable.\nSearching by Name We can use the -name parameter to search for files and folders by name. For example, this is how we would search for anything with the letters log in the name:\n$ find . -name \u0026quot;*log*\u0026quot; ./logs ./logs/web-server-logs.txt ./logs/apm-logs ./logs/apm-logs/apm05.logs ./logs/apm-logs/apm02.logs ./logs/apm-logs/apm03.logs ./logs/apm-logs/apm00.logs ./logs/apm-logs/apm01.logs ./logs/apm-logs/apm04.logs You can see I've used a * wildcard before and after the letters log - this means that I have actually supplied a pattern, which could be read as \u0026lsquo;any characters (including nothing), followed by the characters log, followed by any other characters (including no characters)'.\nIf I don't use a wildcard, the logs folder will not be found - because it doesn't match the pattern log - without using a wildcard, the pattern is explicitly looking for an exact match:\n$ find . -name \u0026quot;log\u0026quot; Note the output - no files or folders were found, as none have the exact name log. The -name parameter is very specific - it will only match files or folders with the name provided. Here's an example:\n$ find . -name \u0026quot;apm00.logs\u0026quot; ./logs/apm-logs/apm00.logs Here I have used -name to search for an exact name. What about if I search for apm-logs?\n$ find . -name \u0026quot;apm-logs\u0026quot; ./logs/apm-logs The folder named apm-logs is found, but not the files in the folder - the names of those files don't match the pattern apm-logs. What if we make it a wildcard pattern?\n$ find . -name \u0026quot;*apm-logs*\u0026quot; ./logs/apm-logs The same results! This is because for the files in the apm-logs folder they don't have apm-logs in their name anywhere - that is in their path, i.e. the full address of the file including its folder. So let's look at how to search by path next!\nAn Important Note on Quotes\nMake sure you use quotes when building your patterns. This command:\n$ find . -name \u0026quot;*log*\u0026quot; Will give different output to this command:\n$ find . -name *log* Why is this? In the first case, we explicitly pass the text *log* to the find command and let it deal with it. It uses the wildcards to build a pattern. Because we've surrounded the parameter with quotes, the shell itself doesn't try to do anything clever with the wildcard.\nIn the second case, the shell itself tries to deal with the wildcards, then passes the results to find. And the shell deals with them differently. You can see exactly what the shell expands them to with this snippet:\n$ parameter=(*log*) $ echo $parameter logs In the second case the shell is performing its own expansion of the wildcard and not searching through all of the child directories. We need to wrap the parameter with quotes so that the shell knows not to interfere with the text and instead pass it to find, so that find can deal with the wildcard.\nThe shell is using globbing in the second case, which is covered in a later chapter.\n Searching by Path The -path parameter can be used to filter the results based on a pattern in the path of the file:\n$ find . -path \u0026quot;*apm-logs*\u0026quot; ./logs/apm-logs ./logs/apm-logs/apm05.logs ./logs/apm-logs/apm02.logs ./logs/apm-logs/apm03.logs ./logs/apm-logs/apm00.logs ./logs/apm-logs/apm01.logs ./logs/apm-logs/apm04.logs Again, note that this is very specific, we've added wildcards to the pattern, making it *apm-logs*. Without the wildcards we would find nothing, because none of the results have the exact path apm-logs.\nCombining Searches - the AND and OR operators We can provide multiple search options. For example, if we want to search only for files which end in .logs, we can do this:\n$ find . -type f -name \u0026quot;*.logs\u0026quot; ./logs/apm-logs/apm05.logs ./logs/apm-logs/apm02.logs ./logs/apm-logs/apm03.logs ./logs/apm-logs/apm00.logs ./logs/apm-logs/apm01.logs ./logs/apm-logs/apm04.logs By using both the -type and -name parameters, we've created an \u0026lsquo;AND\u0026rsquo; style search - i.e. we're looking for items which match both of the given criteria.\nWhat if we want to perform a search which returns items which match either of the patterns (i.e an \u0026lsquo;OR\u0026rsquo; search)? For that we can use the -or parameter:\n$ find . -name \u0026quot;*.js\u0026quot; -or -name \u0026quot;*.html\u0026quot; ./websites/simple/index.html ./websites/simple/code.js ./programs/web-server/web-server.js In this case we show results which match either of the expressions.\nCase Insensitive Searches Any one of the search parameters you've seen so far can be made case-insensitive by putting the letter i before the parameter name.\nThis means that the following commands are identical:\n$ find . -name \u0026quot;*.js\u0026quot; -or -name \u0026quot;*.Js\u0026quot; -or -name \u0026quot;*.jS\u0026quot; -or name \u0026quot;*.JS\u0026quot; And:\n$ find . -iname \u0026quot;*.js\u0026quot; I know which one I'd rather type! You can use -iname for case-insensitive name searches, or -ipath for case-insensitive path searches.\nGrouping and the NOT operator We can build more complex expressions by grouping together patterns using brackets, or by using the -not pattern. Here's an example:\n$ find . \\( -name \u0026quot;*.js\u0026quot; -or -name \u0026quot;*.html\u0026quot; \\) -and -not -path \u0026quot;*programs*\u0026quot; ./websites/simple/index.html ./websites/simple/code.js The first section groups together two expressions, meaning \u0026ldquo;files with names which match *.js or *.html, the second section then says \u0026ldquo;and also the text programs must not be in the path\u0026rdquo;.\nThe only annoying thing about grouping is that you must escape the brackets, by putting a \\ backslash before them, as brackets have a special meaning in the shell and we're telling the shell not to do anything smart with them but instead pass them to the find command.\nWhy the Weird Parameters? The find command bothered me for years. The parameters looked odd - for example why is it -name instead of -n or --name, which would be more standard2?\nAlso, why is it that I cannot type this:\nfind -name \u0026quot;something.txt\u0026quot; /home/ But instead have to put the folder name before the parameters, which again is non-standard?\nThe reason is actually quite simple - most of what we've seen so far are not parameters in the normal sense, they're just elements of a search expression.\nThe structure of the find command is as follows:\nfind \u0026lt;options\u0026gt; [starting point...] [expression] The options (or parameters) for the find command are short, one-letter options which go before the file name. The -L (follow links) option is an example - it goes before the starting point of the search:\nfind -L /usr/bin -name \u0026quot;*sh\u0026quot; All of the other things we've seen so far which we've described as parameters are actually used to build the expression - the actual search pattern.\nThe -name, -and, -or, -ipath and similar constructs we've looked at are actually part of a mini \u0026lsquo;search language\u0026rsquo;, they're not parameters to the command.\nThis might seem obvious, or it might seem silly, but either way, remembering that the structure of the find command is find \u0026lt;options\u0026gt; [starting points...] [expression] may help you remember what order to write each part of the command in.\nIt certainly took a few years for me to realise this was the reason, and until that point I used to get frustrated with find as I could never seem to remember how to use it properly! Once the structure of the command clicked it became far easier to quickly use the command in day-to-day work.\nYou can find details on this in the manpage for find, open it with man find.\nPerforming Actions on Search Results A lot of the time you are not just going to be searching for a file or folder - you'll be searching so that you can do something with what you find. It might be to delete, copy, edit, move or whatever.\nThe find command can perform operations on the files which are found.\nNow before we continue, I'll warn that I'm not going to go into much detail here. The reason is that I actually recommend not using these operations. Instead, use the xargs command which is covered in Chapter 14 - Build Commands on the Fly with Xargs. The Xargs command lets you take the list of files from find and create any command you can think of. I think this is far more sensible than trying to learn all of the options for find - and it is closer to the Unix Philosophy of having the find command \u0026lsquo;do one thing and do it well\u0026rsquo;.\nHowever, you'll see these operations in other books and articles, or perhaps in scripts you have to work with, so let's take a quick look at some of the common operations and how they are used. Just remember that later on we'll see a more flexible (and easier to remember!) way of operating on the files we've found!\nPrinting Paths The -print action is the default action and doesn't need to be explicitly specified. But if you feel it makes your scripts or code more readable, you can always include it, and it gives us a way to show the syntax for actions.\nHere's how we'd find all files in the user's home directory with the ending .tmp and show their path:\n$ find ~ -name \u0026quot;*.tmp\u0026quot; -print /home/dwmkerr/commands1.tmp /home/dwmkerr/commands2.tmp /home/dwmkerr/commands3.tmp You are unlikely to need to use -print - but it will come in handy to know it exists later on when we look at the -print0 option (we'll see this in Chapter 14). Let's look at the other actions we can perform.\nDeleting Files We can delete the files and folders found by using the -delete action:\n$ find ~ -name \u0026quot;*.tmp\u0026quot; -delete This can be a convenient way to delete files, but I would recommend extreme caution. This command does not show what has been deleted and does not ask for confirmation. It also slightly changes the order of results processed so that the children of folders are deleted where necessary before the folders themselves. This might not be what you expect because that's not what the -print output would show (although you can force this behaviour with the -depth option).\nCheck Chapter 14 for a better solution - in short we can use find ~ -name \u0026quot;*.tmp\u0026quot; -print0 | xargs -p -0 rm to instead pass the files to rm and ask the user to confirm before the deletion happens. This will be explained in a lot more detail in Chapter 14.\nExecute a Command You can use the -exec action to execute an arbitrary command. Use the special characters {} as a placeholder for the filename.\nHere's an example - we'll find all text files and count the number of words in each one:\n$ find ~/effective-shell -name \u0026quot;*.txt\u0026quot; -exec wc -w {} \\; 29 /home/parallels/effective-shell/text/simpsons-characters.txt 20 /home/parallels/effective-shell/quotes/iain-banks.txt 16 /home/parallels/effective-shell/quotes/ursula-le-guin.txt 10373 /home/parallels/effective-shell/logs/web-server-logs.txt We use -exec to tell find we want to execute a command. Then we use wc -w {} to say \u0026ldquo;call the wc (word count) command with the -w (words) flag. The {} text is expanded to the list of files. Finally, we need a semi-colon to tell find where the end of the exec command is. And because the semi-colon has a special meaning in the shell, we have to escape this semi-colon by putting a \\ backslash before it.\nThe -exec action is very powerful - we can construct almost any arbitrary command with it, which can be really useful. But remember we'll see what I think is a more flexible way to build commands a little later.\nExecute a Command with a Confirmation Now if there is one action to learn, it is the -ok action, which works just like -exec, but asks the user for a confirmation first. Here's how it might look if I use it to try and delete all files which end in *.txt:\n$ find ~/effective-shell -name \u0026quot;*.txt\u0026quot; -ok rm {} \\; \u0026lt; rm ... /home/parallels/effective-shell/text/simpsons-characters.txt \u0026gt; ? n \u0026lt; rm ... /home/parallels/effective-shell/quotes/iain-banks.txt \u0026gt; ? n \u0026lt; rm ... /home/parallels/effective-shell/quotes/ursula-le-guin.txt \u0026gt; ? n \u0026lt; rm ... /home/parallels/effective-shell/logs/web-server-logs.txt \u0026gt; ? n Note that each operation which will be performed is first printed, then I am asked for confirmation before the operation runs. In each case I've typed n (for \u0026lsquo;no\u0026rsquo;). Type y (for \u0026lsquo;yes\u0026rsquo;) if you want to run the command.\nHandling Symlinks It is worth briefly mentioning symlinks - because if you don't understand how find handles symlinks then you might be surprised.\nAs an example of how this might surprise you, compare the output of the two commands below:\n$ find /usr/bin /usr/bin /usr/bin/uux /usr/bin/cpan /usr/bin/BuildStrings /usr/bin/loads.d /usr/bin/write ... $ find /bin /bin It seems that /bin doesn't contain any files - but is that the case? Running ls /bin will probably show that you have quite a few files. If you are on MacOS instead try running find /tmp to show the same oddity - the find program doesn't seem to show the contents of the files.\nSo why did find /bin not show the files in the /bin folder?\nThe answer is that /bin is a symlink (or if you are on MacOS and want to test the same behaviour and are using /tmp, /tmp is a symlink). You can see this by running the command below:\n$ ls -l / /usr | grep bin lrwxrwxrwx 1 root root 7 Aug 7 18:06 bin -\u0026gt; usr/bin lrwxrwxrwx 1 root root 8 Aug 7 18:06 sbin -\u0026gt; usr/sbin drwxr-xr-x 2 root root 40960 Jan 25 17:17 bin drwxr-xr-x 2 root root 20480 Jan 25 16:42 sbin Note how the root bin and sbin folders are actually just symlinks to usr/bin and usr/sbin respectively.\nWhen using the find command just remember that it won't follow symlinks by default - provide the -L option to follow symlinks:\n$ find -L /bin /bin /bin/fwupdtool /bin/gnome-keyring /bin/dpkg-gencontrol /bin/prltoolsd ... There are more options which control how find works with links, check man find for the details.\nScratching the Surface The find command is incredibly powerful. To go into detail on all of the options or potential ways these options can be combined to create operations could fill an entire book!\nMy recommendation is to ensure that you know at least the basics we've shown so far. But just as a hint of what can be done with find, which might make you want to learn more, here are a few commands which show just how versatile it can be!\nFind large files\nThe -size test can be used to search by file size - note how with a + and - we can set the minimum and maximum sizes:\nfind / -size +1G -500G Find recently edited files\nThe -mtime test can be used to find files which were recently modified:\nfind . -not -path \u0026quot;*/\\.*\u0026quot; -mtime -2 Note how a -not -path test is used to skip anything which starts with a . dot (i.e files and folders which are normally considered hidden).\nFind files which have had permissions changed\nThe -ctime test can be used to find files which have had their attributes (such as permissions) changed:\nfind ~/.ssh -ctime -30 Find any executable scripts and make them non-executable\nWe can search by permissions, as shown below:\nfind ~ -perm /a=x -exec chmod -x {} + This example uses the -perm test, checking if \u0026lsquo;all\u0026rsquo; (users, the owner and group) have the x (executable) bit set, then executes the chmod -x command to remove the executable bit. We also end the command with + rather than ;, which means we will execute chmod once with each file passed to the command (rather than running chmod for each file). Note that the + operator can cause an error if the list of files is too big for the command you pass it to to handle!\nFind empty folders and remove them with a confirmation\nWe can use the -empty test to find empty folders:\nfind ~ -type d -maxdepth -empty -ok rmdir {} \\; This example uses the -empty test, as well as the -maxdepth parameter to limit the search to only three folders deep.\nThese examples just scratch the surface of what find can do. The goal of this chapter is not to have an exhaustive description of the find command, but equip you with the essentials. When you feel comfortable with the essentials you can then build on your knowledge of find.\nSummary In this chapter we introduced the find command, an incredibly powerful tool that lets us search for files and folders using simple or complex expressions. It also allows us to perform actions on the search results.\nIn the next chapter we'll take a look in a bit more detail into what a shell actually is!\n Share - with \u0026ldquo;why the hell are the parameters so stupid\u0026rdquo; Blog post on linux essentials, refer to alpine for an example of why find is important Test work in progress page\nFootnotes\n  If you are not sure what the \u0026lsquo;dot\u0026rsquo; folder is, check Chapter 2 - Navigating Your System \u0026#x21a9;\u0026#xfe0e;\n This is not just a personal preference thing; this is based on the POSIX standard: https://www.gnu.org/software/libc/manual/html_node/Argument-Syntax.html which recommends a specific set of patterns to make commands consistent and intuitive for user. \u0026#x21a9;\u0026#xfe0e;\n   "});index.add({'id':12,'href':'/docs/part-2-core-skills/what-is-a-shell/','title':"What is a Shell?",'content':"Chapter 12 - Interlude - What is a Shell? This is the second of the \u0026ldquo;interludes\u0026rdquo; which end each section of the book. These interludes give flavour, concepts, context and the history of some of the concepts we're dealing with. These interlude are not essential to mastering the skills of the shell, but you might find them interesting.\nIn this interlude we'll actually look at what a shell is, all the way from the highest level, which non-technical readers will be able to comfortably follow, to the low level, which advanced users may find illuminating.\nIntroduction for the Non-Technical Reader It might come as a surprise that many technical computer users (programmers, data scientists, systems administrators etc) spend a lot of time using an interface which looks like it's from the sixties:\nIf you work with technologists, you might have seen them using an interface like this. This kind of simple, text-based interface is called a shell, and it has been a common way to interface with computers ever since the first screens and keyboards were created.\nGiven how much computing has advanced, why would people use such an interface? Just look at how much the Windows operating-system has changed over the last three decades:\n(By Source (WP:NFCC#4), Fair use, https://en.wikipedia.org/w/index.php?curid=58853841)\nWhy would people choose to use such an archaic interface as a shell?\n Typing is fast: A skilled shell user can manipulate a system at dazzling speeds just using a keyboard. Typing commands is generally much faster than exploring through user interfaces with a mouse Shells are programmable: Users will often being programming as they work in a shell, creating scripts to automate time-consuming or repetitive processes Shells are portable: A shell can be used to interface to almost any type of computer, from a mainframe to a Raspberry Pi, in a very similar way.  Not all technical users will use a shell regularly, but there are many who will spend the bulk of their time in such an interface. It is such a crucial skill to be able to operate one effectively that I have been writing this series primarily to show ways to be more efficient with this kind of interface.\nIntroduction for the Technical Reader You may be familiar with the shell, but it can be useful to understand some of the surrounding concepts in detail. How does a shell differ from a terminal? What is a tty? How do shells really work? Hopefully as you read this chapter you'll discover something that you didn't know about shells.\nLet's Get Started! To understand what shells, terminals, command-prompts and so on are and how they relate, we need to start with the basics: how a modern computer works!\nA Computer in a Nutshell The diagram below shows a simplified view of a typical computer:\nAlready there's a lot going on.\nYour computer is going to have a CPU1 and memory2, and almost certainly a network adapter3 and display adapter4. Most computers will have at least one hard disk. For home PCs, there'll also likely be a bunch of peripherals, such as a mouse, keyboard, printers, flash drives, webcams and so on.\nThe Operating System The operating system is the piece of software installed on a computer that can interface with the hardware. Without hardware, such as a CPU, memory, a network adapter, a graphics card, disk drives and so on, there's not much that you can do with the computer. The operating system is the primary interface to this hardware. No normal programs will talk to hardware directly - the operating system abstracts this hardware away and provides a software interface to it.\nThe abstraction the operating system provides is essential. Developers don't need to know the specifics of how to work with individual devices from different vendors; the operating system provides a standardised interface to all of this. It also handles various tasks such as making sure the system starts up properly.\nThe operating system is generally broken down into two parts - the kernel and user space:\nLet's look at these in more detail.\nThe Kernel This is the part of the operating system that is responsible for the most sensitive tasks: interfacing with physical devices, managing the resources that are available for users and programs, starting up the various systems that are needed, and so on.\nSoftware running in the kernel has direct access to resources, so is extremely sensitive. The kernel will balance resources between the programs in user space, which we'll look at shortly. If you've ever had to install \u0026lsquo;drivers\u0026rsquo;, these are examples of pieces of software that will run in the kernel - they'll have direct access to a physical device you've installed, and expose it to the rest of the software on the computer.\nWhy \u0026lsquo;kernel\u0026rsquo;? The kernel is the soft, edible part of a nut or seed, which is surrounded by a shell. Below you can see a walnut - the kernel is the soft bit in the middle, and the shell surrounds and protects it. This is a useful metaphor that is used for parts of a computer.\n(By Kkchaudhary11 - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=49069244)\nThe operating system kernel really is the core of the operating system. It's such a sensitive area of the operating system that we actually want to avoid running software in it if possible5. And that is where user space comes in.\nUser Space The vast majority of programs run in \u0026lsquo;user space\u0026rsquo; (also commonly called \u0026lsquo;user land\u0026rsquo;).\nWhen a program starts, the kernel will allocate it a private segment of memory and provide limited access to resources. The program is given access to a library of functions by the operating system, which it can use to access resources such as files, devices and so on. Programs in user space are essentially in sandboxes, where there is a limit to how much damage they can do.\nFor example, a program running in user space can use the standard fopen function, which is provided on almost every operating system as part of the C Standard Library. This allows a program to attempt to open a file. The operating system will make a decision on whether the program is allowed to open the file (based on things such as permissions, where the file is and so on) and then, if it is OK with the call, will give the program access to the file. Under the hood, this \u0026lsquo;user space\u0026rsquo; call translates to a system call in the kernel.\nNow that the key components have been introduced, we can look at the shell. The name should come as no surprise, as it is a wrapper or outer layer to the operating system (which itself contains the sensitive nugget of the kernel).\nThe Shell So what is the shell? The shell is just a general name for any user space program that allows access to resources in the system, via some kind of interface.\nShells come in many different flavours but are generally provided to aid a human operator in accessing the system. This could be interactively, by typing at a terminal, or via scripts, which are files that contain a sequence of commands.\nFor example, to see all of the files in a folder, the human operator could write a program in a language such as C, making system calls to do what they want. But for day-to-day tasks, this would be repetitive. A shell will normally offer us a quick way to do that exact task, without having to manually write a program to do it.\nHere's an example, where a shell is being used to show the \u0026lsquo;png\u0026rsquo; images in the folder I am working in6:\nSo a shell is a user-space program to interface with the computer. But there a few more moving parts than just a shell we are seeing in the image above. There are different types of shells, there are terminal programs, and there are the programs or commands that the shell calls (in the example above, tree is a program). Let's pick these apart.\nHere's a diagram that more accurately shows what is going on:\nWe've introduced a few new things here. There's a user, who is interfacing with a terminal, which is running a shell, which is showing a command prompt. The user has written a command that is calling a program (in this case, the tree program).\nLet's dissect this bit by bit.\nThe Terminal We're not directly interacting with the \u0026lsquo;shell\u0026rsquo; in this diagram. We're actually using a terminal. When a user wants to work with a shell interactively, using a keyboard to provide input and a display to see the output on the screen, the user uses a terminal.\nA terminal is just a program that reads input from the keyboard, passes that input to another program (normally a shell), and displays the results on the screen. A shell program on its own does not do this - it requires a terminal as an interface.\nWhy the word terminal? This makes sense when you look at how people interfaced with computers historically. Input to a computer might be through punch cards, and output would often be via a printer. The Teletype Terminal7 became a common way for users to interface with computers.\n(Photograph by Rama, Wikimedia Commons, Cc-by-sa-2.0-fr, CC BY-SA 2.0 fr, https://commons.wikimedia.org/w/index.php?curid=17821795)\nAt this time, computers were very large, complex, and expensive machines. It was common to have many terminals connected to a single large machine (or \u0026lsquo;mainframe\u0026rsquo;), or a few terminals that people would share. But the terminal itself was just a human interface to the operating system. A more modern terminal would be something like an IBM 3486:\n(By ClickRick - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=6693700)\nThis is a very small computer in its own right but still basically just a dumb screen and keyboard connected by a cable to a larger mainframe computer in another location.\nThis mechanism is still very much the case today. When I want to work with a computer in a data centre, I don't go and find the machine, plug in a keyboard and a display and directly interface to it. I run a terminal program on my computer to provide access to the remote machine. My terminal program allows me to use my keyboard and display to work with a remote machine - all via a secure shell - which is a secured-shell connection over a network.\nSo terminals in many ways are quite simple - they are interfaces. But because they are quite simple programs, we can't do much with them. So normally, the first thing that a terminal program will do is run a shell program - a program that we can use to operate the computer.\nThere's nothing special about terminals - anyone can write a program to operate as a terminal, which is why you will see many different terminals around. Examples are the standard \u0026lsquo;terminal\u0026rsquo; app for MacOS X, the gnome-terminal for Linux, and iTerm2 and Hyper. There's a bunch of screenshots of different setups at the end of the article.\nBack to the Shell Now that we've described the terminal, we can go back and look at the shell in detail.\nThe shell is the program that is going to take input from somewhere and run a series of commands. When the shell is running in a terminal, it is normally taking input interactively from the user. As the user types in commands, the terminal feeds the input to the shell and presents the output of the shell on the screen.\nA shell program can also take input from files; these files will then generally be \u0026lsquo;shell scripts\u0026rsquo;. This might be used to run automated operations, such as cleaning up certain folders when a computer starts.\nShells can write output to files or other locations, and so on. You can run a shell program outside of a terminal - you just won't be able to interface with it using a keyboard or display. And in fact, lots of operations happen in this way: automated scripts, startup tasks, installers and so on.\nSo what else does a shell do? Most of the features are related to helping human operators work with the system more efficiently.\n Quickly enter commands, see the history of commands and quickly restructure commands (see Chapter 8 - Fly on the Command Line) Navigate through the file system, moving from folder to folder (see Chapter 1- Navigating Your System), which makes it easier for an operator to navigate the file system Chain the output of commands together - for example, taking the output of one basic program, such as the tree program we saw, and writing it to a file (see Chapter 7 - Thinking in Pipelines) Offer a programming language, allowing the operator to perform more complicated tasks  And a lot more! In fact, that's what the whole book is about - how to get the most from these powerful programs, particularly for those who use them regularly.\nThe Command Prompt or Command Line The last part of the diagram, which we haven't covered yet, is the command prompt.\nWhen a shell is running in terminal, it knows that a human operator will be interfacing with it. So to make sure that the operator has some kind of visual hint that they have to enter commands, the shell will output some kind of prompt.\nI've included a set of screenshots at the end of the article, just after this section, and you can see how some different command prompts look.\nNote that shells don't have to use command prompts - if you use a shell program to execute a script, there will be no command prompt. Shells only show a prompt when they know they are being used interactively. Many programs which allow a user to operate interactively will show a command prompt.\nShell command prompts can be customised, so they will often look different from machine to machine. Below is an example that shows a lot of technical information. This is from the highly popular oh-my-zsh framework for the \u0026lsquo;Z Shell\u0026rsquo; shell, which is very popular among developers:\n*(Source: https://ohmyz.sh/)\nShell Commands and Different Shells A lot of the \u0026lsquo;commands\u0026rsquo; in a shell, such as cat (which shows the contents of a file), are actually just simple programs, which will interface with the kernel. No matter what shell you use, these commands will behave the same way, because really all you are doing is calling another program.\nSome commands, such as cd (change directory), are built into the shell. Some commands are functions that have been defined, or aliases to other commands (for more details on commands, see Chapter 10 - Understanding Commands). Commands will often differ between shells.\nNot all shells are created equal - anyone can write a shell program, maybe creating a simple interface to the computer or a highly complex one with many features. In fact, a later article in this series will look at the genealogy of the most common shells.\nOn most Unix-like systems, the default shell is a program called bash, which stands for \u0026quot; Bourne Again Shell\u0026rdquo; (the name and history around it will be discussed at length in the later article). But there are many other shells: the C Shell, the Korn Shell, Z Shell and Fish, just to name just a few.\nUsers and administrators can configure what shell they like to use. When a terminal opens, it will immediately start the user's preferred shell program. It is possible to change this. Different users will have different preferences, given that shells offer varying features. This can cause complexity when working with systems, as we cannot always expect every user to have the same shell, or even for the same shell to be set up consistently, as they can be extensively customised.\nLet's review the earlier diagram again:\nWe can see the real internals of what is going on in this \u0026ldquo;Terminal -\u0026gt; Shell -\u0026gt; Program\u0026rdquo; chain in the diagram above quite easily.\nTry the command pstree -psa $$ in a shell8:\nThe first systemd process is the primary process for the OS - it is process number 1, which initialises everything else. The second systemd process is the process that is running the interface for my user. We can ignore these for now; they are internals to how the operating system boots and starts processes.\nWhat is interesting is that we can see a terminal (the gnome terminal), which has started my preferred shell (which is zsh), which is running a command (the program pstree). Here we can see the exact chain as shown in the diagram earlier.\nThat's a Wrap! These are the key technologies and concepts that surround a shell.\nIf you are interested in more technical details of working with shells, then my Effective Shell series goes into these topics in depth. The goal of this series is to help teach techniques that making working with shells more efficient.\nTo close the article, below are some examples of different terminals, shells, command prompts and so on.\nExample: iTerm 2 / tmux / zsh In this example, we have:\n A MacOS operating system iTerm2 as the terminal program tmux running as a \u0026lsquo;terminal multiplexer\u0026rsquo; (see Effective Shell: Terminal Multiplexers) zsh (Z Shell) as the shell program, using \u0026lsquo;oh my zsh\u0026rsquo;, which is easily recognised by the % sign in the command prompt. A customised command line, which shows the user and folder on one line, with only the % symbol below, to leave lots of space for the input commands[^10].  Example: Bash In this example, we have:\n A Linux operating system (Ubuntu 14) The gnome terminal bash as the shell In the second screenshot, the user has \u0026lsquo;root privileges\u0026rsquo;, and to indicate this, bash helpfully changes the default command prompt from a dollar sign to a hash sign  Example: Windows Explorer In this example, we have:\n The Windows 10 operating system No terminal The explorer.exe program showing us a graphical shell  This looks different from previous examples. The program, which shows the familiar Windows interface, explorer.exe, is in fact a shell as well, offering interactive access to the operating system and computer resources. The bulk of the Windows APIs to interact with this interface are in the Shell Library. I also maintain a popular library for building extensions to the graphical Windows shell - sharpshell.\nExample: Windows Command Prompt In this example, we have:\n The Windows 10 operating system The command prompt terminal and shell  In Windows, the terminal and shell are combined into a single cmd.exe program. There's an excellent article on the internals - Microsoft DevBlogs: Windows Command-Line: Inside the Windows Console\nExample: Windows PowerShell In this example, we have:\n The Windows 10 operating system The PowerShell terminal  PowerShell is an improvement on the \u0026lsquo;command prompt\u0026rsquo; program that was originally used in Windows, offering much more functionality for scripting and other modern shell features.\nExample: Windows Subsystem for Linux (WSL) In this example, we have:\n The Windows 10 operating system The Bash.exe program  This screenshot, from MSDN: Frequently Asked Questions about Windows Subsystem for Linux shows Bash running in Windows. This is a relatively new feature at the time of writing, allowing Windows users to use a Linux interface to the PC. This is a feature that may become increasingly valuable, as in general it is challenging to write shell code that can run on Windows and Unix-like systems.\nShare and Discuss If you enjoyed this article, please do share it! Feel free to include suggestions, improvements or corrections in the comments below.\n Useful References\n A simple Linux kernel module, showing how basic kernel programming works in Linux: github.com/dwmkerr/linux-kernel-module How Linux Works - Brian Ward StackExchange: What is the exact difference between a \u0026lsquo;terminal\u0026rsquo;, a \u0026lsquo;shell\u0026rsquo;, a \u0026lsquo;tty\u0026rsquo;, and a console? Microsoft: Inside the Windows Console   Footnotes\n  CPU: central processing unit. This is the chip in the computer that does most of the work (which after many layers of abstraction eventually becomes arithmetic and sending simple instructions to other places). \u0026#x21a9;\u0026#xfe0e;\n Memory is the \u0026lsquo;working space\u0026rsquo; where the state of your system is stored. If you are writing a document, the text lives in memory, until you save it, when it then gets written to a hard drive. Memory is ephemeral - everything is gone when you turn off the power to it. \u0026#x21a9;\u0026#xfe0e;\n This is the part of your computer that knows how to do things like connect to a WiFi network, or has a network socket you might plug a network cable into. \u0026#x21a9;\u0026#xfe0e;\n This is the part of your computer you plug the screen into. \u0026#x21a9;\u0026#xfe0e;\n This is because a mistake in Kernel Mode programs can have disastrous effects. It could access any files, no matter who they belong do, control the hardware, install more software - almost anything. Errors in this code can cause terrible issues (like the infamous Windows \u0026lsquo;blue screen of death\u0026rsquo;), and malicious code in the kernel essentially has full access to not only all your data but also your webcam, network adapter and so on. \u0026#x21a9;\u0026#xfe0e;\n As an aside, if you are curious about the visual style of my setup or customisations that have been made, everything in my setup is available online on my \u0026lsquo;dotfiles\u0026rsquo; repo - github.com/dwmkerr/dotfiles. \u0026#x21a9;\u0026#xfe0e;\n And that's where the \u0026lsquo;TTY\u0026rsquo; acronym you will see sometimes comes from. Enter the ps command, and you'll actually see the TTY interface each process is attached to. This is a topic that will come up later in the series. \u0026#x21a9;\u0026#xfe0e;\n $$ is a Bash internal variable. These will also be covered in a later article in the series. \u0026#x21a9;\u0026#xfe0e;\n   "});index.add({'id':13,'href':'/docs/part-3-manipulating-text/regex-essentials/','title':"Regex Essentials",'content':"Chapter 13 - Regex Essentials Many of the tools we're going to introduce in this part of the book support regular expressions or regexes - a sophisticated language which allows us to describe different patterns of text.\nBefore we look at how to use regular expressions in the shell, it is important to understand some of the basic regular expression concepts and techniques. This chapter covers the essentials - if you are already familiar with regular expressions feel free to skip to the next chapter.\nIn this chapter we'll look at why regular expressions can be intimidating, how to manage complexity and not overwhelm yourself, some of the core concepts in regular expressions and a few things to watch out for. Once we've seen the theory we'll be able to apply it in practice in the following chapters!\nAn Introduction to Regular Expressions Regular Expressions (or regexs) are special \u0026lsquo;patterns\u0026rsquo; which describe text. They are infamous (or even notorious) amongst technologists as being complex and opaque. For many years I avoided regular expressions as I found them overly complicated and hard to reason about. But over time I discovered that used carefully, they can be incredibly powerful and useful.\nIt is no surprise that regular expressions can be seen as opaque. Let's say we wanted to find a way to see whether an arbitrary string matches the structure of a valid email address. A quick search on the internet will find a regular expression such as this:\n(?:[a-z0-9!#$%\u0026amp;'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%\u0026amp;'*+/=?^_`{|}~-]+)*|\u0026quot;(?:[\\x01-\\x08 \\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\u0026quot;)@(?:(? :[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[ 0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0 -9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\ x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\]) Note that I have split the expression into multiple lines so that it fits on the page.\nThis is horrendously complex. It is extremely long, almost impossible for even an experienced user to parse or reason about, and attempting to change or modify it would be very risky.\nMany people see examples like the above and decide (probably quite sensibly) that regular expressions are something they just simply will not learn - they're too complex.\nSo should we learn regular expressions? Are they even valuable if they are as complex as the example above?\nManaging Complexity with Regular Expressions Regular expressions do not have to be as complex as the example above. In fact, in most cases they shouldn't be. My general advice for regular expressions is start simple and add complexity only if you need it.\nWe can build regular expressions using an \u0026lsquo;iterative\u0026rsquo; process, starting with the basics, then adding more features as we need them. An invaluable tool I have used for this is the website regex101.com. This website not only lets you test regular expressions, it also breaks down how they work so that you can reason about what is happening.\nLet's take validating an email address as an example. The way I would build a regular expression to validate an email address would be to use the following steps:\n Create a small list of valid email address Add some items to the list which look \u0026lsquo;kind of\u0026rsquo; valid but are not quite right Build a regular expression which matches the correct email address Refine the expression to eliminate the invalid addresses  In most cases this will be sufficient.\nLet's start with the following set of addresses:\ndave@effective-shell.com dave@effective-shell to: dave@effective-shell.com dave@effective-shell.com \u0026lt;Dave Kerr\u0026gt; test123.effective-shell.com @yahoo.com whatever123@😂.com dave@kerr@effective.shell.com Some are valid, some are not. Some you might not be sure about - such as the one with the emoji. This one is valid if someone sets up a mail server which can handle it, but probably not a good address to use as some mail programs or servers will reject it (Gmail for example, at time of writing, would allow you to send and receive from an email address like this, but not create an email address like this).\nBuilding Regexes - Start with the Basics Here's how I would start building a regex for an email address:\n Any set of characters Followed by an @ ampersand Followed by any set of characters  That regex would look like this:\n.*@.* The first bit, .* means \u0026lsquo;any character\u0026rsquo; (this is what the . dot symbol means), \u0026lsquo;any number of times (this is what the * asterisk symbol means).\nThe second bit is just the literal @ ampersand character.\nThe third bit is the same as the first - any characters any number of times.\nLet's see how that would look in regex101:\nHere we can see in blue the lines that the regex has matched. We can also see which part of each line corresponds to which part of the expression. But perhaps the most useful thing we see is the \u0026lsquo;explanation\u0026rsquo; on the right which explains exactly what each character does.\nNow that we have a basic pattern which matches the valid address, we can refine it to eliminate invalid addresses.\nNote that from this point onwards we'll not show screenshots of the results as you would see them in the regex101 website, instead we'll just highlight the matched part of the text in bold, this should make it easier to read. But to see a breakdown of how each part of the text is matched and what each part of the pattern means, feel free to run the examples in the regex101 website!\nBuilding Regexes - Quantifiers The regular expression we have is very simple - .*@.*. The complexity in regular expressions tends to come from the fact that we need to handle \u0026lsquo;edge cases\u0026rsquo; and be very explicit about what we can and cannot allow.\nLet's see how we can refine this expression further to eliminate some of the invalid addresses. Let's start with `@yahoo.com`. It doesn't have anything before the ampersand.\nThis is being matched by our pattern because our pattern allows any characters before and after the ampersand any number of times - including zero times.\nLet's change number of characters before and after the ampersand to be \u0026lsquo;between one and many\u0026rsquo;. To do this we use a different quantifier (a \u0026lsquo;quantifier\u0026rsquo; is the part of a pattern which says \u0026lsquo;how many occurrences of the characters do we expect).\nPreviously we used the * ampersand quantifier (which means \u0026lsquo;any number of times\u0026rsquo;). Now we'll use the + plus quantifier (which means \u0026lsquo;at least one time\u0026rsquo;). Let's see how it looks:\n .+@.+ dave@effective-shell.com dave@effective-shell to: dave@effective-shell.com dave@effective-shell.com \u0026lt;Dave Kerr\u0026gt; test123.effective-shell.com @yahoo.com dave@ whatever123@😂.com dave@kerr@effective.shell.com  This is better - we've eliminated some invalid addresses, test123.effective-shell.com, @yahoo.com and dave@.\nWe have introduced a key concept - the quantifier. The quantifier we have used is the + plus symbol. This is the part of a regular expression which says \u0026lsquo;how many times can a character be matched?'.\nThere are a few different quantifiers, here is a quick reference:\n   Quantifier Meaning     * Any number of characters.   + At least one character.   ? Between zero and one character.   {10} Exactly ten occurrences of the character.   {10,} Ten or more occurrences of the character.   {10,20} Between ten and twenty occurrences of the character.    Now let's look at the character itself.\nBuilding Regexes - Character Sets and Metacharacters When we are matching text, we match a set of characters a number of times. The set of characters we match can be a character set (which is when we explicitly say what is allowed), or a metacharacter (which is a predefined character set). This concept is far easier to explain with an example.\nLet's look at the address dave@kerr@effective.shell.com. This is clearly invalid, it has two ampersands. We can use character sets or metacharacters to fix this.\nThe reason this address matches our expressions is that we are using the . dot metacharacter before and after the ampersand. The dot metacharacter means \u0026lsquo;any character\u0026rsquo; (except a newline). This includes the ampersand character.\nThere are a few ways we would be more explicit. Let's look at each of them, as each one will show a character set or metacharacter in detail.\nCharacter Sets - Ranges\nA character set starts and ends with square brackets. We can use letters or numbers with a hyphen in-between to denote a range of characters for the character set. For example:\n[A-Za-z0-9] This character set matches any of the letters A to Z (uppercase) or a-z (lowercase) or the digits 0-9. Let's see how it looks with the pattern:\n [A-Za-z0-9]+@[A-Za-z0-9]+ dave@effective-shell.com dave@effective-shell to: dave@effective-shell.com dave@effective-shell.com \u0026lt;Dave Kerr\u0026gt; test123.effective-shell.com @yahoo.com dave@ whatever123@😂.com dave@kerr@effective.shell.com  This fails to match the valid email address `dave@effective-shell.com` - because it has a hyphen after the ampersand, and the hyphen character is not in our character set. It also fails to match others for the same reason - we haven't got the \u0026lsquo;dot\u0026rsquo; character in our character set.\nLet's see how we can do better.\nCharacter Sets - Special Characters\nWe can add more characters to our character set. To include the dot and the hyphen, we just add them directly to the set:\n[A-Za-z0-9-.] That's all there is to it! We can now see our pattern is more correct:\n [A-Za-z0-9-.]+@[A-Za-z0-9-.]+ dave@effective-shell.com dave@effective-shell to: dave@effective-shell.com dave@effective-shell.com \u0026lt;Dave Kerr\u0026gt; test123.effective-shell.com @yahoo.com dave@ whatever123@😂.com dave@kerr@effective.shell.com  However, the expression is getting larger and larger. We can use a metacharacter instead of the character range to make it easier to read. A metacharacter is a special character which is used to represent a range of characters. For example:\n\\w+@\\w+  \\w+@\\w+ dave@effective-shell.com dave@effective-shell to: dave@effective-shell.com dave@effective-shell.com \u0026lt;Dave Kerr\u0026gt; test123.effective-shell.com @yahoo.com dave@ whatever123@😂.com dave@kerr@effective.shell.com  The uses the \u0026lsquo;word\u0026rsquo; metacharacter, \\w. This is just a shorthand for [a-zA-Z0-9_]. But now our pattern fails again as the \\w metacharacter doesn't include the hyphen or the dot. We can fix this easily - a character set can include metacharacters! So we can just combine \\w and the hyphen and dot:\n [\\w+-.]+@[\\w+-.]+ dave@effective-shell.com dave@effective-shell to: dave@effective-shell.com dave@effective-shell.com \u0026lt;Dave Kerr\u0026gt; test123.effective-shell.com @yahoo.com dave@ whatever123@😂.com dave@kerr@effective.shell.com  Character Sets - Negating Characters\nWe can use the ^ circumflex symbol to negate a character. This allows us to build a character set which doesn't match a pattern. For example, we could rewrite our pattern like this:\n [\\S^@]+@[\\S^@]+ dave@effective-shell.com dave@effective-shell to: dave@effective-shell.com dave@effective-shell.com \u0026lt;Dave Kerr\u0026gt; test123.effective-shell.com @yahoo.com dave@ whatever123@😂.com dave@kerr@effective.shell.com  We've used the character set [\\S^@] which means \u0026lsquo;any none-whitespace character\u0026rsquo; (this is the \\S part) and \u0026lsquo;not the ampersand character\u0026rsquo; (this is the ^@ part).\nNotice that in this case we have more matches - because the set of characters we are using is larger than a set such as \\w. This expression now covers the email address with the emoji, because the emoji is not a whitespace character or an ampersand.\nCharacter Sets - Escaping Characters\nWhat do you do if you actually want to use the circumflex symbol in a character set? Just escape it! This means you put a slash first, the regex will then treat the character which follows the slash as a literal character. This is how we can match special characters like square brackets.\nHere's an example:\n[\\[\\]]+ This matches square brackets between one and many times.\n[\\^]\\+ This matches the circumflex followed by the plus sign. If there is ever a point at which you need to use a special character in a regular expression as a literal character, escape it by putting a slash in front of it.\nCharacter Sets - Quick Reference\nWe've seen quite a few character sets and metacharacters, here's a quick reference for some commonly used ones:\n   Quantifier Meaning     . Any character except for a line break.   \\w Any \u0026lsquo;word\u0026rsquo; character, [a-zA-Z0-9_].   \\W Any non \u0026lsquo;word\u0026rsquo; character.   \\s Any \u0026lsquo;whitespace\u0026rsquo; character (space, tab, etc).   \\S Any non \u0026lsquo;whitespace\u0026rsquo; character.   \\d Any \u0026lsquo;digit\u0026rsquo; character [0-9].   \\D Any non \u0026lsquo;digit\u0026rsquo; character.    There are many other metacharacters, you can find more in the manpage man re_pattern\nBuilding Regexes - Anchors At the moment, if we use the expression below:\n [\\S^@]+@[\\S^@]+ dave@effective-shell.com dave@effective-shell to: dave@effective-shell.com dave@effective-shell.com \u0026lt;Dave Kerr\u0026gt; test123.effective-shell.com @yahoo.com dave@ whatever123@😂.com dave@kerr@effective.shell.com  Then we match any line which contains an email address. But what if we only want to match complete email addresses? What if we need to exclude lines which have extra stuff at the beginning or end?\nFor this, we can use anchors. Anchors represent special parts of a string, such as the start or beginning of a line.\nIf we want to only match lines which contain a complete email address, we can use the ^ circumflex (start of line) anchor and $ dollar (end of line) anchor:\n ^[\\S^@]+@[\\S^@]+$ dave@effective-shell.com dave@effective-shell to: dave@effective-shell.com dave@effective-shell.com \u0026lt;Dave Kerr\u0026gt; test123.effective-shell.com @yahoo.com dave@ whatever123@😂.com dave@kerr@effective.shell.com  This allows us to create expressions which match patterns of text at certain points - anchors. For example, if we want to match any line which starts with the letters to:  we could just use this:\n ^to: .* dave@effective-shell.com dave@effective-shell to: dave@effective-shell.com dave@effective-shell.com \u0026lt;Dave Kerr\u0026gt; test123.effective-shell.com @yahoo.com dave@ whatever123@😂.com dave@kerr@effective.shell.com  You will see the start of line and end of line anchors quite often, they can be extremely useful when making a regular expression more specific.\nBuilding Regexes - Capture Groups You can extract only part of what you match in a regular expression using capture groups. A capture group lets you break up the expression into smaller parts and then operate on either the entire match, or only one of the groups.\nHere's an example:\n(.+)@(.+) \u0026lt;strong\u0026gt;dave@effective-shell.com\u0026lt;/strong\u0026gt; Now the entire line matches, but everything surrounded by () parentheses is a capture group. This means that the regular expression has actually made three matches:\n dave@effective-shell.com - The first match in an expression is always the complete match dave - This is the first capture group, everything before the ampersand effective-shell.com - This is the second capture group, everything after the ampersand.  We're actually going to see how to use capture groups directly in the shell in the next chapter so we won't go into much more detail now.\nBuilding Regexes - Lazy and Greedy Expressions Regular expressions can be lazy or greedy. Whether an expression is lazy or greedy affects how it matches patterns - basically whether it stops at the earliest point the pattern is matched, or continues until the pattern no longer matches.\nRegular expressions are greedy by default. This means that if you are matching a pattern, the regular expression will capture as much as it possibly can - all the way to the last match. As an example, let's look at how we might capture the contents of an html tag:\n \u0026lt;.+\u0026gt; This text is \u0026lt;strong\u0026gt;bold\u0026lt;/strong\u0026gt;.  The regular expression \u0026lt;.+\u0026gt; matches an angled bracket, then at least one character, then a closing angled bracket. Because regular expressions are greedy by default, it captures all the way until the last angled bracket on the line - i.e. everything up until the closing \u0026lt;/strong\u0026gt; tag.\nWe can create a lazy expression by using the ? question mark symbol after the quantifier - this means that the expression will capture as few characters as possible until the end of the pattern is found:\n \u0026lt;.+?\u0026gt; This text is \u0026lt;strong\u0026gt;bold\u0026lt;/strong\u0026gt;.  In this example we have actually captured two results - the contents of the opening and closing tag. Because the expression \u0026lt;.+?\u0026gt; is lazy it matches only until the first closing brace it finds, meaning that the results are quite different.\nTo get the same results without using the lazy quantifier, we'd have to have an expression like this:\n \u0026lt;[^\u0026gt;]+?\u0026gt; This text is \u0026lt;strong\u0026gt;bold\u0026lt;/strong\u0026gt;.  In this case we've changed the match to \u0026lsquo;any character which is not a closing brace\u0026rsquo;. Whether this is easier for the reader to understand than the lazy quantifier is hard to say, but it is useful to understand the difference between lazy and greedy expressions.\nAvoiding Advanced Topics - Backtracking, Lookarounds and Atomic Grouping I think that if you have the basics of quantifiers, character sets and metacharacters and capture groups, then you are probably well equipped to use regular expressions. Knowing how to make an expression lazy can also make working with regexes more straightforward.\nHowever - you might come across a few terms when you are working with regular expressions which may be unfamiliar. These relate to perhaps more advanced topics. I'll give brief overview here, but if you have had your fill of regexes for now then you can safely skip to the next section!\nI am not going to show examples of each of these concepts. I'll explain why after a brief summary of the concepts.\nBacktracking - this refers to the process a regular expression engine goes through to try and identify a greedy match. In short, it is possible to inadvertently write a regular expression which has exponential processing complexity based on the length of the input string.\nThis has led to cases of what is called \u0026lsquo;catastrophic backtracking\u0026rsquo; - where the processing involved to match a pattern can cause system failures or even lead to exploits1.\nIn short - very broad and greedy expressions such as .+ (match anything at least once) may be susceptible to this problem. Be careful when writing your expressions to test them with short and long strings to see if there's a noticeable performance difference. Regex101 and other tools can show you if your expression is time consuming. Avoid this by making expressions lazy when you can and matching more explicit characters.\nLookarounds are special constructs which allow you to essentially say \u0026ldquo;find me a pattern, but only if it comes before or after another pattern\u0026rdquo;. A lookahead is used to say \u0026ldquo;find me a pattern, but only match it if it comes before another pattern\u0026rdquo;, a lookbehind says \u0026ldquo;find me a pattern, but only match it if it comes after another pattern\u0026rdquo;. There are \u0026lsquo;negative\u0026rsquo; lookaheads and lookbehinds which essentially say \u0026ldquo;find me a pattern which is not preceded or followed by another pattern\u0026rdquo;.\nAs an example, the expression \\d+(?=€) matches digits (this is the \\d metacharacter), at least one or more (this is the + plus symbol), but only if the digits are followed by a Euro symbol. In this case the (?=€) part of the pattern is a \u0026lsquo;positive lookahead\u0026rsquo;.\nI have not yet found a situation where I've really needed lookaround expressions. For example, I would simply write the above expression as:\n(\\d+)€ Which simply matches digits which precede a Euro symbol and puts them in a capture group.\nAtomic Groups are a more advanced construct which can be used to avoid backtracking which is described above. Lookarounds are atomic. Essentially when an atomic group is matched all backtracking ceases, so can provide a \u0026lsquo;get out\u0026rsquo; clause to avoid catastrophic backtracking.\nThis is somewhat opinionated, but in my years of engineering I am yet to find a situation which genuinely was made more simple with the use of lookarounds or atomic groups. I would instead advise that if your expression is highly complex, find a way to break up the input first and then process it in multiple steps. That will likely lead to scripts and code which is easier for others to read and reason about.\nA Word of Warning Different tools process regular expressions in different ways. There are subtle differences between how they are processed in Bash, JavaScript, Perl, Python, Golang and other languages. This can make them painful to work with.\nIn general most of the features we've seen in this chapter will work the same regardless of the tool you are using, but as you move into more sophisticated features, you may find that some tools have slightly different syntaxes for certain types of capture groups. However, this generally only affects the more advanced features such as named capture groups (which is a special syntax allowing you to give capture groups a descriptive name).\nI would advise that you keep expressions simple if possible - if they are getting too complex then break up your input or break up the processing into smaller chunks of work!\nRemember that a regular expression does not have to be the only way you validate input. You might use a regular expression to do a quick check on a form on a website to make sure that an email address has at least the correct structure, but you might then use a more sophisticated check later on (such as sending the user an activation email) to actually confirm that the address actually belongs to the user.\nUsing a website like regex101 you can quickly check how a regex works with different tools. Wherever you might encounter these differences in content in this book I've tried to call it out!\nSummary Hopefully this gives a basic grounding in the fundamentals of regular expressions. Knowing only a few concepts - types of characters, quantifiers and capture groups is plenty for most people. And the online tool regex101 is a superb way to learn regular expressions.\nNow we've learned the theory - in the next chapter we'll see some built-in ways to manipulate text in the shell, which include some clever regular expression features.\n   There is a fascinating write up of how this led to a severe Cloudflare outage in 2019 available online at: https://blog.cloudflare.com/details-of-the-cloudflare-outage-on-july-2-2019/ \u0026#x21a9;\u0026#xfe0e;\n   "});index.add({'id':14,'href':'/docs/part-3-manipulating-text/get-to-grips-with-grep/','title':"Get to Grips with Grep",'content':"Chapter 14 - Get to Grips with Grep The grep tool is a real workhorse for shell users - once you've learned how to use it you will find yourself using it again and again. In this chapter we'll see how you can use grep for common tasks, and how to use it in combination with other tools.\nAs with the other tools we'll introduce in this chapter, we'll also look at when grep is the right tool for the job and when we should consider other options.\nWhat is Grep A quick check of the manual page for grep gives an overview:\n$ man grep GREP(1) BSD General Commands Manual GREP(1) NAME grep, egrep, fgrep, zgrep, zegrep, zfgrep -- file pattern searcher SYNOPSIS grep [-abcdDEFGHhIiJLlmnOopqRSsUVvwxZ] [-A num] [-B num] [-C[num]] [-e pattern] [-f file] [--binary-files=value] [--color[=when]] [--colour[=when]] [--context[=num]] [--label] [--line-buffered] [--null] [pattern] [file ...] DESCRIPTION The grep utility searches any given input files, selecting lines that match one or more patterns. By default, a pattern matches an input line if the regular expression (RE) in the pattern matches the input line without its trailing newline. An empty expression matches every line. Each input line that matches at least one of the patterns is written to the standard output. Wow. Lots of options for this command. And confusingly, lots of alternative forms as well (such as egrep, zgrep and so on).\nMaybe the tldr tool will provide a more concise overview?\n$ tldr grep grep Matches patterns in input text. Supports simple patterns and regular expressions. That is indeed a little more concise. By the way, if you are not familiar with how you can get help on commands, check out Chapter 5 - Getting Help. As the manual pages indicate, grep is used to match patterns in files. More advanced users will most likely know exactly what this means, but a more simple description is just:\n Grep lets you search for text or filter text.\n That's it. You can search in files, but you are not limited to searching in files. And you can search for literal text, such as the word \u0026lsquo;error\u0026rsquo;, or you can search for patterns. Patterns in this case means regular expressions - expressions which allow you to be more specific in how you search (such as looking for a set of 16 numbers in a row, like a credit card number, or any text which looks like an email address). You can also do the opposite - filtering out parts of text.\nWe'll use grep to search through text. Let's get straight into it!\nWhy Grep? Why the odd name? Grep is such a commonly used tool that the name has become a verb amongst technologists (people will often suggest you grep for something in files).\nThe name comes from a command which was used in the original ed text editor - the command:\ng/re/p This command ran on all lines (g, for global), applied a regular expression (re, for regular expression) and then printed (p for print) the results. A colleague of Ken Thompson, one of the early innovators and inventors in the Unix world, needed to edit a large file - a file which was too large to fit in ed. Ken wrote the grep program overnight to allow the file's text to be filtered - and the results passed to the ed editor!\nYou can read more about this story and some of the fascinating history of the early days of Unix in a great interview with Brian Kerninghan from Computerphile1.\nSearching Through Text If you've been working through this book, you've probably entered quite a few commands in the shell. Most shells keep a history of the commands you type. Under the hood, when you use the up and down keys to look through commands you entered earlier, or use the Ctrl-R shortcut to search through earlier commands, your shell is looking through this file. If these tricks are not familiar, check Chapter 9 - Fly on the Command Line.\nThe file which keeps the history can vary from shell to shell. For example, on my system, my history for Bash is in the file ~/.bash_history. But most \u0026lsquo;Bash-like\u0026rsquo; shells provide a built-in environment variable which let's you find the path of the shell history. Let's at this file:\ncat $HISTFILE ... cat ~/.ssh/config ssh bastion.cloudops help echo help cd exit This file will generally contain a list of the commands that the logged in user has entered in their shell. This file will likely be huge. Let's search through it using grep! Here's how we can use the grep command to search for lines which contain the text man:\n grep man $HISTFILE ... man socket k describe services eventstoredb-http-management man cal gcb refactor/performance/standardise-eventstore vi src/tests/handlers/test_command_handlers.py gco src/handlers/command_handlers.py gcb feat/performance/use-eventstore-writer nvim performance.md man grep  Here I can see all of the commands I have recently entered which have the text man in them. Note that the text which matches is highlighted and shown in bold.\nNow what if you a different shell, or forget where the history file lives? A nice trick here is to use the history command. This command prints out the history, as well as the line number. The history command writes to stdout. If we don't give grep a source file, it will simply search through stdin. Just as we learnt in Chapter 7 - Thinking in Pipelines this means we can just grep the output of the history command!\nHere's how that would work:\n history | grep man ... 9125 man socket 9188 k describe services eventstoredb-http-management 9211 man cal 9341 gcb refactor/performance/standardise-eventstore 9344 vi src/tests/handlers/test_command_handlers.py 9347 gco src/handlers/command_handlers.py 9352 gcb feat/performance/use-eventstore-writer 9355 nvim performance.md 10002 man grep  This is easier to remember! There's one more cool trick - if we just type in the exclamation point symbol followed by any line number shown above, we repeat the command! For example, typing in:\n!9355 Would repeat line 9355 of the history (which is nvim performance.md).\nUsing Patterns Now as you can see from the output above, when we searched through my history, we didn't just find times I executed the man command - we found any line which has the characters man in it. What about if we only wanted to find the lines which start with man?\nTo perform a search like this, we can use a regular expression. Here's how it would work:\n history | grep \"[0-9]\\+ man\" ... 9125 man socket 9211 man cal 10002 man grep  Let's break this down. In this search, we are using a pattern to search for text. The pattern in this case is a basic regular expression. Regular expressions allow us to use some clever constructs to search for text. The expression we've used is made up of the following components:\n [0-9]\\+ At least one number - any character in the range zero to nine.  man The literal text written, i.e. two spaces and the letters man.  Now for anyone who is familiar with regular expressions, you might wonder why we have a slash before the + symbol, when the + symbol has a specific meaning in regular expressions (it means \u0026lsquo;at least one of the previous characters). The reason we have a leading slash is that by default grep is using basic regular expressions. In general, this will be less familiar for users and will be different to what they are used to from different tools.\nTo make regular expressions more \u0026lsquo;standard\u0026rsquo;, we can use the -E flag to tell grep to use Extended Regular Expressions. We can also use the egrep tool, which assumes the pattern will be an extended regular expression. Using either approach will work, and allow you to re-write the pattern as below:\nhistory | grep -E \u0026quot;[0-9]+ man\u0026quot; # ...or... history | egrep \u0026quot;[0-9]+ man\u0026quot; This is just a little hint of the power of regular expressions. They can be daunting at first, and many people never become comfortable with them, but I would strongly encourage you to start exploring them.\nGetting Help on Regular Expressions\nIf you want to find out more about the difference between the slightly old-fashioned basic regular expressions and modern regular expressions, you can use:\nman re_format This manpage gives lots of information on regular expressions, including the differences between basic and extended patterns.\n If you ever want to see how a regular expression works, try using the website regex101.com. It let's you test out regular expressions and also describes exactly how they work. For example, if I enter the regular expression we just used I'll see this:\nWe're going to see more about regular expressions as we go through the book.\nFinding Problems If there's one command I use a lot, it's this:\ngrep -i err The -i flag makes the search case-insensitive. This makes this a very quick way to scan through a file for any text which matches the letters err - making it a very quick way to find errors in log files.\nYou can try this out by using some of the log files in the logs folder of the playground. Here's how you can try it out:\n grep -i err ~/effective-shell/logs/web-server-logs.txt ... 2020-11-29T12:50:30.594Z: info - Serving file '../../../website/public/docs/part-2-core-skills/7-thinking-in-pipelines/images/diagram-stderr-redirect.png'... 2020-11-29T12:50:31.827Z: error - Unhandled error EACCES trying to read '../../../website/public/svg/calendar.svg', returning a 500 2020-11-29T12:50:31.827Z: error - Unhandled error EACCES trying to read '../../../website/public/svg/calendar.svg', returning a 500 2020-11-29T12:50:31.827Z: error - Unhandled error EACCES trying to read '../../../website/public/svg/calendar.svg', returning a 500 2020-11-29T12:50:31.848Z: error - Unhandled error EACCES trying to read '../../../website/public/svg/edit.svg', returning a 500 2020-11-29T12:50:31.849Z: error - Unhandled error EACCES trying to read '../../../website/public/svg/edit.svg', returning a 500  This is a very useful trick. You could use this technique to search for warnings, problems, specific messages and so on.\nThe ABC of Grep There are three really useful parameters for grep, which I used to struggle to remember, until I realised that they are simple - ABC!\nHere's how they work:\n $ grep host -A 3 ./programs/web-server/web-server.js host: process.env.HOST || 'localhost', port: process.env.PORT || getOptonalEnvInt('PORT', 8080), root: process.env.ROOT || process.cwd(), defaultPage: 'index.html', -- httpServer.listen({ host: config.host, port: config.port }); log.info(`Server running on: ${config.host}:${config.port}`); } main();  A stands for after. In this example we show the three lines after each occurrence of the work host in the web-server.js script. This is a quick way to see how something you search for might be used!\nB stands for before - we can use this to see what comes before a match when we're searching. What can lead to us sending an error in our web server? Let's see:\n $ grep throw -B 5 ./programs/web-server/web-server.js // Helper to return an optional numeric environment variable or the default. function getOptonalEnvInt(name, defaultValue) { const val = process.env[name]; if (!val) return defaultValue; const intVal = parseInt(val, 10); if (isNaN(intVal)) throw new Error(`Unable to parse environment variable named '${name}' with value '${val}' into an integer`);  And finally C, the most useful of them all. C stands for context, and lets you see a number of lines before and after each match. What was I up to the last time I ran the git init command? Let's see!\n $ history | grep -C 5 'git init' 5802 git push --follow-tags \u0026\u0026 git push origin 5803 cd ../java-maven-standard-version-sample 5804 rm -rg .git 5805 rm -rf git 5806 rm -rf .idea 5807 git init -h 5808 git remote add origin git@github.com:dwmkerr/java-maven-standard-version-sample.git 5809 git push origin -u 5810 git push -u origin 5811 git push --set-upstream origin master 5812 git rm --cached tpm  Don't forget that these flags need to be capitalised! These three flags are very useful - knowing how to find context of a match can be a lifesaver when quickly searching through text.\nWorking with Multiple Files What about if you have a bunch of files you want to search? One problem we have at the moment is that everything we search through has been a single file. But if we are searching through multiple files, how can we identify where the matches come from?\nThere's a useful pair of flags for this. -H stands for \u0026lsquo;header\u0026rsquo;, which shows the file name before each match. -n stands for \u0026lsquo;number\u0026rsquo;, which makes sure the line number is shown. Here's how we might use this command:\n $ grep -Hn ERROR ./logs/apm-logs/*.logs ... ./logs/apm-logs/apm02.logs:34893:2020-11-27T12:24:37.429Z ERROR [request] middleware/log_middleware.go:95unauthorized {\"request_id\": \"53a41a98-ba12-454e-aadf-72c97dc40e96\", \"method\": \"POST\", \"URL\": \"/config/v1/agents\", \"content_length\": 27, \"remote_address\": \"127.0.0.1\", \"user-agent\": \"elasticapm-python/5.9.0\", \"response_code\": 401, \"error\": \"unauthorized\"} ./logs/apm-logs/apm02.logs:34906:2020-11-27T12:25:11.415Z ERROR [request] middleware/log_middleware.go:95unauthorized {\"request_id\": \"a49d5546-b8d2-4e50-9dd0-6cbf419a365e\", \"method\": \"POST\", \"URL\": \"/config/v1/agents\", \"content_length\": 27, \"remote_address\": \"127.0.0.1\", \"user-agent\": \"elasticapm-python/5.9.0\", \"response_code\": 401, \"error\": \"unauthorized\"}  Note that in this case we searched through many files - anything which matches the *.logs wildcard. To help us identify in which file the match was found, we used the -Hn flags. The beginning of the lines now start with the path of the file and the line number, for example:\n./logs/apm-logs/apm02.logs:34906 You can take this even further:\n$ grep -R -Hn -i error ./logs Adding the -R or recursive flag tells grep to search recursively in folders if they are included in the search.\nV for Invert As long as your remember that -i is the flag for case insensitive, it makes it a little easier to remember v for invert. This tells grep to exclude lines which match the pattern. This works kind of like a filter.\nHere's how I could look through my log files, excluding any messages with \u0026lsquo;debug\u0026rsquo; in them:\n$ grep -v debug ./logs/web-server.logs Don't forget, you can always pipe a series of grep commands together. Rather than trying to work out a perfect pattern which searches for exactly what you want, you could just pipe a a set of commands together:\n$ grep -i error -R ./logs | grep -i -v memory | grep -i -v 'not found' This set of small, simple, commands is chained together to make a more sophisticated operation:\n First we recursively search for any error text in the ./logs folder Then we exclude anything which matches memory Then we exclude anything which matches not found  This is the essence of the Unix Philosophy - with a small number of simple tools, we can compose a more complex workflow!\nDon't Forget Your Pipelines! We've introduced a very powerful command in this chapter. For familiar users, grep becomes a verb they use regularly - you grep the output of something, or might be grepping to find something. Remember that grep, just like most of the tools in this section, works on stdin by default. So you can easily grep the output of almost anything!\nHere are a few simple examples just to show you how easy it is to perform more complex tasks with grep.\nps -a | grep vim Show all processes, then filter the list down to only vim processes.\ngrep -Hv -C 3 -R password ./k8s/**/*.yaml | less Search through all of the yaml files in my k8s folder, for the text \u0026lsquo;password\u0026rsquo;, show three lines of context, as well as the file name and number, and put the output in my pager so that it is easy to search through.\nls -al /usr/bin /bin /usr/local/bin | grep zip Search through all of my installed programs for programs which have zip in the name.\nhistory | grep grep | tail -n 10 Show me the last ten grep commands I typed in my shell!\nWe'll see a lot more examples as we go through the book - just remember that grep is alwaays available to search or filter text!\nAlternatives to Grep Grep is a very commonly used tool and has been around for a long time. It can vary a bit from system to system. Over the years a number of alternatives have been developed. Most of these alternatives are either designed to be faster, so that you can search through files much more quickly, or easier so that you don't have to remember too many flags.\nIn general, I would advise against using alternatives - until you genuinely find you are limited by grep. Every alternative is another tool to learn, which might not be present on other systems you use. It is also less likely to be available if you are writing scripts or instructions for others.\nIf you find yourself really struggling with performance - perhaps you often search huge folders of text or if you find yourself regularly struggling to find ways to craft your search patterns, then perhaps you can investigate some of the popular alternatives. But I would suggest that you master the core grep functionality first, before installing other tools.\nIf you do decide you want to add some more text searching tools to your toolkit, I would suggest ripgrep, ag and ack as three potential options. Each of them offer performance improvements and additional functionality.\nSummary Grep is a simple text-based search tool! If you need to find text, or want to filter text, then grep should be your go-to tool.\nHere's a summary of what we've covered:\n grep pattern file searches file for the text pattern the -E flag lets you use regular expressions for more sophisticated searches You can make the search case insensitive with the -i flag Remember the ABC flags - after, before and context, which show lines after, before and around the matches Include the filename and line number with the -Hn flags V for invert! Use the -v flag to invert the search, or filter out matches grep works great in pipelines! Use it to search or filter when working with other commands   Footnotes\n  See the interview at: https://www.youtube.com/watch?v=NTfOnGZUZDk\u0026amp;feature=emb_title \u0026#x21a9;\u0026#xfe0e;\n   "});index.add({'id':15,'href':'/docs/part-3-manipulating-text/slice-and-dice-text/','title':"Slice and Dice Text",'content':"Chapter 15 - Slice and Dice Text In Chapter 14 we looked at how to use the grep command to search through text and filter text. In this chapter we're going to look at some of the basic commands which we can use to manipulate text. There are a whole raft of commands and options available.\nWe'll start with the basics and move onto some of the more sophisticated commands in the next chapter.\nHeads and Tails The commands head and tail are very simple but incredibly useful.\nhead is used to extract part of the top of a file and tail is used to extract part of the end of a file. Once you starting using these commands you'll find yourself using them regularly.\nLet's start with head. Imagine we have a data file which has been sent to us, we don't know exactly what is in it, but we know it is large. How can we take a quick look?\n$ head ~/effective-shell/data/top100.csv \u0026quot;Rank\u0026quot;,\u0026quot;Rating\u0026quot;,\u0026quot;Title\u0026quot;,\u0026quot;Reviews\u0026quot; \u0026quot;1\u0026quot;,\u0026quot;97\u0026quot;,\u0026quot;Black Panther (2018)\u0026quot;,\u0026quot;515\u0026quot; \u0026quot;2\u0026quot;,\u0026quot;94\u0026quot;,\u0026quot;Avengers: Endgame (2019)\u0026quot;,\u0026quot;531\u0026quot; \u0026quot;3\u0026quot;,\u0026quot;93\u0026quot;,\u0026quot;Us (2019)\u0026quot;,\u0026quot;536\u0026quot; \u0026quot;4\u0026quot;,\u0026quot;97\u0026quot;,\u0026quot;Toy Story 4 (2019)\u0026quot;,\u0026quot;445\u0026quot; \u0026quot;5\u0026quot;,\u0026quot;99\u0026quot;,\u0026quot;Lady Bird (2017)\u0026quot;,\u0026quot;393\u0026quot; \u0026quot;6\u0026quot;,\u0026quot;100\u0026quot;,\u0026quot;Citizen Kane (1941)\u0026quot;,\u0026quot;94\u0026quot; \u0026quot;7\u0026quot;,\u0026quot;97\u0026quot;,\u0026quot;Mission: Impossible - Fallout (2018)\u0026quot;,\u0026quot;430\u0026quot; \u0026quot;8\u0026quot;,\u0026quot;98\u0026quot;,\u0026quot;The Wizard of Oz (1939)\u0026quot;,\u0026quot;120\u0026quot; \u0026quot;9\u0026quot;,\u0026quot;96\u0026quot;,\u0026quot;The Irishman (2019)\u0026quot;,\u0026quot;441\u0026quot; The head command just shows the first ten lines of a file. Here we can see that this is a comma separated values file which seems to be a list of movies. This file is actually a list of the top 100 films on \u0026lsquo;Rotten Tomatoes\u0026rsquo; at the time of writing, with the score, tomato meter, name and number of votes. We'll use it a lot in this chapter to demonstrate text manipulation.\nYou can use the -n flag to specify the number of lines you want to see, for example:\n$ head -n 3 ~/effective-shell/data/top100.csv \u0026quot;Rank\u0026quot;,\u0026quot;Rating\u0026quot;,\u0026quot;Title\u0026quot;,\u0026quot;Reviews\u0026quot; \u0026quot;1\u0026quot;,\u0026quot;97\u0026quot;,\u0026quot;Black Panther (2018)\u0026quot;,\u0026quot;515\u0026quot; \u0026quot;2\u0026quot;,\u0026quot;94\u0026quot;,\u0026quot;Avengers: Endgame (2019)\u0026quot;,\u0026quot;531\u0026quot; The tail command works in the same way - but looks at the end of a file. This is more useful when you are looking content which changes over time, like log files. In this case you probably want to see only the most recent entries.\nHere's how we can see the ten most recent commands we entered in our shell:\n$ tail $HISTFILE : 1606818280:0;ls : 1606818300:0;ln -s $(pwd) ~/effective-shell : 1606818308:0;cat ~/effective-shell/data/top100.csv : 1606818342:0;head -n 3 ~/effective-shell/data/top100.csv : 1606819062:0;head ~/effective-shell/data/top100.csv : 1606819647:0;gcd : 1606819649:0;git stash : 1606819650:0;gcd : 1606819662:0;git stash pop : 1606819803:0;tail $HISTFILE What is $HISTFILE?\nMost Bash-like shells keep a file called the history file. This is essentially a record of all of the commands which have been written in the shell. The history command can be used to show the contents of this file. But if we want to work with the file directly, we can find its location with the special variable called $HISTFILE.\nEnter help history for more information on the shell history.\n We can be more specific, just like with head, by specifying the number of lines to show:\n$ tail -n 3 $HISTFILE : 1606819650:0;gcd : 1606819662:0;git stash pop : 1606819803:0;tail $HISTFILE tail can also be used to show the changes to a file in real time. Add the -f flag to follow the contents of the file - this means the tail command show each new line as it gets added to the file.\nTo try it out, run the following command in one shell:\n$ tail -f $HISTFILE In another terminal window, start entering commands. You'll see that the tail command in the first window is writing the updates to the terminal as they are entered in the file. Press Ctrl+C to close the tail program.\nAnother trick I use a lot with tail is to use -n +2. This shows everything from the second line - the + symbol indicates we show everything from the given line onwards. This makes it easy to strip the header, or first line, from content. Here's how you might use it:\n$ head ~/effective-shell/data/top100.csv | tail -n +2 \u0026quot;1\u0026quot;,\u0026quot;97\u0026quot;,\u0026quot;Black Panther (2018)\u0026quot;,\u0026quot;515\u0026quot; \u0026quot;2\u0026quot;,\u0026quot;94\u0026quot;,\u0026quot;Avengers: Endgame (2019)\u0026quot;,\u0026quot;531\u0026quot; \u0026quot;3\u0026quot;,\u0026quot;93\u0026quot;,\u0026quot;Us (2019)\u0026quot;,\u0026quot;536\u0026quot; \u0026quot;4\u0026quot;,\u0026quot;97\u0026quot;,\u0026quot;Toy Story 4 (2019)\u0026quot;,\u0026quot;445\u0026quot; \u0026quot;5\u0026quot;,\u0026quot;99\u0026quot;,\u0026quot;Lady Bird (2017)\u0026quot;,\u0026quot;393\u0026quot; \u0026quot;6\u0026quot;,\u0026quot;100\u0026quot;,\u0026quot;Citizen Kane (1941)\u0026quot;,\u0026quot;94\u0026quot; \u0026quot;7\u0026quot;,\u0026quot;97\u0026quot;,\u0026quot;Mission: Impossible - Fallout (2018)\u0026quot;,\u0026quot;430\u0026quot; \u0026quot;8\u0026quot;,\u0026quot;98\u0026quot;,\u0026quot;The Wizard of Oz (1939)\u0026quot;,\u0026quot;120\u0026quot; \u0026quot;9\u0026quot;,\u0026quot;96\u0026quot;,\u0026quot;The Irishman (2019)\u0026quot;,\u0026quot;441\u0026quot; Here I've taken the head of the file (otherwise the output gets quite difficult to follow), then piped the results into tail -n +2 to grab everything from the second line onwards - which removes the heading line. We see the films only, not the titles of each column.\nWe're going to use head and tail quite a lot when working with text. These are two crucial tools which can really speed up your work.\nReplacing Text The next tool we'll look at is tr (translate characters). This program is very simple. My most common use for tr is to perform a simple substitution of characters.\nLet's create a list of each of the columns in the data file we saw before to show how the command works:\n$ head -n 1 ~/effective-shell/data/top100.csv | tr ',' '\\n' \u0026quot;Rank\u0026quot; \u0026quot;Rating\u0026quot; \u0026quot;Title\u0026quot; \u0026quot;Reviews\u0026quot; What about if we wanted to remove the quotes?\n$ head -n 1 ~/effective-shell/data/top100.csv | tr ',' '\\n' | tr -d '\u0026quot;' Rank Rating Title Reviews Here we've seen two variations on how we can run the command. The first form is used to replace characters. Running:\ntr ',' '\\n' Replaces the first specified character with the second. The \\n character is the special newline character, which is used to create a line break at the end of a file.\nThe second form uses the -d flag to specify a set of characters to delete:\ntr -d '\u0026quot;' In the form above we delete quote (\u0026quot;) characters.\nWhen using tr remember that it works on characters. For example, the following might not work as you expect:\n$ echo \u0026quot;Welcome to the shell\u0026quot; | tr 'shell' 'machine' Wcicomc to tac macii The reason the output is like this is that we're specifying character replacements - so we're changing characters as shown below:\ns -\u0026gt; m h -\u0026gt; a e -\u0026gt; c l -\u0026gt; h l -\u0026gt; i There are plenty of ways to replace entire words or perform more complex operations, but we'll use sed or awk for these operations - which we'll see in the following chapter.\nThere is one final thing it is worth mentioning about tr. It can be provided with character classes. This is easiest to explain with an example:\n$ echo \u0026quot;Use your inside voice...\u0026quot; | tr '[[:lower:]]' '[[:upper:]]' USE YOUR INSIDE VOICE... In this case we are transforming characters in the lower class (lowercase characters) to the upper class (uppercase characters).\nOn Linux systems you can find more about character classes with man 7 regex. I am not going to go deeper into character classes at this stage. They provide a simple way to specify things like digits, alphabetic characters and so on, but there are other ways to do this (with extended regexes) which I think are likely to be more useful to learn about instead.\nHow to Cut The next command is one which I've used far more than I expected. The cut command splits a line of text, using a given delimiter. Let's see some examples:\n$ cut -d',' -f 3 ~/effective-shell/data/top100.csv | head \u0026quot;Title\u0026quot; \u0026quot;Black Panther (2018)\u0026quot; \u0026quot;Avengers: Endgame (2019)\u0026quot; \u0026quot;Us (2019)\u0026quot; \u0026quot;Toy Story 4 (2019)\u0026quot; \u0026quot;Lady Bird (2017)\u0026quot; \u0026quot;Citizen Kane (1941)\u0026quot; \u0026quot;Mission: Impossible - Fallout (2018)\u0026quot; \u0026quot;The Wizard of Oz (1939)\u0026quot; \u0026quot;The Irishman (2019)\u0026quot; This is the first way to use cut. We specify the -d flag to choose a delimiter which we will cut the text with, then -f to choose which field we want to see. In this case we show split on the command character and show the third field - the title of the film in the data file.\nThis can be extraordinarily useful. Let's see how to get the names of the Kubernetes pods I have running on a cluster. I can use the following command to get the pods:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE elastic-operator-0 1/1 Running 0 35d elk-apm-server-65b698fb8c-rzncz 1/1 Running 0 13d elk-es-default-0 1/1 Running 0 35d elk-kb-6f8bb6457b-bbbnn 1/1 Running 0 35d filebeat-beat-filebeat-ccgl7 1/1 Running 1 13d filebeat-beat-filebeat-dvf2l 1/1 Running 2 13d filebeat-beat-filebeat-mnpms 1/1 Running 329 13d kube-state-metrics-5cb57bdc45-mqv9d 1/1 Running 0 35d metricbeat-beat-metricbeat-2xm7t 1/1 Running 6103 35d metricbeat-beat-metricbeat-96dkt 1/1 Running 6097 35d metricbeat-beat-metricbeat-n7kxm 1/1 Running 6109 35d Now to get the name I can just cut the lines on the \u0026lsquo;space\u0026rsquo; character and grab the first field:\n$ kubectl get pods | cut -d' ' -f 1 NAME elastic-operator-0 elk-apm-server-65b698fb8c-rzncz elk-es-default-0 elk-kb-6f8bb6457b-bbbnn filebeat-beat-filebeat-ccgl7 filebeat-beat-filebeat-dvf2l filebeat-beat-filebeat-mnpms kube-state-metrics-5cb57bdc45-mqv9d metricbeat-beat-metricbeat-2xm7t metricbeat-beat-metricbeat-96dkt metricbeat-beat-metricbeat-n7kxm And if we want to strip the first line? We can use the tail -n +2 command to tail everything from the second line onwards:\n$ kubectl get pods | cut -d' ' -f 1 | tail -n +2 elastic-operator-0 elk-apm-server-65b698fb8c-rzncz elk-es-default-0 elk-kb-6f8bb6457b-bbbnn filebeat-beat-filebeat-ccgl7 filebeat-beat-filebeat-dvf2l filebeat-beat-filebeat-mnpms kube-state-metrics-5cb57bdc45-mqv9d metricbeat-beat-metricbeat-2xm7t metricbeat-beat-metricbeat-96dkt metricbeat-beat-metricbeat-n7kxm Bingo - we've removed the heading line. If you remember grep from the previous chapter, you might have spotted that we could also just filter the content:\n$ kubectl get pods | cut -d' ' -f 1 | grep -v NAME elastic-operator-0 elk-apm-server-65b698fb8c-rzncz elk-es-default-0 elk-kb-6f8bb6457b-bbbnn filebeat-beat-filebeat-ccgl7 filebeat-beat-filebeat-dvf2l filebeat-beat-filebeat-mnpms kube-state-metrics-5cb57bdc45-mqv9d metricbeat-beat-metricbeat-2xm7t metricbeat-beat-metricbeat-96dkt metricbeat-beat-metricbeat-n7kxm With even just a few simple shell commands there are often many ways to accomplish the same goal!\nThere is another way we can cut text. We can cut by slicing a number of characters from each line.\nLet's take a look at our web logs file:\n$ tail ~/effective-shell/logs/web-server-logs.txt 2020-11-29T12:50:52.721Z: info - Request: GET /en.search.min.1f83b222e24a227c0f5763727cb9e4f3b435f08b936f6ce529c9c9359f6b61a8.js 2020-11-29T12:50:52.722Z: info - Serving file '../../../website/public/en.search.min.1f83b222e24a227c0f5763727cb9e4f3b435f08b936f6ce529c9c9359f6b61a8.js'... 2020-11-29T12:50:52.762Z: info - Request: GET /svg/menu.svg 2020-11-29T12:50:52.763Z: info - Serving file '../../../website/public/svg/menu.svg'... 2020-11-29T12:50:52.763Z: info - Request: GET /svg/calendar.svg 2020-11-29T12:50:52.764Z: info - Serving file '../../../website/public/svg/calendar.svg'... 2020-11-29T12:50:52.765Z: info - Request: GET /svg/edit.svg 2020-11-29T12:50:52.766Z: info - Serving file '../../../website/public/svg/edit.svg'... 2020-11-29T12:50:52.784Z: info - Request: GET /fonts/roboto-v19-latin-300italic.woff2 2020-11-29T12:50:52.785Z: info - Serving file '../../../website/public/fonts/roboto-v19-latin-300italic.woff2'... We can use the -c (characters) flag to specify the characters in the line we want to see. Let's extract the timestamp only:\n$ tail -n 3 ~/effective-shell/logs/web-server-logs.txt | cut -c 12-19 12:50:52 12:50:52 12:50:52 We can also use the character option to extract everything from a specific point onwards:\n$ tail -n 3 ~/effective-shell/logs/web-server-logs.txt | cut -c 27- info - Serving file '../../../website/public/svg/edit.svg'... info - Request: GET /fonts/roboto-v19-latin-300italic.woff2 info - Serving file '../../../website/public/fonts/roboto-v19-latin-300italic.woff2'... By cutting from the 27th character onwards (-c 27-) we remove the timestamp and just get the log message.\nAs a nice trick you can use the same syntax when splitting by fields:\n$ tail -n 3 ~/effective-shell/data/top100.csv | cut -d',' -f 3- \u0026quot;Pinocchio (1940)\u0026quot;,\u0026quot;55\u0026quot; \u0026quot;Chinatown (1974)\u0026quot;,\u0026quot;75\u0026quot; \u0026quot;The Dark Knight (2008)\u0026quot;,\u0026quot;342\u0026quot; This is field three onwards. If we just want fields two and three, we use:\n$ tail -n 3 ~/effective-shell/data/top100.csv | cut -d',' -f 2,3 \u0026quot;100\u0026quot;,\u0026quot;Pinocchio (1940)\u0026quot; \u0026quot;99\u0026quot;,\u0026quot;Chinatown (1974)\u0026quot; \u0026quot;94\u0026quot;,\u0026quot;The Dark Knight (2008)\u0026quot; There's a surprising amount you can do with the cut tool. As we introduce more complex tools later on, like sed and awk, we'll see other ways to accomplish the same goals, but I often find that by filtering down the content with grep first I can cut my way to what I need without having to use more complex tools.\nA Trick with Rev There is a very simple command called rev which reverses the given input. For example:\n$ echo \u0026quot;A nut for a jar of tuna\u0026quot; | rev anut fo raj a rof tun A At first glance this doesn't seem very useful - but there's a nice trick we can do with this:\n$ pwd | rev | cut -d\\ -f 1 | rev effective-shell-playground Here we take the current working directory, reverse it, cut the first field, then reverse it again. Here's what's happening at each stage:\npwd /Users/dwmkerr/effective-shell rev llehs-evitceffe/rrekmwd/sresU/ cut -d'/' -f 1 llehs-evitceffe rev effective-shell This is a neat trick to rip all of the text from the final occurrence of a character. You might not use it very often but it's an interesting reminder that you can often do more than you think by chaining together simple commands into a pipeline!\nSort and Unique Two other commands which can be really helpful are sort and uniq. Let's see sort first:\n$ cut -d',' -f 3 ~/effective-shell/data/top100.csv | sort | head \u0026quot;12 Years a Slave (2013)\u0026quot; \u0026quot;A Hard Day's Night (1964)\u0026quot; \u0026quot;A Night at the Opera (1935)\u0026quot; \u0026quot;A Quiet Place (2018)\u0026quot; \u0026quot;A Star Is Born (2018)\u0026quot; \u0026quot;Alien (1979)\u0026quot; \u0026quot;All About Eve (1950)\u0026quot; \u0026quot;Argo (2012)\u0026quot; \u0026quot;Arrival (2016)\u0026quot; \u0026quot;Avengers: Endgame (2019)\u0026quot; Here we've grabbed the third field in our data file (the name of the film), sorted, then shown the first ten values.\nYou can reverse the direction of sort with the -r flag:\n$ cut -d',' -f 3 ~/effective-shell/data/top100.csv | sort -r | head \u0026quot;Zootopia (2016)\u0026quot; \u0026quot;Wonder Woman (2017)\u0026quot; \u0026quot;Won't You Be My Neighbor? (2018)\u0026quot; \u0026quot;Widows (2018)\u0026quot; \u0026quot;War for the Planet of the Apes (2017)\u0026quot; \u0026quot;Us (2019)\u0026quot; \u0026quot;Up (2009)\u0026quot; \u0026quot;Toy Story 4 (2019)\u0026quot; \u0026quot;Toy Story 3 (2010)\u0026quot; \u0026quot;Toy Story 2 (1999)\u0026quot; There are actually quite a few other options for sort, you can see them with man sort. However, most of them perform functionality which you can get from other tools (such as making the lines unique, which we can do with uniq). You might find some of them useful so don't be shy to explore some of the other options.\nThe uniq command removes duplicate lines from a stream of text. Note that this only removes duplicate lines when they are next to each other. This means that you will often have to sort first.\nHere's an example of where I might use uniq - getting all unique error messages in a log file:\n$ cut -c 27- ~/effective-shell/logs/web-server-logs.txt | grep error | sort | uniq error - Unhandled error EACCES trying to read '../../../website/public/docs/part-1-transitioning-to-the-shell/5-getting-help/index.html', returning a 500 error - Unhandled error EACCES trying to read '../../../website/public/svg/calendar.svg', returning a 500 error - Unhandled error EACCES trying to read '../../../website/public/svg/edit.svg', returning a 500 info - Request: GET /docs/1-getting-started/images/ls-applications-windows-error.png info - Request: GET /docs/part-1-transitioning-to-the-shell/3-managing-your-files/images/rm-error-directory.png info - Serving file '../../../website/public/docs/1-getting-started/images/ls-applications-windows-error.png'... info - Serving file '../../../website/public/docs/part-1-transitioning-to-the-shell/3-managing-your-files/images/rm-error-directory.png'... Let's break this down:\n cut -c 27- ~/effective-shell/logs/web-server-logs.txt - extract log messages from a log file, skipping the timestamp grep error - filter down to lines which contain the text error sort - sort the output uniq - show only unique values  This is a powerful technique - if we had thousands of errors in the file, this would make sure we only see distinct errors, rather than showing every error.\nDon't Forget Your Pager! In Chapter 5 - Getting Help we talked about the pager - the program your shell uses to make it easier to look through larger text files, giving the option to move backwards and forwards a page at a time (or searching and so on). Don't forget to use your pager when you are working with text. When you are trying to build a pipeline and want to see intermediate results (perhaps before you use head or tail) then you can use the pager to avoid filling your screen and terminal with too much text.\nFor example, when looking at the sorted list of films, I might run this:\n $ cut -d',' -f 3 ~/effective-shell/data/top100.csv | sort | less \"Jaws (1975)\" \"King Kong (1933)\" \"La Grande illusion (Grand Illusion) (1938)\" \"La La Land (2016)\" \"Lady Bird (2017)\" \"Laura (1944)\" /Jaws  I've made the output smaller so that it is easier to see what is happening. In this example I've cut out the film name from my data file, sorted it, then piped the result into less so that I can page through the data and ensure it is correct - I've also searched for the text Jaws to see where it is in the file.\nSummary In this chapter we introduced a number of basic tools which let us work with text.\n head will show the first ten lines of a file. head -n 30 will show the first thirty lines of a file, using the -n flag to specify the number of lines. tail will show the final ten lines of a file. tail -n 3 uses the -n flag to specify three lines only. The $HISTFILE environment variable holds the path to the shell command history file. tail -f $HISTFILE uses the -f flag to follow the file, printing output as it is written to the file. tr 'a' 'b' is the translate text command, which turns one set of characters into another tr -d '!' shows how the -d or delete flag can specify characters to delete. The cut command can be used to extract parts of a line of text. cut -d',' -f 3 shows how the -d or delimiter flag is used to specify the delimiter to cut on and how the -f or field flag specifies which of the fields the text has been cut into is printed. cut -c 2-4 uses the -c or characters flag to specify that we are extracting a subset of characters in the line, in this case characters two to four. cut -c 10- cuts from character ten to the end of the line The cut command also allows for multiple fields to be specified when cutting by field, such as -f 2,3 for the second and third field, or -f 4- for fields four onwards. rev reverses text - by reversing, cutting and then re-reversing you can quickly extract text from the end of a line. sort sorts the incoming text alphabetically. The -r flag for sort reverses the sort order. The uniq command removes duplicate lines - but only when they are next to each other, so you'll often use it in combination with sort. Your pager, for example the less program can be useful when inspecting the output of your text transformation commands.  "});index.add({'id':16,'href':'/docs/part-3-manipulating-text/advanced-text-manipulation/','title':"Advanced Text Manipulation",'content':"Chapter 16 - Advanced Text Manipulation with Sed In Chapter 15 we introduced some simple commands to work with text - specifically head, tail, tr and cut. Now we are going to introduce sed the Stream Editor command.\nsed can be used to perform a variety of tasks with text. In many cases a small command involving sed can quickly solve problems. We'll look at some of the common ways to use sed, some more sophisticated examples and discuss when you might want to consider using alternative tools like awk or a programming language.\nIntroducing the Stream Editor The stream editor command takes input from a stream - which in many cases will simply be a file. It then performs operations on the text as it is read, and writes the output to stdout. This command is extremely powerful - there are many sophisticated transformations which can be applied.\nSeeing sed commands can be a little intimidating - they can look complex! But we'll start with the basics and hopefully you'll see just how effective this tool can make you!\nTransformations with Sed Rather than dissecting each and every option or flag and every nuance of the program, I'm going to try and show some real world examples of how you can use sed. This will allow us to see the functionality in easier to digest chunks. It also keeps things practical! Let's dive in and look at some common tasks and how we can solve them with sed.\nRunning the Samples\nEach of these samples assumes you are in the effective-shell samples folder. If you don't have this folder, just run:\nmkdir -p ~/effective-shell curl -s -L https://effective-shell.com/downloads/effective-shell-playground.tar.gz | tar -xzf - -C ~/effective-shell cd ~/effective-shell This command will download and extract the samples folder in your home directory an then set it as your working directory.\n Replacing Text The samples folder has a script which copies some configuration files to a backup folder. Let's take a quick look at it:\n$ cd scripts $ cat backup-config.sh #!/usr/bin/env bash # Make sure we have a backup directory. mkdir ~/backup # Copy over alicloud, aws, azure, gcp and ssh config and credentials. cp ~/.aliyun/config.json ~/backup/settings/aliyun/ cp ~/.aws/config ~/backup/settings/aws/ cp ~/.aws/credentials ~/backup/settings/aws/ cp ~/.azure/config ~/backup/settings/azure/ cp ~/.config/gcloud/credentials.db ~/backup/settings/gcloud/ cp ~/.ssh/config ~/backup/settings/ssh/ cp ~/.ssh/id_rsa ~/backup/settings/ssh/ # is this safe? cp ~/.ssh/id_rsa.pub ~/backup/settings/ssh/ We could use the sed command to change the name of the folder we backup to. If we wanted to change the settings folder to configuration we could run the command below:\n$ sed 's/settings/configuration/' backup-config.sh #!/usr/bin/env bash # Copy over alicloud, aws, azure, gcp and ssh config and credentials. cp ~/.aliyun/config.json ~/backup/configuration/aliyun/ cp ~/.aws/config ~/backup/configuration/aws/ cp ~/.aws/credentials ~/backup/configuration/aws/ cp ~/.azure/config ~/backup/configuration/azure/ cp ~/.config/gcloud/credentials.db ~/backup/configuration/gcloud/ cp ~/.ssh/config ~/backup/configuration/ssh/ cp ~/.ssh/id_rsa ~/backup/configuration/ssh/ # is this safe? cp ~/.ssh/id_rsa.pub ~/backup/configuration/ssh/ This shows the basics of how sed works. We give sed an expression, which describes a set of operations we want to perform and it then applies that expression to the file we specify. As with most commands we've seen it can apply the expression to stdin.\nLet's look at the expression in detail:\ns/settings/dotfiles/  The s indicates that we are going to run the substitute function, which is used to replace text The / indicates the start of the pattern we are searching for\u0026hellip; \u0026hellip;settings is the pattern - which is just the literal text \u0026lsquo;settings\u0026rsquo; The second / indicates the start of the replacement we will make when the pattern is found The final / indicates the end of the replacement - we can also optionally put flags after this slash  Just as we saw in Chapter 13 - Get to Grips with Grep, we can provide a regular expression as the pattern to search for. By default, sed will use basic regular expressions. We can use extended regular expressions by providing the -E flag.\nBasic and Extended Regular Expressions\nIf you are not sure about the difference between Basic and Extended Regular Expressions, you can use the:\nman re_pattern Command to get detailed documentation.\n Applying Multiple Expressions We can use the -e \u0026lt;expression\u0026gt; flag to supply multiple expressions to sed.\nIf we wanted to delete our backup folders, we could run two substitutions - one which changes the cp command to rmdir command and one which deletes the first parameter, which was the source directory for cp but is not needed for rmdir1.\nReplacing Text on Specific Lines Let's start with the first expression, which should change cp to rmdir:\n$ sed -e 's/cp/rmdir/' backup-config.sh #!/usr/bin/env bash # Make sure we have a backup directory. mkdir ~/backup # Copy over alicloud, aws, azure, grmdir and ssh config and credentials. rmdir ~/.aliyun/config.json ~/backup/settings/aliyun/ rmdir ~/.aws/config ~/backup/settings/aws/ rmdir ~/.aws/credentials ~/backup/settings/aws/ rmdir ~/.azure/config ~/backup/settings/azure/ rmdir ~/.config/gcloud/credentials.db ~/backup/settings/gcloud/ rmdir ~/.ssh/config ~/backup/settings/ssh/ rmdir ~/.ssh/id_rsa ~/backup/settings/ssh/ # is this safe? rmdir ~/.ssh/id_rsa.pub ~/backup/settings/ssh/ We have provided a single expression with the -e parameter. If you are providing a single expression only then this parameter is superfluous, but we will shortly add another expression.\nThis has changed all of the cp commands to rmdir. But did you notice the bug? It has also changed the letters cp in the comment which mentions gcp to rmdir, meaning our comment line has changed. I've made the changes bold to make this easier to see below:Before, it was:\n # Copy over alicloud, aws, azure, gcp and ssh config and credentials.  Now it is:\n # Copy over alicloud, aws, azure, grmdir and ssh config and credentials.  This isn't the worst bug in the world, but we can do better. Let's change the expression to only run on lines which start with the letters cp:\n$ sed -e '/^cp/s/cp/rmdir/' backup-config.sh #!/usr/bin/env bash # Make sure we have a backup directory. mkdir ~/backup # Copy over alicloud, aws, azure, gcp and ssh config and credentials. rmdir ~/.aliyun/config.json ~/backup/settings/aliyun/ rmdir ~/.aws/config ~/backup/settings/aws/ rmdir ~/.aws/credentials ~/backup/settings/aws/ rmdir ~/.azure/config ~/backup/settings/azure/ rmdir ~/.config/gcloud/credentials.db ~/backup/settings/gcloud/ rmdir ~/.ssh/config ~/backup/settings/ssh/ rmdir ~/.ssh/id_rsa ~/backup/settings/ssh/ # is this safe? rmdir ~/.ssh/id_rsa.pub ~/backup/settings/ssh/ Let's look at the expression in detail. Before the s symbol, which indicates that we are performing a substitution, we now include a line pattern. A line pattern tells sed which lines it should apply the expression to. Our line pattern is just ^cp, which is the regular expression meaning \u0026ldquo;any line which starts with cp\u0026rdquo;.\nLine patterns can be quite sophisticated and are covered in the box later on in this chapter!\nSo all in all:\n ^cp- use the line pattern ^cp (lines which start with cp) for the expression s - use the substitution function (which replaces text) cp - find the pattern cp rmdir - replace each occurrence with rmdir on lines which start with the text cp, replace any occurrences of the text text cp with the text rmdir  We combine these parts together using a / symbol, this is needed so that sed knows where the line pattern starts and end, the function starts and end and so on2.\nRemoving Parts of a Line Now we need to remove the first parameter for the rmdir command, it was used when we had cp as the command but doesn't make sense any more.\nLet's apply a new expression:\n$ sed -E -e '/^cp/s/cp/rmdir/' -e '/^rmdir/s/~[^ ]+ //' backup-config.sh #!/usr/bin/env bash # Make sure we have a backup directory. mkdir ~/backup # Copy over alicloud, aws, azure, gcp and ssh config and credentials. rmdir ~/backup/settings/aliyun/ rmdir ~/backup/settings/aws/ rmdir ~/backup/settings/aws/ rmdir ~/backup/settings/azure/ rmdir ~/backup/settings/gcloud/ rmdir ~/backup/settings/ssh/ rmdir ~/backup/settings/ssh/ # is this safe? rmdir ~/backup/settings/ssh/ Let's take a look at the second expression, component by component:\n ^rmdir - use a line pattern which matches lines which start with rmdir s - use the substitution function to replace text ~[^ ]+  - search for the tilde ~ symbol and any non-space characters, up until the first space character  We actually don't even include a replacement - which is why the expression ends with // - the inside of the replacement part of the expression is empty. This means we search for a tilde ~, match everything up until the next space   and then replace it with nothing - meaning we remove it!\nNote as well that we've used the -E flag to tell sed to use extended regular expressions. This allows us to use the + symbol without escaping it with a \\ first. In general I would recommend always using extended regular expressions as they are more commonly used and if you work with programming languages as well as the shell, those languages will likely use something closer to extended regular expressions rather than basic ones.\nLet's look at a few other ways to use sed for some real-world tasks.\nStripping Comments In the script we've used above, there are some comments. Let's use sed to remove them:\n$ sed -E 's/#.*$//' backup-config.sh mkdir ~/backup cp ~/.aliyun/config.json ~/backup/settings/aliyun/ cp ~/.aws/config ~/backup/settings/aws/ cp ~/.aws/credentials ~/backup/settings/aws/ cp ~/.azure/config ~/backup/settings/azure/ cp ~/.config/gcloud/credentials.db ~/backup/settings/gcloud/ cp ~/.ssh/config ~/backup/settings/ssh/ cp ~/.ssh/id_rsa ~/backup/settings/ssh/ cp ~/.ssh/id_rsa.pub ~/backup/settings/ssh/ Notice how we have removed the comments which started at the beginning of the file, as well as the comment after the cp ~/.ssh/id_rsa line. This is because the regular expression matches any hash symbol # and strips everything which follows it.\nWhat about if we want to get rid of those empty lines? We can use the d or delete function:\n$ sed -E -e 's/#.*$//' -e '/^ *$/d' backup-config.sh mkdir ~/backup cp ~/.aliyun/config.json ~/backup/settings/aliyun/ cp ~/.aws/config ~/backup/settings/aws/ cp ~/.aws/credentials ~/backup/settings/aws/ cp ~/.azure/config ~/backup/settings/azure/ cp ~/.config/gcloud/credentials.db ~/backup/settings/gcloud/ cp ~/.ssh/config ~/backup/settings/ssh/ cp ~/.ssh/id_rsa ~/backup/settings/ssh/ cp ~/.ssh/id_rsa.pub ~/backup/settings/ssh/ The delete command is applied to all lines which match the line pattern. In this case, the line pattern is a regular expression, ^ *$, which matches any line made up only of space characters (including zero space characters, i.e. empty lines).\nLine Patterns\nLine patterns, which can be used on many sed functions, are actually quite sophisticated. Here are a few examples:\n /test/ - any line matching the pattern test /test/! - any line not matching test 6 - line six $ - the last line 1-10 - lines one to ten 1-10! - lines _except one to ten  Check man sed to see more about line patterns.\n Appending Text In a regular expression the ampersand $ symbol represents the end of a line.\nWe can use this symbol to add content to the end of lines - we just search for $ and replace it with whatever we want to end the line with! And if we want to only do this on certain lines, we can use a line pattern to limit where we apply the expression.\nHere's how we can make sure that the cp command in the script doesn't fail:\n$ sed -E -e '/^cp/s/$/ || true/' backup-config.sh #!/usr/bin/env bash # Make sure we have a backup directory. mkdir ~/backup # Copy over alicloud, aws, azure, gcp and ssh config and credentials. cp ~/.aliyun/config.json ~/backup/settings/aliyun/ || true cp ~/.aws/config ~/backup/settings/aws/ || true cp ~/.aws/credentials ~/backup/settings/aws/ || true cp ~/.azure/config ~/backup/settings/azure/ || true cp ~/.config/gcloud/credentials.db ~/backup/settings/gcloud/ || true cp ~/.ssh/config ~/backup/settings/ssh/ || true cp ~/.ssh/id_rsa ~/backup/settings/ssh/ # is this safe? || true cp ~/.ssh/id_rsa.pub ~/backup/settings/ssh/ || true Now we have a command which strips comments, deletes empty lines and then adds the text  || true to the end of the line. This little trick ensures that the script won't fail if one of the individual cp commands fails. We'll see more about shell scripts later.\nWhat if we want to add a semicolon to the end of all lines?\nsed s/$/;/ Easy!\nPrepending Text In a regular expression the caret ^ symbol represents the start of a line.\nWe can apply the same trick as with the ampersand $ symbol to add text to the start of a line - we just replace ^ with whatever we want the line to start with.\nHere's how we can use this trick!\n$ sed -E -e '/^cp/s/$/\u0026quot;/' -e '/\u0026quot;$/s/^/echo \u0026quot;/' backup-config.sh #!/usr/bin/env bash # Make sure we have a backup directory. mkdir ~/backup # Copy over alicloud, aws, azure, gcp and ssh config and credentials. echo \u0026quot;cp ~/.aliyun/config.json ~/backup/settings/aliyun/\u0026quot; echo \u0026quot;cp ~/.aws/config ~/backup/settings/aws/\u0026quot; echo \u0026quot;cp ~/.aws/credentials ~/backup/settings/aws/\u0026quot; echo \u0026quot;cp ~/.azure/config ~/backup/settings/azure/\u0026quot; echo \u0026quot;cp ~/.config/gcloud/credentials.db ~/backup/settings/gcloud/\u0026quot; echo \u0026quot;cp ~/.ssh/config ~/backup/settings/ssh/\u0026quot; echo \u0026quot;cp ~/.ssh/id_rsa ~/backup/settings/ssh/ # is this safe?\u0026quot; echo \u0026quot;cp ~/.ssh/id_rsa.pub ~/backup/settings/ssh/\u0026quot; We first put at quote at the end of each line which starts with cp, then put echo \u0026quot; at the beginning of each line which ends with a quote!\nThis has now tweaked our script so that it doesn't actually copy the files, it just prints out the commands to the screen. It also demonstrates how the order of the expressions is important, the second expression is applied after the first. If you are using multiple expressions be careful that an earlier expression doesn't alter the line in a way which breaks the next expression!\nExtracting Information What about if we want to extract some information from lines in a file?\nLet's take a quick look at our movies file in our data folder for reference:\n$ cd ~/effective-shell/data $ head -n 3 top100.csv \u0026quot;Rank\u0026quot;,\u0026quot;Rating\u0026quot;,\u0026quot;Title\u0026quot;,\u0026quot;Reviews\u0026quot; \u0026quot;1\u0026quot;,\u0026quot;97\u0026quot;,\u0026quot;Black Panther (2018)\u0026quot;,\u0026quot;515\u0026quot; \u0026quot;2\u0026quot;,\u0026quot;94\u0026quot;,\u0026quot;Avengers: Endgame (2019)\u0026quot;,\u0026quot;531\u0026quot; It should be easy to create a regular expression to find the year for each movie - it would just have to match all numeric values between brackets. Let's try it out!\n$ head -n 3 data/top100.csv | sed -E 's/.*\\(([0-9]+)\\).*/\\1/' \u0026quot;Rank\u0026quot;,\u0026quot;Rating\u0026quot;,\u0026quot;Title\u0026quot;,\u0026quot;Reviews\u0026quot; 2018 2019 This works because we match any text, then capture any digits enclosed in brackets, then replace with the \\1 text, which means \u0026lsquo;the first match\u0026rsquo;.\nGiven what we know about line patterns, we can also easily exclude the first line:\n$ sed -E -e '1d' -e 's/.*\\(([0-9]+)\\).*/\\1/' data/top100.csv 2018 2019 ... Here we have just added the 1d expression - delete the first line.\nAdvanced - Surrounding Parts of Text with Quotes Let's look at another example. Here we have some yaml content, a set of keys and values. Note that some of the values are quoted, and some are not:\ntitle: \u0026quot;Advanced Text Manipulation\u0026quot; slug: advanced-text-manipulaton weight: 14 In fact this is the text content at the top of the file I am working on now. But this file is not a YAML file, it is a Markdown file (which is fairly close to plain text), so also has a lot of other content in it.\nFirst, let's see if we can extract the keys:\n $ grep -E '[^:]+:' ~/effective-shell/docs/chapter12.md title: \"Advanced Text Manipulation\" slug: advanced-text-manipulaton weight: 14 Let's say we have a script which is used to backup some local files to an Amazon S3 bucket. We can see a script like this here: We could use the `sed` command to change the S3 bucket. For example, if we wanted to change the `settings` text to `dotfiles` we could run the command below: Let's look at the expression in detail: ...snip...  Well this kind of worked. The first three matches were correct, but we then found lots of other text. It looks like our pattern is not correct.\nThe pattern is [^:]+:, which means \u0026lsquo;at least one character which is not the : character, followed by :.\nWe can improve on it by telling it to not include spaces before the colon, and be explicit that this pattern we are searching for must be at the start of the line:\n % grep -E '^[^: ]+:' ~/effective-shell/docs/chapter12.md title: \"Advanced Text Manipulation\" slug: advanced-text-manipulaton weight: 14 \"2\",\"94\",\"Avengers: Endgame (2019)\",\"531\"  Much better. The pattern now starts with ^ meaning \u0026lsquo;start of line\u0026rsquo;, and the type of characters we search for [^: ] is not \u0026lsquo;anything which is not a colon or space. But we've also found a film title from the text - we can improve our pattern further by eliminating quotes, which are not valid for YAML keys anyway:\n $ grep -E '^[^: \"]+:' ~/effective-shell/docs/chapter12.md title: \"Advanced Text Manipulation\" slug: advanced-text-manipulaton weight: 14  Awesome!\nBuilding Regular Expressions\nI was a hold out for years, but regular expressions are incredibly useful if you take the time to learn them. Exercises like this are a great way to do it. Start simple, add the elements you need bit by bit. It's a great way to learn exactly what each element does.\nAvoid just searching online for the perfect expression - they'll often be very long (because they are bullet-proof and cover every possible edge case hopefully). If you are building an expression which is critical to get right, then do search online for help if you need it, but for day to day tasks, practicing like this will really help.\n Now let's start using this pattern in sed. Let's print all lines which match the pattern:\n$ sed -E -n '/^[^: \u0026quot;]+:/p' ~/effective-shell/docs/chapter12.md title: \u0026quot;Advanced Text Manipulation\u0026quot; slug: advanced-text-manipulaton weight: 14 Now there are two critical things we've added to sed here while we're working. The first is the -n flag, which means no automatic printing - meaning it will show no output unless told. Then in the pattern, we finish the command with p, meaning \u0026lsquo;print lines which match the pattern\u0026rsquo;.\nThese options make sed behave like grep, which is useful because we are still building the command.\nNow let's actually quote the result. To do that, we need to find lines where the value is not already quoted - so let's add that to our pattern:\n$ sed -E -n '/^[^: \u0026quot;]+: +[^\u0026quot;]+$/p' ~/effective-shell/docs/chapter12.md slug: advanced-text-manipulaton weight: 14 Our pattern now reads like this:\n ^[^ :\u0026quot;]+: - match the start of a line, any characters which are not space, colon or quote, which then finish with a colon and a space.  +[^\u0026quot;]+$ - match at least one space, then any set of characters which don't have a quote in them all the way to the end of the line.  The pattern is working - its found our two unquoted keys. Now let's get it to print the substitution.\nFirst, we're going to surround the key part and value part in brackets - this will make them \u0026lsquo;capture groups\u0026rsquo; - chunks of text we can use in the substitution. Here's how capture groups work:\n$ sed -E -n 's/(^[^: \u0026quot;]+:)( +[^\u0026quot;]+$)/Key is \u0026quot;\\1\u0026quot;/p' ~/effective-shell/docs/chapter12.md Key is \u0026quot;slug:\u0026quot; Key is \u0026quot;weight:\u0026quot; We are now not just searching for a pattern, we're using the s (substitute) function to replace all of the matched text with Key is \u0026quot;\\1\u0026quot;. The \\1 just means \u0026lsquo;what you found in the first capture group\u0026rdquo;.\nWe could just as easily show the value:\n$ sed -E -n 's/(^[^: \u0026quot;]+:)( +[^\u0026quot;]+$)/Value is \u0026quot;\\2\u0026quot;/p' ~/effective-shell/docs/chapter12.md Value is \u0026quot; advanced-text-manipulaton\u0026quot; Value is \u0026quot; 14\u0026quot; Here we're printing the second capture group. Now you might have noticed that in the key, we're including the colon, and in the value, we're including the space (or spaces). This isn't strictly right - they're the separators. So let's capture them separately:\n$ sed -E -n 's/(^[^: \u0026quot;]+)(: +)([^\u0026quot;]+$)/Key \u0026quot;\\1\u0026quot;, Value \u0026quot;\\3\u0026quot;, Separator \u0026quot;\\2\u0026quot;/p' ~/effective-shell/docs/chapter12.md Key \u0026quot;slug\u0026quot;, Value \u0026quot;advanced-text-manipulaton\u0026quot;, Separator \u0026quot;: \u0026quot; Key \u0026quot;weight\u0026quot;, Value \u0026quot;14\u0026quot;, Separator \u0026quot;: \u0026quot; I think this is really starting to show just how powerful sed and regular expressions are.\nLet's finally tie it together - add quotes around unquoted values:\n$ sed -E -n 's/(^[^: \u0026quot;]+)(: +)([^\u0026quot;]+$)/\\1\\2\u0026quot;\\3\u0026quot;/p' ~/effective-shell/docs/chapter12.md slug: \u0026quot;advanced-text-manipulaton\u0026quot; weight: \u0026quot;14\u0026quot; Awesome!\nIf we wanted to actually change the file, we could remove the -n flag, so that we write out everything. This makes the p option at the end of the substitution superfluous:\n$ sed -E 's/(^[^: \u0026quot;]+)(: +)([^\u0026quot;]+$)/\\1\\2\u0026quot;\\3\u0026quot;/' ~/effective-shell/docs/chapter12.md \u0026gt; updated.md Let's peep at the top of the file we created to see if it looks right:\n$ head -n 10 updated.md --- title: \u0026quot;Advanced Text Manipulation\u0026quot; slug: \u0026quot;advanced-text-manipulaton\u0026quot; weight: \u0026quot;15\u0026quot; --- # Chapter 15 - Advanced Text Manipulation In [Chapter 14](/docs/part-3-manipulating-text/slice-and-dice-text/) we introduced some simple commands to work with text - specifically `head`, `tail`, `tr` and `cut`. Now we are going to take a look at how we can perform more sophisticated tasks with text. Impressive - we've found a very specific pattern in a large file, substituted to match what we need and then saved the results.\nThis is just scratching the surface - but even with these basic tools, there is an incredible amount you can do. For example, what about if we didn't want to quote values which are just numbers? We'd just change the pattern from:\n$ sed -E -n '/(^[^: \u0026quot;]+:)( +[^\u0026quot;0-9]+$)/p' ~/effective-shell/docs/chapter12.md slug: advanced-text-manipulaton All we've done is changed the value pattern from [^\u0026quot;] (anything except quotes) to [^0-9] (anything except quotes and digits).\nAdvanced - Template Files Here's another example which I have found useful again and again. We can use sed's text replacement capabilities to create a basic templating system.\nFor example, let's say we have the file below:\napiVersion: v1 kind: Secret metadata: name: database-credentials namespace: dev stringData: username: admin password: ThisIsVerySensitive! This the definition of a \u0026lsquo;secret\u0026rsquo; for Kubernetes. It doesn't really matter how the file is structured because what we'll do in this example could work for any file.\nLet's say we want to be able to not have the username and password stored in the file itself, because they are sensitive. We also want to make the \u0026lsquo;namespace\u0026rsquo; value configurable.\nWe can do this by putting some easy to find patterns as placeholders in the file, then replacing them at runtime when we need them with sed.\nFirst, let's create the \u0026lsquo;template\u0026rsquo; version of this file:\n$ cat \u0026lt;\u0026lt; EOF \u0026gt; secret.template.yaml apiVersion: v1 kind: Secret metadata: name: database-credentials namespace: %NAMESPACE% stringData: username: %DB_USERNAME% password: %DB_PASSWORD% EOF The first line is using a \u0026lsquo;heredoc\u0026rsquo; to write multiple lines of text to a file. We see heredocs in detail in a later chapter. The file is also in the playground at templates/secret.template.yaml.\nNow let's apply our substitution:\n$ sed -e 's/%NAMESPACE%/staging/' \\ -e 's/%DB_USERNAME%/admin/' \\ -e 's/%DB_PASSWORD%/secret/' \\ ~/effective-shell/templates/secret.template.yaml apiVersion: v1 kind: Secret metadata: name: database-credentials namespace: staging stringData: username: admin password: secret I've used the \\ backslash character to split up the command into multiple lines. This command is really quite straightforward - we search for the very obvious to find patterns and replace them with values.\nWhat if we wanted this to be dynamic, and instead of using hard-coded values get the values from environment variables? This is very straightforward:\n$ export NAMESPACE=production $ export DB_USERNAME=prod-admin $ export DB_PASSWORD=Dhhs22kfid9c $ sed -e \u0026quot;s/%NAMESPACE%/${NAMESPACE}/\u0026quot; \\ -e \u0026quot;s/%DB_USERNAME%/${DB_USERNAME}/\u0026quot; \\ -e \u0026quot;s/%DB_PASSWORD%/${DB_PASSWORD}/\u0026quot; \\ ~/effective-shell/templates/secret.template.yaml apiVersion: v1 kind: Secret metadata: name: database-credentials namespace: production stringData: username: prod-admin password: Dhhs22kfid9c In fact, we could even take this a step further and simply replace every environment variable!\nfile_path=\u0026quot;~/effective-shell/templates/secret.template.yaml\u0026quot; env_var_names=$(env | sed -E -n 's/^([^=]+)(=.*)/\\1/p') for env_var_name in ${env_var_names}; do echo \u0026quot;Checking for '${env_var_name}'...\u0026quot; if grep -q \u0026quot;%${env_var_name}%\u0026quot; \u0026quot;${file_path}\u0026quot;; then echo \u0026quot;-\u0026gt; Found '${env_var_name}', expanding now...\u0026quot; env_var_value=\u0026quot;${!env_var_name}\u0026quot; escaped_env_var_value=$(echo ${env_var_value} | sed -e 's/[\\/\u0026amp;]/\\\\\u0026amp;/g') sed -e \u0026quot;s/%${env_var_name}%/${escaped_env_var_value}/\u0026quot; \\ \u0026quot;${file_path}\u0026quot; \u0026gt; \u0026quot;${file_path}.tmp\u0026quot; mv \u0026quot;${file_path}.tmp\u0026quot; \u0026quot;${file_path}\u0026quot; fi done Now this might look like a lot at first, and to be fair it is! But almost everything here is actually using commands and concepts we've already seen.\nHere's what's going on blow-by-blow:\n file_path - we're just creating a variable to hold the name of the file, this makes it easier to apply the command to other files env_var_names=... - we use sed to get the name of each environment variable. This comes from everything before the = sign in the output of the env command. for ... - this lets us \u0026lsquo;loop\u0026rsquo; through each environment variable name found - we'll see more about this in the sections on scripting. if grep -q - check to see if the environment variable name is used in the file\u0026hellip; \u0026hellip;and if it is, get the value of it with ${!env_var_name} - the ! exclamation mark is variable indirection and is Bash specific. It allows us to get the value of a variable which has a variable name escaped_env_var_value we need to replace some of the special characters which might be in the environment variable so that they don't confuse sed sed -e run the replacement, putting the results in a temporary file\u0026hellip; \u0026hellip;replace the source file with the temporary one  Many of these concepts, like for loops and variable indirection we will see in more detail later. But this little snippet really shows the power of Linux, the GNU tools and the shell - we can create sophisticated operations by composing together small and simple commands.\nA Note on Security It is very important to be careful when running commands like this or writing scripts which use this kind of pattern.\nAllowing the contents of your environment variables to be put into files, or even the shell's history can be a serious security concern.\nAs an example - note what would happen if we replaced ${DB_USERNAME} with ${USERNAME} in the script above? In Z Shell rather than the value we provided being put in the file, the actual username of the user running the script would be written (which in my case would be dwmkerr).\nBe very careful when working with environment variables to make sure you avoid exposing private information. Even more sensitive variables might be things like ${AWS_SECRET_ACCESS_KEY} - exposing variables like this could allow attackers to start accessing resources on your cloud environments.\n What About \u0026lsquo;In Place\u0026rsquo; Editing? You might be aware that there is an \u0026lsquo;in-place\u0026rsquo; feature in sed which allows you to change the file you pass it directly. This allows you to do something like the below:\n$ sed -i '.bak' 's/staging/production/' test.txt This would perform the substitutions and then put them in a file with .bak appended. To just overwrite the existing file, you could use:\n$ sed -i '' 's/staging/production/' test.txt In this case we don't append anything to the name of the overwritten file, so we end up replacing the original file itself.\nHow the -i flag works can vary on some systems so I generally prefer to simply output the result of sed to a new file and then replace the old one. However, it is useful to know what this flag is and how it is used, as you will often see it in examples.\nWhat about Awk? There is another very powerful text manipulation tool - awk. Often if you trying to find out how to perform more complex text based operations, you'll see awk in the mix as a potential solution.\nAwk is very sophisticated and has its own language to support complex transformations and operations. My advice is to first master sed and then consider learning awk if you regularly find yourself limited by sed.\nWhen to Program Personally, if I have tasks which are too complex for me to solve with the fairly basic knowledge of sed that I have, I will generally write a small program in Python, Node or another high-level and expressive language to do the work, and call that instead.\nThis will often be easier to maintain and understand than an extremely complex sed expression. But when you decide to move from a shell command to a programming language will be a decision which you will have to make on a case by case basis.\nIn a later chapter, we'll look at how to write well-behaved command line programs which we can compose together using familiar mechanisms like pipelines to build more tools for our toolkit.\nSummary In this chapter we:\n Introduced sed, the stream editor command Saw that when we need to transform or manipulate text, sed is often a great tool for the job Learnt how to perform simple substitutions with sed, using commands such as sed 's/old-text/new-text/' Saw the components which make up a sed expression - the function, the expression and when needed, the line pattern Saw how to use -e to apply multiple patterns Went deeper into extended regular expressions Saw an example of how to strip comments from a file with sed Saw how to remove empty lines with sed Looked into line patterns in more detail, showing how we can choose what lines sed operates on Saw how to append text to lines with sed Saw how to use patterns and capture groups to extract information from lines with sed Saw more advanced examples of capture groups, allowing us to capture different parts of a match and manipulate them individually, or recompose them Saw how sed can be used to implement a simple \u0026lsquo;template\u0026rsquo; mechanism for files Saw how the -i (in place) parameter works for sed Briefly described awk as a potential alternative tool to learn about for more sophisticated text manipulation Suggested that if something is too complex to write easily in sed, it may be faster to implement it with a programming language   Footnotes\n  Note that if we used rm -r instead of rmdir, which is very common, we run the risk of making a big mistake - passing the source directory for the cp command as the first parameter to remove. This would mean we run rm -r on the source files for the backup and the backup folder itself, deleting both! This is one place where using rmdir makes a bit more sense - it won't delete the source files if we make a mistake! \u0026#x21a9;\u0026#xfe0e;\n You can use any character to delimit expressions - sometimes people will use # or | rather than a forward slash, typically because they want to use the forward slash as part of a pattern. \u0026#x21a9;\u0026#xfe0e;\n   "});index.add({'id':17,'href':'/docs/part-3-manipulating-text/build-commands-on-the-fly/','title':"Build Commands on the Fly",'content':"Chapter 17 - Build Commands on the Fly with Xargs In the earlier chapters of this part of the book we've seen a number of ways to manipulate text. Now we're going to introduce the xargs command and show how to use our text manipulation skills to dynamically build complex commands on the fly.\nIntroducing Xargs The xargs [build and execute commands] command takes input, uses the input to create commands, then executes the commands. I tend to remember it as \u0026ldquo;Execute with Arguments\u0026rdquo; as the name xargs sounds a little odd!\nHow xargs is used is probably easiest to see with an example. Let's use it to build a set of commands which will remove any empty files from a folder.\nBefore we show xargs let's create some empty files which we'll later clean up:\n$ mkdir -p ~/effective-shell/tmp $ cd ~/effective-shell/tmp $ touch file{1..100}.txt We're using a nice shell trick here called Brace Expansion - the shell will expand file{1..100}.txt into file1.txt, file2.txt and so on, all the way to file100.txt.\nWe could just look for empty files in our /tmp folder for this example, but a file in that folder might be in use, so a safer way to demonstrate xargs is to use some temporary files which we create ourselves.\nWe could search for empty files with the command below:\n$ find . -empty file1.txt file2.txt file3.txt file4.txt file5.txt ... A refresher on Finding Files\nIn this chapter we'll be using the find (find files and folders) command a lot - if you need a refresher, check Chapter 11 - Finding Files.\n The find command has outputted a list of files, now we want to use the rm (remove file) command to delete each one. Let's just pipe the list of files to the rm command, check Chapter 7 - Thinking in Pipelines if you need a reminder of how piping works:\n$ find . -empty | rm rm: missing operand Try 'rm --help' for more information. What's going on here? Well basically the issue is that the rm command doesn't actually read the list of files from stdin, the list of files has to be passed as a parameter to the command. How can we take this list of files and pass it to rm as a set of parameters?\nThis is what xargs is for! Before we delete the files, let's just see what happens when we pass the list to xargs:\n$ find . -empty | xargs ./file40.txt ./file8.txt ./file35.txt ./file81.txt ... By default xargs take the input, joins each line together with a space and then passes it to the echo command. The echo command writes it out to the screen.\nWe can change the command xargs passes the arguments to:\n$ find . -empty | xargs echo rm rm ./file40.txt ./file8.txt ./file35.txt ./file81.txt ... Very interesting! Now we've told xargs to pass the output to the echo rm command - this just writes out rm followed by the list of files. Putting echo before whatever command you want to run is a useful way to check the command before we commit to running it.\nLet's finish the job and delete each file:\n$ find . -empty | xargs rm Done! You can run ls to confirm that the file has been deleted.\nThis is xargs - it constructs and executes a command using arguments from standard input. Now let's see how we can take this further.\nHandling Whitespace, Special Characters and Tracing One common challenge with xargs is how to deal with spaces. To see what I mean, let's create three files with spaces in the names:\n$ touch \u0026quot;chapter \u0026quot;{1,2,3}.md $ find . -type f ./chapter 1.md ./chapter 2.md ./chapter 3.md What if we wanted to delete these files? Let's try that with rm:\n$ find . -type f | xargs rm rm: cannot remove './chapter': No such file or directory rm: cannot remove '1.md': No such file or directory ... The file name has a space in it, which is confusing rm as it thinks we're providing six paths rather than three.\nWe can use the -t (trace) option to see what xargs actually tried to do:\n$ find . -type f | xargs -t rm rm ./chapter 1.md ./chapter 2.md ./chapter 3.md ... Hopefully you can spot the error - the rm command thinks it needs to remove six files, because there are spaces in the filenames and there are not quotes around the filenames to let rm know this!\nFortunately, find loves xargs - they are part of the same package of tools (which is called \u0026lsquo;findutils\u0026rsquo;). And there's a special pair of options that can deal with this.\nFor find, we are going to use the -print0 action and for xargs we'll use the -0 option. Let's see how it looks now, then describe what's going on under the hood:\n$ find . -type f -print0 | xargs -0 -t rm rm './chapter 1.md' './chapter 2.md' './chapter 3.md' In Chapter 11 - Finding Files we saw that the default action of the find command is -print, which writes out the path of each item found. The -print0 action is very similar - but it instead it writes out each item followed by a special \u0026lsquo;null\u0026rsquo; character1.\nNow that we've told find to end each result with a special \u0026lsquo;null\u0026rsquo; character, we just tell xargs that the \u0026lsquo;null\u0026rsquo; character is what separates each line of input. We do this with the -0 (use NUL as separators) option.\nYou don't need to really understand the internals - if you are a computer programmer it might make sense, this is how strings in things like the C Programming language work. All you need to know is that it means the xargs program won't get confused when it sees spaces, tabs, quotes, newlines, or anything else which might be goofy in a file name.\nMy recommendation would be to always pair up the -print0 action with the -0 option - it means you won't get caught out by odd file names. And definitely make use of the -t (trace) option to see what xargs is actually doing!\nOne Command or Many Commands? By default xargs takes all of the input and passes it as a set of arguments to the provided command. We can see this below:\n$ touch file{1..5} $ find . -type f | xargs echo ./file1 ./file2 ./file3 ./file4 ./file5 We don't need to provide echo to xargs, it is the default, but I have added it for clarity. But what is really important is that we have called echo once and once only.\nxargs has passed all of the arguments it has been given to the command.\nWe can tell xargs how many lines of input it should use for the command with the -L (max lines) parameter:\n$ find . -type f | xargs -L 1 echo ./file1 ./file2 ./file3 ./file4 ./file5 We've now called the echo command once for each line of input, meaning that echo has been called five times.\nIn general if you can provide all of the arguments to a single command the system may be able to process the command slightly faster. However, if there are lots of arguments, the command itself might not be able to handle all of the arguments you give it.\nYou can set -L to other values too - xargs will use up to the number of lines provided:\n$ find . -type f | xargs -L 3 echo ./file1 ./file2 ./file3 ./file4 ./file5 Here we've allowed up to three input lines per command.\nYou will probably not use the -L parameter very often, but it is really important that you understand what it does. And that is because many of the other options we'll use imply -L 1 - we'll see why in the next example.\nfind . -name \u0026quot;chapter*\u0026quot; | xargs rm rm: cannot remove './chapter': No such file or directory rm: cannot remove '1.txt': No such file or directory Constructing more complex commands with the \u0026lsquo;I\u0026rsquo; Parameter You have probably noticed by now that the xargs command puts the arguments it is given at the end of the command you write.\nWhat if you need the arguments to go somewhere else? For example, what if I wanted to copy every text file in a folder to another location?\nHere's how we might start - and what'll go wrong!\n$ find . -name \u0026quot;*.txt\u0026quot; -print0 | xargs -0 -t cp ~/backups cp /home/dwmkerr/backups ./file2.txt ./file3.txt ./file1.txt cp: target './file1.txt' is not a directory The problem is that the destination location for where we copy the files has to be the last parameter - but xargs puts the list of files at the end of the command.\nAnd by the way - we've used the -0 parameter to make sure that funny filenames are handled properly (a good habit to get into) and the -t parameter to trace - which means we see the command which will be run.\nSo we need to tell xargs where to put the list of arguments. We can do that with the -I (replace string) parameter. This parameter lets us tell xargs exactly where we want to put the arguments:\n$ find . -name \u0026quot;*.txt\u0026quot; -print0 | xargs -0 -t -I {} cp {} ~/backups cp ./file2.txt /home/dwmkerr/backups cp ./file3.txt /home/dwmkerr/backups cp ./file1.txt /home/dwmkerr/backups Here we have set the \u0026lsquo;replacement string\u0026rsquo; to be {}. This means when xargs sees {} in the command it will replace it with the arguments we provide as its input.\nThe first observation you might make is that as soon as we use the -I parameter it automatically implies that we use the -L 1 parameter, i.e. we run the command once for each individual input line.\nFor the example we have shown above, this isn't really necessary, xargs could just write all of the arguments. The reason xargs does this is that we are not actually limited to using the replacement once only - we can use it multiple times.\nHere's a similar example, but in this one we put .bak at the end of each filename as we copy it:\n$ find . -name \u0026quot;*.txt\u0026quot; -print0 | xargs -0 -t -I {} cp {} ~/backups/{}.bak cp ./file2.txt /home/dwmkerr/backups/./file2.txt.bak cp ./file3.txt /home/dwmkerr/backups/./file3.txt.bak cp ./file1.txt /home/dwmkerr/backups/./file1.txt.bak Because we can use the replacement string multiple times, xargs splits up the commands so it is one command per input argument. If it didn't do this and we tried the command above it would not work properly.\nThe -I parameter is incredibly powerful, it lets us construct complex commands.\nYou don't need to use the {} letters as the replacement string, any sequence of characters will work. For example:\n$ env | xargs -I % echo \u0026quot;You have env var: % set!\u0026quot; You have env var: SHELL=/bin/bash set! You have env var: COLORTERM=truecolor set! You have env var: EDITOR=vi set! In this example we used % as the replacement string.\nYou might wonder why is {} so commonly used in examples or the manpages. The reason is that this is the default replacement string used by find if we perform an action like -exec:\n$ find . -type f -empty -exec stat {} \\; The {} characters are used as the placeholder for files found with the find command, so people often use the same placeholder for xargs, but you are not required to use these characters.\nRequesting Confirmation with the Prompt Option The -p (prompt) option tells xargs to ask the user to confirm each command before it is run.\nLet's test this out by deleting a set of \u0026lsquo;pods\u0026rsquo; from a Kubernetes cluster. You don't have to worry about what a Kubernetes cluster is, I'm just using this as an example to highlight that you don't have to be limited to using find as the input for xargs!\nOn my machine I can show the pods available to me in my cluster with this command:\n$ kubectl get pods -o name pod/my-app pod/nginx pod/postgres Three pods are shown. I could use this command to build input to xargs to let me to chose which pods to delete, interactively:\n$ kubectl get pods -o name | xargs -L 1 -p kubectl delete kubectl --context minikube delete pod/my-app?...n kubectl --context minikube delete pod/nginx?...y pod \u0026quot;nginx\u0026quot; deleted kubectl --context minikube delete pod/postgres?...n This is fantastic! We've used -L 1 to make sure that we only deal with one pod at a time (rather than trying to delete all three at once) and the -p flag to ask the user to press \u0026lsquo;y\u0026rsquo; or \u0026lsquo;n\u0026rsquo; in each case. The xargs command helpfully shows us what it is going to do and asks for confirmation first.\nI think this really hints at the true power of xargs - yes it can be combined with find to perform operations on files, but it can also be used with other tools to build more complex operations.\nSplitting up Input with a Delimiter We can ask the user whether they want to see files in all of their \u0026lsquo;path\u0026rsquo; locations with the command below:\n$ echo $PATH | xargs -d ':' -p -L 1 ls ls /home/dwmkerr/.pyenv/shims ?...n ls /home/dwmkerr/.nvm/versions/node/v14.15.1/bin ?...n The $PATH environment variable holds all of the folders the shell will search in for binaries - and each folder is separated by a : colon character (you can read more about $PATH in Chapter 10 - Understanding Commands.\nWe use the -d (delimiter) parameter to tell xargs that each argument in the input is separated with a colon. We also use the -L 1 and -p parameters to process this input one folder at a time and ask the user if they want to see the contents of the folder.\nSummary In this chapter we introduced xargs, a powerful command which allows us to build other commands on the fly. We can trace, showing how the resulting command will look, ask the user for confirmation, control how many commands we run and more.\nThere are more options for the xargs command, you can read all about them with man xargs. But I think if you learn the key parameters we've shown in this chapter you'll be well equipped to use xargs in your day to day wok.\nIn the next chapter we'll look at some of the advanced features which are built into most shells which allow us to manipulate text.\n Footnotes\n  The character is ASCII NUL, which is the number zero. This is often used in programming to represent \u0026lsquo;null\u0026rsquo; or \u0026lsquo;nothing at all\u0026rsquo;, not the digit zero as is used when printing to the screen, which is actually represented by number 30. You can see the actual ASCII table with man ascii. \u0026#x21a9;\u0026#xfe0e;\n   "});index.add({'id':18,'href':'/docs/part-4-shell-scripting/shell-script-essentials/','title':"Shell Script Essentials",'content':"Chapter 18 - Shell Script Essentials First we're going to look at how to write shell scripts as well as the different ways to execute them. We'll look at how shell script files should be structured and how to use \u0026lsquo;shebangs\u0026rsquo; to define how a shell script will run.\nThese will be essential techniques to have as a foundation for building your own scripts. Even if you are familiar with shell scripts I would suggest skimming this chapter to make sure you understand each of the concepts, particularly the later section where we talk about using the env command in shebangs.\nWhat is a Shell Script? A shell script is just a text file which contains a set of commands. As soon as you find yourself repeating the same sequence of commands in a shell, it might be worth saving these commands to a file and running the file instead.\nSaving your commands to a file has a number of benefits. It saves time - you don't need to type the commands out each time you want to run them! You can use your favourite editor to build the script file, and you can add \u0026lsquo;comments\u0026rsquo; to describe what you are trying to achieve (which will make it far easier to update the script over time). Files can also easily be shared - meaning you can copy these scripts to other machines or share them with others who might find them useful.\nCreating a Basic Shell Script Let's create a simple shell script that shows us our most commonly used shell commands.\nAlmost every command that is needed to build the script has been discussed in the book already, so it shouldn't be too unfamiliar. But I'll still break it down blow-by-blow to help us understand it.\nAs we go through this section of the book we're going to extend this script and make it more useful!\nThe \u0026lsquo;common\u0026rsquo; Command We're going to create a new command, called \u0026lsquo;common\u0026rsquo;, that shows the most commonly used shell commands.\nWe should be able to do this using techniques we've seen so far. We'll do it like this:\n Read a large number of commands from the history Sort the commands, then count the number of duplicates Sort the commands, by the number of duplicates (i.e. ordering by \u0026lsquo;most commonly used\u0026rsquo;) Print the results to the screen.  Let's get started!\nCreating a Simple Script It's going to take some trial and error to get our commands right. So let's start by creating a shell script, which we'll run again and again.\nIn your favourite editor, create a file called common.v1.sh and put it somewhere on your system. As an example, I'm going to create a folder called scripts in my home directory and create the common.v1.sh file there:\n# Create a directory called \u0026#39;scripts\u0026#39;. # Using the \u0026#39;-p\u0026#39; flag means we won\u0026#39;t get an error if the folder exists. mkdir -p ~/scripts # Create the script file. touch ~/scripts/common.v1.sh # Open the script file in my favourite editor. vi ~/scripts.sh I have called the script common.v1.sh rather than common.sh because in each chapter of this section we are going to improve upon the script and change the version number. So in later chapters we will create common.v2.sh, common.v3.sh and so on.\nThese commands should be familiar. The mkdir command creates a directory. The -p (create parent directories if needed) flag stops the command from returning an error if the directory already exist.\nThe touch command creates an empty file with the given name. Finally, I open the file in an editor. I am using Vim, but you can open this file in any editor you like.\nBefore we build the script, let's quickly talk about comments.\nComments\u0026lt;!\u0026ndash;index..\u0026gt; The shell ignores any text which follows a # hash symbol. Whether this is text you type into a shell, or text in a shell script, the shell will ignore the content.\nThis is extremely useful - it means we can use the hash symbol to add comments to our scripts and commands. These comments are not interpreted by the shell, they are added just to make it easier for us to describe what is going on. If you come from a programming background you will likely be familiar with comments.\nHere are a few examples:\n# This is a comment - we can use this to describe what we\u0026#39;re trying to do. echo \u0026#34;Hello Shell\u0026#34; # Comments can go at the end of a line... # You can also use a comment symbol to \u0026#39;comment out\u0026#39; a line: # echo \u0026#34;Goodbye Shell\u0026#34; From this point on we'll use comments a lot to explain what we are trying to accomplish with each section of a script. It is generally good practice to use comments to describe your intent - why you are doing something. This is far more useful for the reader than what you are doing. The \u0026lsquo;what\u0026rsquo; should be clear from the commands - the \u0026lsquo;why\u0026rsquo; is the thing readers will likely want to understand.\nHere's an example of a bad comment:\n# Write the CSV file, reverse it, cut it, reverse it. cat ~/effective-shell/data/top100.csv | rev | cut -d\u0026#39;,\u0026#39; -f1 | rev The comment just describes what the script is doing. But it doesn't explain why. A better comment would be:\n# We want to extract the last field (the number of reviews) for each film. # Because we don\u0026#39;t know how many fields there are we can reverse the text before # we cut it - then the last field becomes the first, which we extract and then # put back into the correct order by reversing it again. cat ~/effective-shell/data/top100.csv | rev | cut -d\u0026#39;,\u0026#39; -f1 | rev If you don't come from a programming background you might think that many of these comments are a little obvious. But as you write more and more code you'll realise that something that seemed obvious when you wrote it a while ago can look surprisingly baffling even just a few days later!\nNow that we've discussed comments, we'll build our common.v1.sh shell script.\nBuilding and Testing the Script Add the following commands to the common.v1.sh file:\n# Write the title of our command. echo \u0026quot;common commands:\u0026quot; # Show the most commonly used commands. tail ~/.bash_history -n 1000 | sort | uniq -c | sed 's/^ *//' | sort -n -r | head -n 10 This is a short script, but there is quite a lot going on. Let's look at it blow-by-blow:\n First we take the last 1000 lines of the ~/.bash_history file using the tail command1 Then we sort the commands. This will put all of the duplicates next to each other Then we remove all duplicates and use the -c (show count) flag to count the duplicates Then we remove the leading spaces from the output (which we need to do so that we can sort properly) Then we sort numerically and in reverse - the highest count first Finally, we limit the results to the first ten items  If you need a refresher on the shell history, sort or uniq the check the Slice and Dice Text chapter. If the sed command doesn't look familiar then check the Advanced Text Manipulation with Sed chapter.\nIf you want to see a more detailed breakdown of how the script works, check Appendix - How the Script Works. But this is not necessary for you to follow the content in this chapter.\nNow save the file. In your shell, run the following command to execute the file:\nsh ~/scripts/common.v1.sh The sh (shell) command starts a new shell. When we pass the path of a shell script, the shell command will run the script and then exit. The output you see will look something like this:\ncommon commands: 463 vi 267 gc 238 ga . 212 ls 169 gpo 122 make dev 112 gl 97 gcm 96 gpr You can see my most common commands are short aliases for Git commands (the ones that start with \u0026lsquo;g\u0026rsquo;), opening Vim, running a makefile command and a few others.\nWe now have a basic shell script. Let's look at a few different ways we can run the script.\nMulti-line Commands You can use the \\ backslash character to create a \u0026lsquo;continuation\u0026rsquo; that tells the shell it needs to join lines up. This allows you to break long commands into multiple lines.\nAs an example, we could re-write our pipeline command to look like this:\n# Show the most commonly used commands. tail ~/.bash_history -n 1000 \\  | sort \\  | uniq -c \\  | sed \u0026#39;s/^ *//\u0026#39; \\  | sort -n -r \\  | head -n 10 This will probably look very familiar to anyone with a background in functional programming!\nBe careful when you split lines up - the continuation character must be the last character on the line. If you add something after it (such as a comment) then the command will fail.\nRunning a Shell Script There are a few different ways we can run shell scripts.\nThe first is to run a shell program and pass the script as a parameter. This is what we did in the earlier example. Here's another example of how we could run the script we created:\nbash ~/scripts/common.v1.sh This is a perfectly valid technique. Now let's see the other ways we can run a script.\nThe next way we can run a script it is make it \u0026lsquo;executable\u0026rsquo;. This means we change the file permissions of the script file, adding the \u0026lsquo;executable bit\u0026rsquo;. This tells the systems we can run the file. We use the chmod (change file mode) command to do this:\nchmod +x ~/scripts/common.v1.sh If the chmod command looks unfamiliar then check the Understanding Commands chapter. Now that the file has been made executable, we can simply enter the path to the file and run it, as if it was any other command:\n~/scripts/common.v1.sh There is a problem with this approach though. How this file is executed is going to vary depending on how your system is set up2. For example, if you are using Bash, then the script will run in a new instance of the Bash shell. However, if you are using the Z shell, then the script will most likely run in the sh program (and depending on your system, this program might just be a link to another type of shell).\nWe want to avoid any ambiguity and be explicit about what program should run our script. We can do this using a special construct called a shebang.\nUsing Shebangs A shebang is a special set of symbols at the beginning of a file that tells the system what program should be used to run the file.\nIf we were to add a shebang to our common.v1.sh file, it would look like this:\n#!/usr/bin/sh  # Write the title of our command. echo \u0026#34;common commands:\u0026#34; # Show the most commonly used commands. tail ~/.bash_history -n 1000 | sort | uniq -c | sed \u0026#39;s/^ *//\u0026#39; | sort -n -r | head -n 10 The shebang is the two characters - #!. The name \u0026lsquo;shebang\u0026rsquo; comes from the names of the symbols. The first symbol is a \u0026lsquo;sharp\u0026rsquo; symbol (sometimes it is called a hash, it depends a little on context). The second symbol is an exclamation point. In programming the exclamation point is sometimes called the \u0026lsquo;bang\u0026rsquo; symbol. When we put the two together, we get \u0026lsquo;sharp bang\u0026rsquo;, which is shortened to \u0026lsquo;shebang\u0026rsquo;.\nImmediately after the shebang you write the full path to the program which should be used to open the file.\nFor example, if you wanted to write a script that is run in Python, you could do this:\n#!/usr/bin/python3 print(\u0026#39;Hello from Python\u0026#39;) If we wanted to explicitly use the Bash shell to run a script, we might use a shebang like this:\n#!/usr/bin/bash  echo \u0026#34;Hello from Bash\u0026#34; What about Node.js? Easy!\n#!/usr/bin/node  console.log(\u0026#34;Hello from Node.js\u0026#34;); Shebangs - Dealing with Paths When we use a shebang we need to provide the full path the executable that will be used to run the script.\nFor example, what if we want to use Ruby to run a script we could write a script like this:\n#!/usr/bin/ruby puts \u0026#39;Hello from Ruby\u0026#39; But there is a problem here. This will only work if you have the Ruby program installed in the location specified after the shebang (i.e. /usr/bin/ruby). If you do not have the Ruby program in this location the script will fail to run.\nHow can we know where the user will have a specific program installed?\nThere is a common trick for dealing with this issue. We can use the env (set environment and execute command) command to run a command and it will work out the path for us.\nThe env command is often used to show environment variables, but you can also use it to execute an arbitrary command (often with a modified environment). One handy feature of the env command is that it looks through the $PATH variable to find the path of the command to execute.\nYou can see this by running a command like the below:\n$ env python3 Python 3.8.5 (default, Jan 27 2021, 15:41:15) [GCC 9.3.0] on linux Type \u0026quot;help\u0026quot;, \u0026quot;copyright\u0026quot;, \u0026quot;credits\u0026quot; or \u0026quot;license\u0026quot; for more information. \u0026gt;\u0026gt;\u0026gt; We've used the env command to run the python3 command - and it worked out the correct path for us.\nTo use env in a shebang, specify the full path to env (which should be the same on all Unix-like systems) and then provide the name of the command to run:\n#!/usr/bin/env bash echo \u0026quot;Hello from Bash\u0026quot; Or another example:\n#!/usr/bin/env ruby puts 'Hello from Ruby' Using a shebang to specify the exact command to run, and then using the env command to allow the $PATH to be searched is generally the safest and most portable way to specify how a shell script should run.\nInstalling Your Script Before we finish with our shell script fundamentals, we'll take a look at one final commonly used pattern to run shell scripts - installing them as a local binary.\nOur common.v1.sh script (with the added shebang) looks like this:\n#!/usr/bin/env sh  # Write the title of our command. echo \u0026#34;common commands:\u0026#34; # Show the most commonly used commands. tail ~/.bash_history -n 1000 | sort | uniq -c | sed \u0026#39;s/^ *//\u0026#39; | sort -n -r | head -n 10 If we have made the script executable with the chmod command, then we can run the script by simply typing the location of the script in the shell:\n$ ~/scripts/common.v1.sh common commands: 97 gcm 96 gpr ... If we want to \u0026lsquo;install\u0026rsquo; this script as a local command which we can run easily, we can create a symbolic link to the shell script in our /usr/local/bin folder:\nln -s ~/scripts/common.v1.sh /usr/local/bin/common The ln (create link) command creates a link (which is like a shortcut in Windows and other desktop systems) in our /usr/local/bin folder, with the name common, which points to the script we have written. We can now run the common command without specifying its path:\n$ common common commands: 97 gcm 96 gpr ... This works because when the shell sees a command, it searches through the folders in the $PATH environment variable to find out where the command is. And the /usr/bin/local folder is in this list of paths.\nWhy do we use the /usr/bin/local folder rather than the /usr/bin folder? This is just a convention. In general, the /usr/bin folder is for commands which are installed with package manager tools like apt or Homebrew (on MacOS). The /usr/local/bin folder is used for commands which you create for yourself on your local machine and manage yourself3.\nSummary In this chapter we've covered quite a few of the fundamentals of shell scripts:\n How to create a shell script How comments work in shell scripts How to handle long lines with continuations How to run a shell script How to make a shell script executable How shebangs work How to use the env command to make our shebangs more portable How to \u0026lsquo;install\u0026rsquo; scripts for the current user  In the next chapter we'll look at how to add logic to our shell scripts.\n Appendix - How the Script Works This section briefly covers how the common.v1.sh script works. Assuming we have a history that looks like this:\nvi README.md git status git checkout main git status restart-shell git status open . vi README.md open . First we sort, putting duplicate lines next to each other:\ngit checkout main git status git status git status open . open . restart-shell vi README.md vi README.md Then we use uniq to remove duplicate adjacent lines, passing the -c flag to include a count:\n 1 git checkout main 3 git status 2 open . 1 restart-shell 2 vi README.md Now we remove the leading whitespace:\n1 git checkout main 3 git status 2 open . 1 restart-shell 2 vi README.md Finally we sort numerically (by using the -n flag) and in descending order (by using the -r flag):\n3 git status 2 vi README.md 2 open . 1 restart-shell 1 git checkout main Why the numeric sort? If we didn't sort numerically and instead performed the default lexographic sort and have more than single digit results, the output would look like this:\n1 git checkout main 1 restart-shell 13 git status 2 open . 2 vi README.md This is a lexographic sort - the line starting with 13 comes after the line starting with 2. We want to sort by the value of the number.\n  The path to the shell history file is normally available in the $HISTFILE environment variable. However, in a non-interactive shell this variable is not set (and when we run a shell script, it is run in a non-interactive shell). We'll see more about interactive and non-interactive shells later, this is just a note in case you are wondering why we don't use the $HISTFILE variable or history command! \u0026#x21a9;\u0026#xfe0e;\n Try putting the command pstree -p $$ in a shell script and running the script - you'll see exactly what process is run. \u0026#x21a9;\u0026#xfe0e;\n If you want to know more about these folders and the conventions behind them then check back soon, I am going to be adding an entire section on Linux Fundamentals, and one of the chapters will specifically be on the Linux Filesystem. This will cover \u0026lsquo;The Linux Filesystem Hierarchy Standard\u0026rsquo; which defines how folders like this should be used. \u0026#x21a9;\u0026#xfe0e;\n   "});index.add({'id':19,'href':'/docs/part-4-shell-scripting/variables-reading-input-and-mathematics/','title':"Variables, Reading Input, and Mathematics",'content':"Chapter 19 - Variables, Reading Input, and Mathematics We've seen variables a few times in our journey so far. In this chapter we'll look at variables in a bit more detail. We'll then see how to read input from the user and also look at how to perform basic mathematical operations in the shell.\nVariables Variables are places where the system, the shell, or shell users like ourselves can store data.\nWe've already seen variables a few times in this book. For example in Chapter 5 - Getting Help we saw the $PAGER variable that is used to specify what pager program should be used in the shell.\nWhen we want to use a variable in the shell, we use the $ dollar symbol to specify the variable name:\necho \u0026#34;Your pager is: $PAGER\u0026#34; If you run this command you will see something like this:\nYour pager is: less By convention, if a variable is in uppercase then it is an _environment variable_ or a built in variable that comes from the shell. An environment variable is a variable that is set by the system. They often contain useful values and are used to help configure your system.\nHere's a few common environment variables we might use:\necho \u0026#34;Your shell is: $SHELL\u0026#34; echo \u0026#34;Your user is: $USER\u0026#34; echo \u0026#34;Your user\u0026#39;s home directory is: $HOME\u0026#34; Your output will look similar to the below:\nYour shell is: /bin/bash Your user is: dwmkerr Your user's home directory is: /home/dwmkerr Setting Variables You can create or set your own variables by simply entering the name you would like to use and putting an = equals symbol after the variable, followed by the value you would like to use.\nThis is the one of the few times that you will use a variable name without putting a dollar symbol before it!\nname=\u0026#34;Dave\u0026#34; location=\u0026#34;Singapore\u0026#34; echo \u0026#34;Hello $namein $location\u0026#34; This will produce the output:\nHello Dave in Singapore By convention, variables that you define yourself should be lowercase. This helps to distinguish between environment variables and your own variables.\nIt is a good habit to use lowercase for variable names. Using uppercase will work, but when you use uppercase you run the risk of \u0026lsquo;overwriting\u0026rsquo; the value of an environment variable and causing unexpected results later.\nFor example, in this snippet I accidentally overwrite the USER variable. If a later part of the script expects the USER variable to contain the Linux username of the user then there will likely be an error because I have set it to something else!\n# Don\u0026#39;t do this! USER=\u0026#34;Dave Kerr\u0026#34; # If I wanted to go to my home directory, this command would fail. That\u0026#39;s # because USER should be \u0026#39;dwmkerr\u0026#39; but I\u0026#39;ve set it to something else! cd \u0026#34;/home/$USER\u0026#34; If you set a system variable to something incorrect, the impact will be limited to only the script you are running or the shell session you are in, or any commands you run from the script or session - other running programs will not have their copy of the variable changed. You can read more about this in the Processes chapter of the Linux Fundamentals section.\nStoring the Output of a Command into a Variable We can use a _subshell_ to run a command and store the result in a variable.\nFor example, if we had a variable which held a user's password and wanted to show it on the screen in a \u0026lsquo;masked\u0026rsquo; form, where all of the characters are replaced with an asterisks symbol, we could write the password variable into the sed command and replace every character with an asterisks symbol like so:\npassword=\u0026#34;somethingsecret\u0026#34; masked_password=$(echo \u0026#34;$password\u0026#34; | sed \u0026#39;s/./*/g\u0026#39;) echo \u0026#34;Setting password \u0026#39;${masked_password}\u0026#39;...\u0026#34; The output of this script will look like this:\nSetting password '***************'... To execute a set of commands in a \u0026lsquo;sub shell\u0026rsquo;, we can use the $() sequence. Everything inside the brackets will be executed in a new shell. We can then store the output of the commands in a variable by using the = equals symbol.\nBeing Explicit with Variable Names You can use curly braces around the name of a variable to be more explicit about what the variable name is. Let's take a look at why you might need to do this:\necho \u0026#34;Creating backup folder at: \u0026#39;$USERbackup\u0026#39;\u0026#34; mkdir $USERbackup This script shows the output:\nCreating backup folder at: '' usage: mkdir [-pv] [-m mode] directory ... Rather than creating a folder called dwmkerrbackup (which is my $USER variable followed by the text backup), the script has actually failed. That is because it is looking for a variable called USERbackup - which has does not exist!\nTo get around this we would surround the variable name with curly braces like so:\necho \u0026#34;Creating backup folder at: \u0026#39;${USER}backup\u0026#39;\u0026#34; mkdir \u0026#34;${USER}backup\u0026#34; This script will show the correct output:\nCreating backup folder at: 'dwmkerrbackup' If there is ever any potential ambiguity with a variable name you should enclose it with curly braces to be on the safe side. Some people will use curly braces in all circumstances to be as explicit as possible about what the variable name is and reduce the risk of mistakes if someone later comes along to edit or change the code.\nThis script would be improved with the use of a variable of our own to avoid us having to repeat the ${USER}backup text:\nbackupdir=\u0026#34;${USER}backup\u0026#34; echo \u0026#34;Creating backup folder at: \u0026#39;${backupdir}\u0026#39;\u0026#34; mkdir \u0026#34;${backupdir}\u0026#34; In this case creating a variable to save us from creating the backup directory folder name each time we want to use it.\nWe've looked at environment variables and our own local variables. Now let's look at how we can read input from the user and store it in a variable for later usage.\nArrays Arrays are variables that can store multiple values. An array is created by using the equals symbol and putting the array values in parenthesis, like so:\ndays=(\u0026#34;Monday\u0026#34; \u0026#34;Tuesday\u0026#34; \u0026#34;Wednesday\u0026#34; \u0026#34;Thursday\u0026#34; \u0026#34;Friday\u0026#34; \u0026#34;Saturday\u0026#34; \u0026#34;Sunday\u0026#34;) Once you have defined your array you can retrieve an element at a given index by using the square bracket notation shown below:\necho \u0026#34;The first day is: ${days[0]}\u0026#34; echo \u0026#34;The last day is: ${days[6]}\u0026#34; Arrays in Bash start at index zero. Arrays in the Z-Shell start at index one - this can cause confusion and mistakes in scripts so it is something you might have to consider if you are writing scripts that can be used by either shell.\nThere are a number of useful operations you can perform on arrays. An example of each is shown below:\n   Operation Syntax Example     Create Array array=() days=(\u0026quot;Monday\u0026quot; \u0026quot;Tuesday\u0026quot; \u0026quot;Wednesday\u0026quot; \u0026quot;Thursday\u0026quot; \u0026quot;Friday\u0026quot; \u0026quot;Saturday\u0026quot; \u0026quot;Sunday\u0026quot;)   Get Array Element ${array[index]} echo ${days[2]} # prints 'Wednesday'   Get All Elements ${array[@]} echo ${days[@]} # prints 'Monday Tuesday Wednesday Thursday Friday Saturday Sunday'   Set Array Element ${array[index]}=value days[0]=\u0026quot;Mon\u0026quot;   Get Array Indexes ${!array[@]} arr=(); arr[3]=\u0026quot;apple\u0026quot;; arr[5]=\u0026quot;pear\u0026quot;; echo ${!arr[@]} # prints 3 5   Get Array Length ${#array[@]} echo ${#days[@]} # Prints 7   Append to Array array+=(val1 val2 valN) fruits=(); fruits+=(\u0026quot;Apples\u0026quot;); fruits+=(\u0026quot;Pears\u0026quot; \u0026quot;Grapes\u0026quot;); echo ${fruits[@]} # prints 'Apples Pears Grapes'   Get a subset of elements ${array[@]:start:number} echo ${days[@]:5:2} # prints 'Saturday Sunday'    It's important to use curly braces around your array expressions. Note that in the examples above when we set an array value we don't use braces or the dollar symbol - this is consistent with what we've seen so far - variable names do not have a dollar symbol when we are setting a value.\nYou might have noticed from the examples that arrays in Bash can be sparse - that means that you can have \u0026lsquo;gaps\u0026rsquo; in your array. Arrays can also have a mixture of strings or numbers - not every element in an array has to be of the same type.\nWe'll see arrays in more detail in the chapter on Loops.\nAssociative Arrays More recent versions of Bash support the concept of Associative Arrays. These are arrays where rather than having a numeric index associated with each value, you can have a string. This allows you to create a \u0026lsquo;map\u0026rsquo; or \u0026lsquo;hash table\u0026rsquo; structure.\nAn associative array is created using the declare (set variable) command:\n# Create an associative array called \u0026#39;user\u0026#39;. declare -A book # Set some values on the array. book[title]=\u0026#34;Effective Shell\u0026#34; book[author]=\u0026#34;Dave Kerr\u0026#34; # Show one of the values. echo \u0026#34;Book details: ${book[title]}- ${book[author]}\u0026#34; Running this command will show the output:\nBook details: Effective Shell - Dave Kerr If you find yourself using associative arrays, I would expect that there is a good chance you are trying to do something that is more complex than is suitable for a shell script. In this circumstance I'd suggest you read the chapter How to avoid scripting! to see how I'd look at alternative options!\nQuoting Variables and Values There is often a lot of confusion about a specific topic in the shell - when should you surround a variable in quotes? This might sound like it is a purely stylistic question, but surrounding a variable in quotes can dramatically change how your script works.\nWe're going to look at each type of quoting and when it should be used in the examples below. But if you ever need a reminder, run man bash and search for the text QUOTING.\nSingle Quotes - Literal Values Use single quotes when declaring a variable or using a value if you want to use literal text. The shell will not expand special characters or variables:\nmessage=\u0026#39; ~~ Save $$$ on with ** \u0026#34;this deal\u0026#34; ** ! ~~ \u0026#39; echo \u0026#34;$message\u0026#34; This script will show:\n ~~ Save $$$ on with ** \u0026quot;this deal\u0026quot; ** ! ~~ Note that the shell has not tried to expand the ~ tilde into /home/dwmkerr. It has not expanded the * asterisks into a wildcard pattern and it has not tried to use the $ dollar symbol to reference an array.\nSingle quotes should be used when you want to put special characters into a variable, or call a command that includes whitespace or special characters.\nSingle Quotes - ANSI C Quoting There is a special form of single quotes called \u0026lsquo;ANSI C Quoting\u0026rsquo; that allows you to use escape sequences from the C language. ANSI C quoting is single quoting that starts with a dollar symbol. You can use it to use special characters like newlines in a variable:\nmessage1=\u0026#39;Hello\\nWorld\u0026#39; message2=$\u0026#39;Hello\\nWorld\u0026#39; echo \u0026#34;Message 1: $message1\u0026#34; echo \u0026#34;Message 2: $message2\u0026#34; This snippet will show the following results:\nMessage 1: Hello\\nWorld Message 2: Hello World Double Quotes - Parameter Expansion Double quotes work in a very similar way to single quotes except that they allow you to use parameter expansion with the $ dollar symbol and escaping with the \\ symbol. The ```` backtick symbol is also treated differently. Let's see some examples:\ndeal=\u0026#34;Buy one get one free\u0026#34; message=\u0026#34;Deal is \u0026#39;$deal\u0026#39; - save \\$\u0026#34; echo \u0026#34;$message\u0026#34; The output of this snippet is:\nDeal is 'Buy one get one free' - save $ Notice that the $deal value has been expanded into the contents of the $deal variable. The last dollar symbol has been escaped with a \\ backslash - the shell knows that this means we want to use the literal value of the dollar symbol at the end of the message. The backslash has been removed.\nThe backtick character is also treated differently, as the backtick can be used to run a sub-shell:\n$ echo \u0026quot;The date is `date`\u0026quot; The date is Sun 23 May 2021 11:36:54 AM +08 However, you should not use the backtick character to run a sub-shell, it is harder to read than using the dollar and parenthesis syntax we've already seen:\n$ echo \u0026quot;The date is `date`\u0026quot; The date is Sun 23 May 2021 11:36:54 AM +08 No Quotes - Shell Expansion If you don't include quotes around a variable or value, then the shell will perform a series of operations called Shell Expansion. This includes many options we've seen so far, let's take a look at some examples:\nhome=~ tilde=\u0026#34;~\u0026#34; echo \u0026#34;My home is: $home\u0026#34; echo \u0026#34;A tilde is: $tilde\u0026#34; This snippet shows the results:\nMy home is: /home/dwmkerr A tilde is: ~ In the first case the shell has expanded the ~ tilde to the home directory.\nWe do not use quotes around a variable or a value if we want the shell to shell expansion. The following expansions will be performed:\n Brace expansion: touch file{1,2,3} is expanded to touch file1 file2 file3 Tilde expansion: cd ~ is expanded to cd /home/dwmkerr Parameter and variable expansion echo $SHELL is expanded to echo /usr/bin/sh (note that this expansion also occurs with double quotes) Command substitution: echo $(date) is expanded to echo the results of the date command (this also occurs with double quotes) Arithmetic expansion: square=$((4 * 4)) has the value 4 * 4 evaluated mathematically (we see this at the end of this chapter) Word splitting: this is a more complex topic discussed in Chapter 21 - Loops and working with Files and Folders Pathname expansion: ls *.txt is expanded to all filename that match the wildcard pattern *.txt  We are going to see more detail on Shell Expansion as we continue through this part of the book. There is also a detailed explanation in the final section of the book and an appendix with a quick reference.\nQuoting Tips Quoting can seem confusing - but remember these tips and you will generally be on the right path:\n Use double quotes most of the time - they will handle variables and sub-shells for you and not do weird things like word splitting Use single quotes for literal values Use no quotes if you want to expand wildcards  Shell Parameter Expansion Shell Parameter Expansion is the process by which the shell evaluates a variable that follows the $ dollar symbol. In most of our examples we've seen simple expansion where we just expand the variable into its value, like so:\n$ echo \u0026quot;My shell is $SHELL\u0026quot; My shell is: /usr/bin/sh But there are a number of special features we can use when expanding parameters. There are many options available and you can find them all by running man bash and searching for the text EXPANSION. Let's take a look at some of the most common ones.\nLength\nThe ${#var} operator returns the length of the variable var:\nvar=\u0026#34;The quick brown fox jumps over the lazy dog\u0026#34; length=${#var} echo \u0026#34;Length: $length\u0026#34; # Prints: 43 Set Default Value\nThe ${var:-default} operator returns the value of the variable var or the text default if it is not found:\nread -p \u0026#34;Enter your username: \u0026#34; user username=${user:-$USER} echo \u0026#34;Username: $username\u0026#34; # Prints what you typed or the value of $USER otherwise Substring\nThe ${var:start:count} operator returns a subset of the var variable, starting at position start and extracting up to count characters. If count is omitted everything from start to the end of the string is returned.\npath=\u0026#34;~/effective-shell\u0026#34; echo \u0026#34;${path:0:2}\u0026#34; # Prints ~/ echo \u0026#34;${path:2}\u0026#34; # Prints effective-shell Make Uppercase\nThe ${var^^} operator returns the value of var with the text transformed to uppercase:\nmessage=\u0026#34;don\u0026#39;t shout\u0026#34; echo ${message^^} # Prints: DON\u0026#39;T SHOUT Make Lowercase\nThe ${var,,} operator returns the value of var with the text transformed to lowercase:\nmessage=\u0026#34;DON\u0026#39;T SHOUT\u0026#34; echo ${message,,} # Prints: don\u0026#39;t shout Variable Indirection\nThe ${!var_name} operator returns the value of the variable with the name in specified in the var_name variable. This is useful if you want to get the value of a variable but don't know the name of the variable:\nread -p \u0026#34;Enter a variable name: \u0026#34; var_name echo \u0026#34;The value of \u0026#39;${var_name}\u0026#39; is: ${!var_name}\u0026#34; The output of this snippet would look like this:\n$ Enter a variable name: SHELL The value of 'SHELL' is: /bin/bash Notice the similarity to the Array operators such as ${#array[@]} to get the length of an array.\nThere are a number of other operators that exist. They allow you to extract parts of a string, apply regular expressions, manipulate the case and perform a number of complex operations. I would avoid these techniques if possible as they are fairly specific to Bash and likely will be confusing to readers. Some of these substitutions are not available in older versions of Bash.\nIf you need to manipulate text I would recommend that you use the techniques described in Part 3 - Manipulating Text.\nIt is generally enough to know that if you see special symbols inside a ${variable} expression then the writer is performing some kind of string manipulation. Hopefully they have included a comment that describes what they are doing to make it easier to follow!\nYou can find out more about these features in the manual under the EXPANSION section1.\nThe Read Command The read (read from standard input) command can be used to read a line of text from standard input. When the text is read it is put into a variable, allowing it to be used in our scripts.\nLet's see how this look in action!\necho \u0026#34;What is your name?\u0026#34; read echo \u0026#34;Hello, $REPLY\u0026#34; Run the script - when you have finished writing your name, press \u0026lsquo;enter\u0026rsquo;. This is needed because read will keep on reading until it reaches the end of a line, so we need to press \u0026lsquo;enter\u0026rsquo; to complete the input.\nWhat is your name? Dave Hello, Dave The read command reads a line of text from standard input and stores the result in a variable called REPLY. We can then use this variable to use the text that was read.\nWhy is the variable in uppercase? That's because even though we are setting the variable itself, it is still a \u0026lsquo;special\u0026rsquo; variable defined by the shell. It is the variable that read puts its input into if we don't explicitly tell read what the variable name should be.\nReading into a Variable We can tell the read command to put the input it reads into a variable with a name of our choice by specifying the variable name after the command, like so:\necho \u0026#34;What is your name?\u0026#34; read name echo \u0026#34;Hello, ${name}\u0026#34; In general you should provide a variable name for read - it will make your script a little easier to understand. Not every user will know that the $REPLY variable is the default location, so they might find it confusing if you don't provide a variable name. By specifying a variable name explicitly we make our script easier to follow.\nThis also shows good coding practices - your variable names should be descriptive, and inform the reader of what they are likely to be used for. This makes the script easier to follow and maintain over time.\nThis is another time that we use a variable name without putting a dollar before it. It might be helpful to remember that the dollar is used when we want to use the variable and the dollar is omitted when we want to set the variable.\nPrompting for Input Before you run the read command you are probably going to write a message to the user letting them know they need to enter some input. We can either write out a message first to prompt the user, using the echo command as shown above, or we can use the special -p (prompt) parameter:\nread -p \u0026#34;Please enter your name: \u0026#34; name echo \u0026#34;Hello, $name\u0026#34; Now the output will look like this:\nPlease enter your name: Dave Hello, Dave Z-Shell Note\nIf you are using the Z-Shell, then this command will fail as zsh does not use the -p parameter for at prompt. To prompt a user for input with the read command in zsh, just put a line of text after the command that starts with a question mark:\nread \u0026#34;?Please enter your name: \u0026#34; echo \u0026#34;Hello, $REPLY\u0026#34; Reading Secrets The -s (silent) flag can be used to hide the input as it is being written. This is useful if you want to read a secret such as a password:\nread -s -p \u0026#34;Enter a new password: \u0026#34; password masked_password=$(echo \u0026#34;$password\u0026#34; | sed \u0026#39;s/./*/g\u0026#39;) echo \u0026#34;\u0026#34; echo \u0026#34;Your password is: $masked_password\u0026#34; The output of this script will be something like the below:\nEnter a new password: Your password is: ******** This uses the same trick as before to mask the characters. Note that when we use the -s flag, the read command does not print what we've typed - meaning we don't print the \u0026lsquo;enter\u0026rsquo; symbol that the user presses to finish entering text. This means we don't see a new line after the read command. So we use echo \u0026quot;\u0026quot; to write a newline before we show the output.\nLimiting the Input There may be times where you don't want to have the user press \u0026lsquo;enter\u0026rsquo; to indicate that they have finished writing input.\nThere are a couple of ways we can limit the input. The first is to use the -n (number of characters) parameter to limit the number of characters that are read:\nread -n 1 -p \u0026#34;Continue? (y/n): \u0026#34; yesorno echo \u0026#34;\u0026#34; echo \u0026#34;You typed: ${yesorno}\u0026#34; This script will only wait for the user to type a single character as we used the -n flag with the value 1 to specify that we want to read a single character only.\nBecause the user doesn't press \u0026lsquo;enter\u0026rsquo; at the end of their input, we need to add a blank newline before we show the output - otherwise it would look like this:\nContinue? (y/n): nYou typed: n It's only when we read a full line of text that we don't need to write an empty line. That's because when we read a full line of text we finish by pressing \u0026lsquo;enter\u0026rsquo;, which moves the cursor down to the next line for us.\nThe other way to limit the input is to specify a character that is to use a delimiter to indicate when read should stop reading input:\nread -d \u0026#39; \u0026#39; -p \u0026#34;Enter your favourite word (then a space): \u0026#34; word echo \u0026#34;\u0026#34; echo \u0026#34;Your favourite word is: ${word}\u0026#34; Because we used the -d ' ' parameter, the read command will read up until it finds a \u0026lsquo;space\u0026rsquo; symbol. This can be confusing for users however - if they press enter then read will read it as a newline and continue waiting for a space. So you should let the user know to finish input with the delimiter you have chosen!\nIn general using another anything than a newline as the delimiter may be confusing to the user, and also causes some problems when the user wants to type special characters such as backspace, so I would suggest that you avoid this trick. Instead, let the user type their input and then use something like sed to extract everything up to the point that you want.\nThere are a number of other options for the read command that you can read about by typing help read. But these are the ones that I think you will see most commonly used.\nMathematics The shell has some built in features that let you perform mathematical operations. You will commonly perform these operations on variables.\nYou might assume that you can use symbols like + directly in the your scripts to perform mathematical operations - but they may not perform as expected. For example, here's what happens if you try to add two numbers together with the + plus symbol:\nread -p \u0026#34;Enter a number: \u0026#34; number1 read -p \u0026#34;Enter another number: \u0026#34; number2 sum=$number1+$number2 echo \u0026#34;The sum of $number1and $number2is $sum\u0026#34; If you run this script you'll see something like this:\nEnter a number: 23 Enter another number: 34 The sum of 23 and 34 is 23+34 The result we see is not the sum of the two numbers - it is the two numbers with the literal + plus symbol between them.\nTo tell the shell that we want to perform an arithmetic operation, rather than just write out a mathematical operator, we use the \u0026lsquo;double parenthesis\u0026rsquo; syntax shown below:\nread -p \u0026#34;Enter a number: \u0026#34; number1 read -p \u0026#34;Enter another number: \u0026#34; number2 sum=$(($number1 + $number2)) echo \u0026#34;The sum of $number1and $number2is $sum\u0026#34; The output of this script will be something like the below:\nEnter a number: 23 Enter another number: 34 The sum of 23 and 34 is 57 There is an alternative syntax - we can use the let keyword to indicate to the shell that we want to perform an arithmetic operation. This would look like this:\nlet sum=$number1 + $number2 I've included the let keyword here for completeness, but I would recommend that you use the double-parenthesis where possible as I think that it is probably the more commonly used construct.\nThere are many arithmetic operators available. Here's a table showing a few common ones and how they are used:\n   Operator Meaning Example     + Addition echo $((3+4)) # prints 7   - Subtraction echo $((4-2)) # prints 2   * Multiplication echo $((4*2)) # prints 8   / Division echo $((4/2)) # prints 2   ** Exponent echo $((4**3)) # prints 64   % Modulus echo $((7%3)) # prints 1   ++i Prefix Increment i=1; echo $((++i)) # prints 1, i is set to 2   i++ Postfix Increment i=1; echo $((i++)) # prints 2, i is set to 2   --i Prefix Decrement i=3; echo $((--i)) # prints 3, i is set to 2   i-- Postfix Decrement i=3; echo $((i--)) # prints 2, i is set to 2   i+=n Increment i=3; echo $((i+=3)) # prints 6, i is set to 6   i-=n Decrement i=3; echo $((i-=2)) # prints 1, i is set to 1    If you want to find the complete set of arithmetic operators available or find more details on how arithmetic works in the shell, use man bash and search for the text ARITHMETIC\\ EVALUATION (the backslash is needed to escape the space between the words when searching in the manual).\nThe script below shows how you can use a combination of operators to convert a value in degrees Celsius to Fahrenheit:\nread -p \u0026#34;Enter a value in Celsius: \u0026#34; celcius fahrenheit=$(( (celcius * 9/5) + 32 )) echo \u0026#34;${celcius}degrees Celsius is ${fahrenheit}degrees Fahrenheit\u0026#34; Note that you can use brackets in your arithmetic expressions to be explicit about the order in which the calculations should be performed. The order that is used if you don't use brackets is detailed in the manual page, but in general using brackets will make things clearer to the reader.\nUpdating the \u0026lsquo;Common\u0026rsquo; Command With our new understanding of variables, we can improve the \u0026lsquo;common\u0026rsquo; command we created in the previous chapter by extracting certain values into variables so that they can be more easily changed.\nLet's look at our original \u0026lsquo;common\u0026rsquo; command:\n# Write the title of our command. echo \u0026#34;common commands:\u0026#34; # Show the most commonly used commands. tail ~/.bash_history -n 1000 | sort | uniq -c | sed \u0026#39;s/^ *//\u0026#39; | sort -n | tail -n 10 We could improve on this by making the number of lines of text in the history we search through and the number of commands to show variables, so that they can be more easily changed.\nCreate a copy of the common.v1.sh script and call it common.v2.sh and update it like so:\n# Write the title of our command. echo \u0026#34;common commands:\u0026#34; # The following variables control how the command runs. history_lines=1000 # The number of lines of history to search through command_count=10 # The number of common commands to show. # Show the most commonly used commands. tail ~/.bash_history -n ${history_lines} \\  | sort \\  | uniq -c \\  | sed \u0026#39;s/^ *//\u0026#39; \\  | sort -n -r \\  | head -n ${command_count} We have replaced two \u0026lsquo;hard-coded\u0026rsquo; values (the number of lines of history to search and the number of common commands to show) with variables, which are now easier to find and change. We have also split the command into multiple lines so that it is easier to read (as the line is quite long otherwise).\nIf you want to replace the installed common command with this new one, update the symlink in your /usr/local/bin folder:\nln -sf $HOME/effective-shell/scripts/common.v2.sh /usr/local/bin/common Note that in this command we use the -f flag to force the creation of the symlink even if one already exists in the given location.\nSummary In this chapter we looked at how environment variables work and how we can use our own variables. We saw how to read input from the user and how to perform arithmetic operations.\nWe've seen a few new constructs in this chapter that will appear again and again, these are summarised below so that you can recognise them!\n ${variable} gets the value of variable - the braces surround the variable name $(echo \u0026quot;$PAGER\u0026quot;) runs the echo command in a subshell - the single parenthesis indicates we are running a subshell $(($left + $right)) adds the values in the variables left and right - the double parenthesis indicate that we are performing arithmetic  In the next chapter we are going to see how to perform logic in scripts - running commands only when certain conditions are met. This is an incredibly powerful technique and will let you create much more sophisticated scripts!\n  There is also a very good discussion on the differences in quoting options in the following Stack Overflow thread: https://stackoverflow.com/questions/10067266/when-to-wrap-quotes-around-a-shell-variable \u0026#x21a9;\u0026#xfe0e;\n   "});index.add({'id':20,'href':'/docs/part-4-shell-scripting/mastering-conditional-logic/','title':"Mastering Conditional Logic",'content':"Chapter 20 - Mastering Conditional Logic In this chapter we'll introduce the \u0026lsquo;conditional logic\u0026rsquo;, a set of powerful features that allow us to run operations only when certain conditions are met. We'll look at the if statement and the different ways we can evaluate conditions. We'll also look at more sophisticated conditional constructs such as the case statement and the select statement, and how to \u0026lsquo;chain\u0026rsquo; commands based on conditions.\nLet's get right into it!\nThe If Statement We can use the if statement to perform operations in shell scripts only when certain conditions are met.\nThe if statement has the following structure:\nif \u0026lt;test-commands\u0026gt; then \u0026lt;conditional-command 1\u0026gt; \u0026lt;conditional-command 2\u0026gt; \u0026lt;conditional-command n\u0026gt; fi The if statement will run the \u0026lsquo;test commands\u0026rsquo;. If the result of the commands are all zero (which means \u0026lsquo;success\u0026rsquo;), then each of the \u0026lsquo;conditional\u0026rsquo; commands will be run. We \u0026lsquo;close\u0026rsquo; the if statement with the fi keyword, which is if written backwards.\nLet's see how the if statement is used with a simple example. We will try and create a folder using mkdir. The mkdir command will return zero if the folder is created successfully:\nif mkdir ~/backups then echo \u0026#34;Successfully created the \u0026#39;backups\u0026#39; folder\u0026#34; fi If you don't have a folder called backups in your home directory then the command will run successfully. The mkdir command will return zero and the conditional statements will be run and you will see the output below:\nSuccessfully created the 'backups' folder If you then run the script again, the mkdir command will fail. In this case it does not return zero and the conditional commands are not executed. We will see an error message from the mkdir command:\nmkdir: /home/dwmkerr/backups: File exists This is the basics of how the if statement works. We provide test commands, if the test commands succeed, a set of conditional commands are then executed.\nYou might be surprised to hear that the result of the test commands has to be zero for the conditional commands to run. This is the opposite of how most programming languages work - normally zero would be considered \u0026lsquo;false\u0026rsquo;.\nThe reason for this - is that for computer programs that run, \u0026lsquo;zero\u0026rsquo; generally means success. Any non-zero value is typically used to indicate an error code. So whilst inside a programming language, an if statement will check for a value to be \u0026lsquo;true\u0026rsquo;, just remember that in the shell an if statement will check for a command to be successful.\nThe Test Command The test (evaluate expression) command is used to check whether a certain condition is true or not. If the condition is true then the test command returns zero to indicate success.\nWe could improve our earlier if statement example by only creating the \u0026lsquo;backups\u0026rsquo; folder if it doesn't already exist, using the test command:\nif ! test -d ~/backups then echo \u0026#34;Creating backups folder\u0026#34; mkdir ~/backups fi The test command evaluates an expression. In this case the expression is:\n-d ~/backups This expression uses the -d (file exists and is a directory) operator to check to see if the provided path is a directory. We to want to create the directory only if it doesn't exist, so we use the \u0026lsquo;not\u0026rsquo; operator to \u0026lsquo;invert\u0026rsquo; the result of test. The \u0026lsquo;not\u0026rsquo; operator is written with the ! exclamation point symbol.\nYou can surround an expression with square bracket and the shell will evaluate the expression with the test command. This can make your scripts far more compact:\nif ! [ -d ~/backups] then echo \u0026#34;Creating backups folder\u0026#34; mkdir ~/backups fi This square bracket syntax is very commonly used - but just remember it is shorthand for the test command.\nOne of the most useful manual pages is the page for the test command as it shows all of the available operators. Open the page with man test.\nUsing Multiple Statements in a Single Line You will often see \u0026lsquo;if\u0026rsquo; and \u0026lsquo;then\u0026rsquo; statements on the same line as below:\nif ! [ -d ~/backups ]; then mkdir ~/backups fi The shell assumes that each individual line is a single statement. If you want to put more than one statement on a line then you need to let the shell know when one statement ends and another starts. We can use a semi-colon for this. The shell uses the semi-colon as a \u0026lsquo;command separator\u0026rsquo; symbol.\nIf you don't include a semi-colon at the end of a command then the shell assumes that the entire line is a single statement. If you try and run the script without the semi-colon you will get an error:\nbash: syntax error near unexpected token `fi' I would suggest you start by writing your if statements with the if and the then on separate lines. Once you are more familiar with the syntax, you can start to combine the lines if you prefer.\nYou can put as many statements on a single line as you like - you could even write the script like so:\nif ! test -d ~/backups; then mkdir ~/backups; fi The then doesn't require a semi-colon as it is a keyword rather than a command. I think that in general keeping things on separate lines will be a bit more readable for other users, but sometimes you may prefer a more compact form.\nThe Else Statement You can use the else statement to define a series of statements that should be executed if the condition in the if statement is not true.\nHere's how we can write a script that informs the user of whether they have installed the \u0026lsquo;common\u0026rsquo; command or not:\nif [ -e /usr/local/bin/common ] then echo \u0026#34;The \u0026#39;common\u0026#39; command has been installed in the local bin folder.\u0026#34; else echo \u0026#34;The \u0026#39;common\u0026#39; command has not been installed in the local bin folder.\u0026#34; fi In this case we used the -e (file or folder exists) operator to check whether a file or folder exists in the location /usr/local/bin/common. The \u0026lsquo;common\u0026rsquo; command is the command we created in Chapter 18 - Shell Script Essentials.\nNow if you run the script and you don't have the \u0026lsquo;common\u0026rsquo; command installed you will see the following output:\nThe 'common' command has not been installed in the local bin folder. Note that we still need to use the \u0026lsquo;fi\u0026rsquo; keyword to close the \u0026lsquo;if\u0026rsquo; statement.\nThe Elif Statement The elif statement (which is short for \u0026lsquo;else if\u0026rsquo;) can be used to create additional checks and define statements that should run if other conditions are true.\nLet's see this in action by updating our script to check to see whether the \u0026lsquo;common\u0026rsquo; command is executable, using the -x (is executable) operator:\nif [ -x /usr/local/bin/common ]; then echo \u0026#34;The \u0026#39;common\u0026#39; command has been installed and is executable.\u0026#34; elif [ -e /usr/local/bin/common ]; then echo \u0026#34;The \u0026#39;common\u0026#39; command has been installed and is not executable.\u0026#34; else echo \u0026#34;The \u0026#39;common\u0026#39; command has not been installed.\u0026#34; fi The message you see will depend on whether you have installed the \u0026lsquo;common\u0026rsquo; command in your local binaries folder and whether the script is executable. If you want to see each of the different messages, you might find the following snippets useful to add or remove the command or change its executable permissions:\n ln -s $HOME/effective-shell/scripts/common.v1.sh /usr/local/bin/common - Create a link to the \u0026lsquo;common\u0026rsquo; command in the local binaries folder chmod -x $HOME/effective-shell/scripts/common.v1.sh remove the \u0026lsquo;executable\u0026rsquo; flag from the \u0026lsquo;common\u0026rsquo; command, making it not executable chmod +x $HOME/effective-shell/scripts/common.v1.sh add the \u0026lsquo;executable\u0026rsquo; flag from the \u0026lsquo;common\u0026rsquo; command, making it executable rm /usr/local/bin/common remove the link to the \u0026lsquo;common\u0026rsquo; command from the local binaries folder  The elif statement looks very similar to the if statement. The statement takes a set of commands. These commands could be normal shell commands, test commands, or test commands written with the square brackets short-hand notation.\nIt is very important to think about the order in which the if and elif statement are executed. If we had written the script like this, it would not work:\nif [ -e /usr/local/bin/common ]; then echo \u0026#34;The \u0026#39;common\u0026#39; command has been installed and is executable.\u0026#34; elif [ -x /usr/local/bin/common ]; then echo \u0026#34;The \u0026#39;common\u0026#39; command has been installed and is not executable.\u0026#34; else echo \u0026#34;The \u0026#39;common\u0026#39; command has not been installed.\u0026#34; fi In this script we check to see if the file exists first. If the file exists then the condition -e operator will return true, and we will not run the check in the elif statement. This means we'll never successfully evaluate the statements in the elif block (because for the file to be executable it must exist, so the first condition in the if statement will always take precedence. So it is important to think about the order of the statements!\nCommon Test Operators There are many operators that can be used in a test expression. You can find the full list by running man test.\nHere are the most common operators you should know about!\n   Operator Usage     -n True if the length of a string is non-zero.   -z True if the length of a string is zero.   var True if the variable var is set and is not empty.   s1 = s2 True if the strings s1 and s2 are identical.   s1 != s2 True if the strings s1 and s2 are not identical.   n1 -eq n2 True if the numbers n1 and n2 are equal.   n1 -ne n2 True if the numbers n1 and n2 are not equal.   n1 -lt n2 True if the number n1 is less than n2.   n1 -le n2 True if the number n1 is less than or equal to n2.   n1 -gt n2 True if the number n1 is greater than n2.   n1 -ge n2 True if the number n1 is greater than or equal to n2.    Common Test Operators for Files One of the great things about the test command is the presence of a number of operators that are specifically used to work with the filesystem. These operators are very handy when you are building shell scripts!\nHere are some of the most useful ones:\n   Operator Usage     -d True if the file exists and is a folder.   -e True if the file exists, regardless of the file type.   -f True if the file exists and is a regular file.   -L True if the file exists and is a symbolic link.   -r True if the file exists and is readable.   -s True if the file exists and has a size greater than zero.   -w True if the file exists is writable.   -x True if the file exists and is executable - if it is a directory this checks if it can be searched.   file1 -nt file2 True if file1 is exists and is newer than file2.   file1 -ot file2 True if file1 is exists and is older than file2.   file1 -ef file2 True if file1 and file2 exist and are the same file.    There are plenty of other operators that you can use when working with files, you can see them all by running man test.\nCombining Tests Often you will want to check multiple conditions. You can use the \u0026amp;\u0026amp; \u0026lsquo;and\u0026rsquo; operator and the || \u0026lsquo;or\u0026rsquo; operator to check for multiple conditions:\nif [ $year -ge 1980 ] \u0026amp;\u0026amp; [ $year -lt 1990 ]; then echo \u0026#34;$yearis in the 1980s\u0026#34; fi This script checks to see whether the variable \u0026lsquo;year\u0026rsquo; is greater than or equal to 1980 and less than 1990.\nYou can use \u0026lsquo;and\u0026rsquo; or \u0026lsquo;or\u0026rsquo; in a single test statement by using the special -a (and) and -o (or) operators. This is how the script would look using the -a operator:\nif [ $year -ge 1980 -a $year -lt 1990 ]; then echo \u0026#34;$yearis in the 1980s\u0026#34; fi These operators can lead to some subtle problems so I would not recommend that you use them. A better option is \u0026lsquo;Conditional Expressions\u0026rsquo; which are described in the next section. However, it is important to be able to recognise these operators so that they don't surprise you if you see them in someone else's script.\nConditional Expressions \u0026lsquo;Conditional Expressions\u0026rsquo; are a feature of Bash, and bash-like shells, that offer a more sophisticated option to perform conditional checks. Conditional expressions use two square brackets rather than one:\nif [[ $year -ge 1980 \u0026amp;\u0026amp; $year -lt 1990 ]]; then echo \u0026#34;$yearis in the 1980s\u0026#34; fi Conditional expressions have a number of benefits over plain test commands. Some of the most important ones are:\n You can use the \u0026amp;\u0026amp; and || operators directly in the expression If you use an || expression and the left hand side of the expression is true, the right hand side will not be evaluated - this is not always the case with older versions of Bash when using the -o operator (this is a subtle difference but can help avoid potentially incorrect behaviour) Numbers are correctly compared even if they are in different formats (for example, you can compare hexadecimal and octal numbers, this does not work in the standard test expression) You can use the incredibly useful =~ operator to use a regular expression in your condition (we'll look at this next)  You can find more details on conditional expressions by using man bash and searching for \\[\\[ (this is the double square brackets with each one escaped with a backslash).\nSome people prefer to use single brackets so that their script is more portable, as the double brackets are specific to Bash and Bash-like shells. Others prefer to use the double brackets so that they can use the additional featured offered.\nWhether you use single or double brackets will partly be down to preference and whether it is more important in your use case to have portability or whether it is more important to have the more \u0026lsquo;correct\u0026rsquo; behaviour.\nUsing Regexes in a Conditional Expression When you use the double square brackets conditional expression syntax you can use the =~ operator to test for a regular expression. This can be extremely useful. If you need a reminder on how regular expressions work check Chapter 13 - Regex Essentials.\nIn the example below we check to see if the user's shell is \u0026lsquo;zsh\u0026rsquo; by seeing whether the path of the shell ends with the text zsh:\nzsh_regex=\u0026#34;zsh$\u0026#34; if [[ $SHELL =~ $zsh_regex ]]; then echo \u0026#34;It looks like your shell \u0026#39;$SHELL\u0026#39; is Z-Shell\u0026#34; fi If you are running Z-Shell you will see the output below:\nIt looks like your shell '/bin/zsh' is Z-Shell It is best to declare the regular expression in a variable rather than including it directly in the expression, this makes it easier to handle special characters such as the dollar symbol.\nYou can use capture groups in your regular expression to help you extract text. For example, we could get the name of the current shell binary with the code below:\nshell_regex=\u0026#34;([^/]+)$\u0026#34; if [[ $SHELL =~ $shell_regex ]]; then echo \u0026#34;Your shell binary is: ${BASH_REMATCH[1]}\u0026#34; else echo \u0026#34;Unable to extract your shell binary\u0026#34; fi On my machine this script shows the following output:\nYour shell binary is: bash The $BASH_REMATCH variable is an array - the first result value in the array is the entire match, each subsequent value in the array is the result of each capture group in the expression. Double check Chapter 19 - Variables, Reading Input, and Mathematics if you need a reminder on how arrays work in Bash.\nChaining Commands You can \u0026lsquo;chain\u0026rsquo; commands together in the shell, this allows you to run a command based on the result of a previous command.\nLet's take a look at how this would work:\nmkdir -p ~/backups \u0026amp;\u0026amp; cd ~/backups In this case we have chained two commands together using the \u0026amp;\u0026amp; operator. The shell will only run the second command if the first command succeeds. It evaluates the result of the first command - if it is successful, then it evaluates the second command. It does this because we are trying to evaluate the combination of both commands. Or, if we were to write this in pseudo-code:\ndoes (command1 and command2) succeed? If command fails, the shell doesn't need to evaluate the second command - because we know the overall result must be false, because one of the commands has already failed.\nContrast this to the || operator:\n[ -d ~/backups ] || mkdir ~/backups In this case we evaluate the second command only if the first command fails. Let's look at the pseudo code:\ndoes (command1 or command2) succeed? If the first command succeeds, the shell doesn't need to evaluate the second command, so it is doesn't. However, if the first command fails, the shell does have to evaluate the second command, to see if either of them succeed.\nIn summary, here's how command chaining works:\n# Run command1, if it succeeds run command2. command1 \u0026amp;\u0026amp; command2 # Run command1, if it does not succeed run command2. command1 || command2 You will see this syntax a lot in shell scrips as it is very succinct. It can also be very useful when using the shell interactively. For example, it is almost second nature for me to write the following commands:\nmake build \u0026amp;\u0026amp; make deploy Here I am using the make (build programs) command. If the \u0026lsquo;build\u0026rsquo; step for a project fails, I want to run the \u0026lsquo;deploy\u0026rsquo; step. But I don't want to run the \u0026lsquo;deploy\u0026rsquo; step if the \u0026lsquo;build\u0026rsquo; step fails!\nCase Statements If you find yourself writing overly complex \u0026lsquo;if statements\u0026rsquo;, you might use a _case statement_ to simplify your code.\nA case statement is a bit like an \u0026lsquo;if statement\u0026rsquo;. The structure is as follows:\ncase \u0026lt;expression\u0026gt; in pattern1) \u0026lt;pattern1-commands\u0026gt; ;; pattern2 | pattern3) \u0026lt;pattern2and3-commands\u0026gt; ;; *) \u0026lt;default-commands\u0026gt; ;; esac Typically you will provide the \u0026lsquo;case\u0026rsquo; statement a variable and use it to check against a number of values. Here's a common example you'll see - checking to see whether a response is \u0026lsquo;yes\u0026rsquo; or \u0026lsquo;no\u0026rsquo;:\nread -p \u0026#34;Yes or no: \u0026#34; response case \u0026#34;${response}\u0026#34; in y | Y | yes | ok) echo \u0026#34;You have confirmed\u0026#34; ;; n | N | no) echo \u0026#34;You have denied\u0026#34; ;; *) echo \u0026#34;\u0026#39;${response}\u0026#39; is not a valid response\u0026#34; ;; esac The example above shows very simple text patterns, but any text pattern can be used:\nread -p \u0026#34;Yes or no: \u0026#34; response case \u0026#34;${response}\u0026#34; in [yY]*) echo \u0026#34;You have (probably) confirmed\u0026#34; ;; [nN]*) echo \u0026#34;You have (probably) denied\u0026#34; ;; *) echo \u0026#34;\u0026#39;${response}\u0026#39; is not a valid response\u0026#34; ;; esac In this example the first pattern is [yY]* which means either the \u0026lsquo;y\u0026rsquo; or \u0026lsquo;Y\u0026rsquo; character followed by zero or more characters, this will match things like \u0026lsquo;yes\u0026rsquo; \u0026lsquo;YES\u0026rsquo; or \u0026lsquo;yay\u0026rsquo;. We have a similar pattern for the negative response.\nThe case statement can look quite complex, I often think that even if it takes more lines to write the logic using \u0026lsquo;if statements\u0026rsquo; it will be more readable, but this is common pattern nonetheless and good to know about!\nUpdating the \u0026lsquo;Common\u0026rsquo; Command Now that we know how to use if statements, we can update the \u0026lsquo;common\u0026rsquo; command that we have been improving as part of each chapter.\nWe will update it to check to see whether the user is using Bash or Z-Shell and search through the history for for common commands appropriately.\nAs a reference, let's look at the common.v2.sh command we created in the previous chapter:\n# Write the title of our command. echo \u0026#34;common commands:\u0026#34; # The following variables control how the command runs. history_lines=1000 # The number of lines of history to search through command_count=10 # The number of common commands to show # Show the most commonly used commands. tail ~/.bash_history -n ${history_lines} \\  | sort \\  | uniq -c \\  | sed \u0026#39;s/^ *//\u0026#39; \\  | sort -n \\  | tail -n ${command_count} We'll create a new version of this script called common.v3.sh that checks the user's shell to work out what file to use to find the history of commands:\n# The following variables control how the command runs. shell_binary=\u0026#34;\u0026#34; # We will work out what shell we are in later. history_file=\u0026#34;\u0026#34; # We will work out the history file later. history_lines=1000 # The number of lines of history to search through command_count=10 # The number of common commands to show # Check to see if we can work out the name of the shell binary. shell_regex=\u0026#34;([^/]+$)\u0026#34; if [[ $SHELL =~ $shell_regex ]]; then # Depending on the name of the shell binary, set the history file path. shell_binary=${BASH_REMATCH[1]} if [[ $shell_binary == \u0026#34;bash\u0026#34; ]]; then history_file=~/.bash_history elif [[ $shell_binary == \u0026#34;zsh\u0026#34; ]]; then history_file=~/.bash_history fi fi # If we are searching through the bash history, we can look at the history file # to get the most common commands. if [[ $shell_binary == \u0026#34;bash\u0026#34; ]]; then # Show the most commonly used commands. tail \u0026#34;${history_file}\u0026#34; -n ${history_lines} \\  | sort \\  | uniq -c \\  | sed \u0026#39;s/^ *//\u0026#39; \\  | sort -n -r \\  | head -n ${command_count} elif [[ $shell_binary == \u0026#34;zsh\u0026#34; ]]; then # Z-Shell history lines look like this: # : 1621135004:0;uname -a # So we run the same command as above, but use the \u0026#39;rev | cut | rev\u0026#39; trick # to extract everything _after_ the semi-colon, which is the command text. tail \u0026#34;${history_file}\u0026#34; -n ${history_lines} \\  | rev \\  | cut -d\u0026#39;;\u0026#39; -f1 \\  | rev \\  | sort \\  | uniq -c \\  | sed \u0026#39;s/^ *//\u0026#39; \\  | sort -n -r \\  | head -n ${command_count} else # Show a warning to the user that we don\u0026#39;t know where the history file is # for their shell. echo \u0026#34;Sorry, I don\u0026#39;t know where to find the history for \u0026#39;${SHELL}\u0026#39;\u0026#34; fi In this script we now first check to see if we can extract the name of the shell binary from the shell path. If we can, we store the name of the shell binary and its associated history in a pair of variables.\nThen when we come to actually search through the history, we check the shell binary. If it is bash, we run the same command as before. If it is zsh we run a similar command, but account for the fact that the Z-Shell history file has some extra content which needs to be removed.\nNote that as well as showing how to use more variables and if statements, as well as nested if statements (when one if statement is inside another) we can also see that we have very descriptive comments. Each comment is giving clear information on what we are trying to accomplish, which should make the script easier to maintain.\nIf you want to replace the installed common command with this new one, update the symlink in your /usr/local/bin folder:\nln -sf $HOME/effective-shell/scripts/common.v3.sh /usr/local/bin/common Note that in this command we use the -f flag to force the creation of the symlink even if one already exists in the given location.\nSummary In this chapter we looked at the If statement - an extremely important statement that allows us to perform conditional logic. In the next chapter we will look at another crucial logical feature of the shell - loops.\nYou can find most of the documentation for conditional logic in the manual, just run man bash and search for GRAMMAR.\n"});index.add({'id':21,'href':'/docs/part-4-shell-scripting/loops-and-working-with-files-and-folders/','title':"Loops and working with Files and Folders",'content':"Chapter 21 - Loops and working with Files and Folders Loops allow us to perform a set of operations over multiple items, such as a set of files or folders or the results of a command. In this chapter we'll look at loops and how to operate on many files and folders.\nThe For Loop We can use the for loop to run a set of commands for each item in a list.\nThe for loop has the following structure:\nfor \u0026lt;name\u0026gt; in \u0026lt;words\u0026gt; do \u0026lt;conditional-command 1\u0026gt; \u0026lt;conditional-command 2\u0026gt; \u0026lt;conditional-command n\u0026gt; done The for loop executes a sequence of commands for every item in a list. In the documentation you will see that this list is called \u0026lsquo;words\u0026rsquo;. There's a technical (and complex) reason for this that we'll discuss in the end of the chapter.\nLet's see how the for loop works by showing a simple example. We will loop through every item in a folder and print its name to the screen:\nfor item in ~/effective-shell/* do echo \u0026#34;Found: $item\u0026#34; done As long as you have the effective-shell folder in your home directory, you will see output that looks like this:\nFound: /home/dwmkerr/effective-shell/data Found: /home/dwmkerr/effective-shell/docs Found: /home/dwmkerr/effective-shell/logs Found: /home/dwmkerr/effective-shell/pictures Found: /home/dwmkerr/effective-shell/programs Found: /home/dwmkerr/effective-shell/quotes Found: /home/dwmkerr/effective-shell/scripts Found: /home/dwmkerr/effective-shell/templates Found: /home/dwmkerr/effective-shell/text Found: /home/dwmkerr/effective-shell/websites Notice how the shell is smart enough to expand the wildcard expression that we have included in the for loop. In just the same way we can use wildcards in commands such as ls or cp or mv, we can also use them in for loops - or in fact any statement1!\nYou will also see that when we specify the name of the variable to use in the loop (which in this example was item) we don't need to use a dollar symbol. Remember - when we are setting a variable, we don't use a dollar symbol, we only use the dollar symbol when we want to get the value of the variable.\nThe for loop is closed with the done keyword. Here we can also see an inconsistency with the shell syntax - for the if statement, the statement is closed with if backwards (fi). But the for loop is closed with done. The shell is an old platform and there are some oddities like that that you might not see in more modern programming languages.\nFor Loops - Arrays In Chapter 19 - Variables, Reading Input, and Mathematics we saw how to create arrays. We can easily loop through the items in an array with a for loop. Here's an example:\ndays=(\u0026#34;Monday\u0026#34; \u0026#34;Tuesday\u0026#34; \u0026#34;Wednesday\u0026#34; \u0026#34;Thursday\u0026#34; \u0026#34;Friday\u0026#34; \u0026#34;Saturday\u0026#34; \u0026#34;Sunday\u0026#34;) for day in ${days[@]} do echo -n \u0026#34;$day, \u0026#34; done echo \u0026#34;happy days!\u0026#34; If we run this script we'll see the following output:\nMonday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday, happy days! It's important to remember that we want to go through every item in the array, so we have to use the ${days[@]} syntax. This is the syntax that means \u0026lsquo;all of the members of the array\u0026rsquo;.\nThe -n (don't output a trailing newline) flag of the echo command is used inside the for loop so that we don't write each day on its own line.\nFor Loops - Words The for loop documentation names the input to the loop as \u0026lsquo;words\u0026rsquo;. We can see this by running help for:\n$ help for for: for NAME [in WORDS ... ] ; do COMMANDS; done Execute commands for each member in a list. ... The reason that the items are called \u0026lsquo;words\u0026rsquo; is that the shell splits up the input into a set of words and loops though each - this can be a real surprise if you come from a programming background.\nLet's see what this means with an example:\nsentence=\u0026#34;What can the harvest hope for, if not for the care of the Reaper Man?\u0026#34; for word in $sentence do echo \u0026#34;$word\u0026#34; done The output of this will be:\nWhat can the harvest hope for, if not for the care of the reaper man? Z-Shell - if you are using Z-Shell then the sentence will not be split up into words. There is an appendix at the end of the chapter that describes the differences between Z-Shell and Bash-like shells (which tend to be closer to the Posix standard).\nThe for loop has split up the sentence variable into a set of words. This might seem illogical, as the shell is making quite a big assumption (that the operator wants their input split up), but we'll see with a few examples how this is often what is needed.\nThis is not how most programming languages would work, so why does the shell do this?\nThe reason is that the shell is a text based environment and the designers have taken this into account. Most of the time when we are running shell commands in a terminal we are running commands that simply output text. If we want to be able to use the output of these commands in constructs like loops, the shell has to decide how to split the output up.\nFor example, let's see how the ls command would write its output:\n$ ls ~/effective-shell data docs logs pictures programs quotes scripts templates text websites The output that the ls program has written is plain text. It is not an array, it is just a set of files separated by spaces. What would we expect the shell to do if we ran the following command?\nfiles=$(ls ~/effective-shell) for file in $files do echo \u0026#34;Found: $file\u0026#34; done The output is:\nFound: data Found: docs Found: logs Found: pictures Found: programs Found: quotes Found: scripts Found: templates Found: text Found: websites Here we see why the shell splits up words in a sentence. It is making a best effort with plain text - trying to split plain text up into sensible \u0026lsquo;chunks\u0026rsquo;.\nWhen we operate in a shell for day to day work we don't have to use the more specific syntax that would be used in a programming language - the shell has more of an emphasis on terseness of statements and the ability to quickly work with files. It is not designed as a general purpose programming tool, so it makes assumptions like this.\nWe go into detail in word splitting nearer the end of this chapter.\nFor Loops - Files with Wildcards One of the most common scenarios for using a for loop is to loop through a set of files or folders.\nThe most simple way to do this is to use a simple wildcard pattern in the for loop statement, like so:\nfor script in ~/effective-shell/scripts/*.sh do echo \u0026#34;Found script: $script\u0026#34; done We will see output that looks like this:\nFound script: /home/dwmkerr/effective-shell/scripts/common.mac.sh Found script: /home/dwmkerr/effective-shell/scripts/common.sh Found script: /home/dwmkerr/effective-shell/scripts/common.v1.sh Found script: /home/dwmkerr/effective-shell/scripts/common.v2.sh Found script: /home/dwmkerr/effective-shell/scripts/common.v3.sh Found script: /home/dwmkerr/effective-shell/scripts/show-info.sh We have to be careful with scripts like this - there is a bug.\nBy default, if the shell doesn't find anything with a wildcard pattern it does not expand it. This is very confusing - so let's see an example.\nTake a look at the sample below - what would you expect it to show?\nfor script in ~/bad-shell/scripts/*.sh do echo \u0026#34;Found: $script\u0026#34; done You might think the logical result is that nothing is printed - there is not bad shell folder, so the pattern should not find any files. But instead, we see the following output:\nFound: ~/bad-shell/scripts/*.sh By default, if a shell \u0026lsquo;glob\u0026rsquo; (a pattern that includes a wildcard) does not match any files, the shell simply leaves the pattern as-is.\nThere are two ways we can deal with this problem. The first way is to enable the \u0026lsquo;nullglob\u0026rsquo; (return null for unmatched globs) option:\nshopt -s nullglob for script in ~/bad-shell/scripts/*.sh do echo \u0026#34;Found: $script\u0026#34; done The shopt (set and unset shell option) command is used to configure shell options. We will be looking at shell options in detail in Part 5. The \u0026lsquo;nullglob\u0026rsquo; option changes the shell behaviour so that if a wildcard pattern does not match any results, it is set to null string1.\nThe second way we can deal with this problem is to just use a test command. I think that this is actually far more readable than the shopt solution. Here's how it would look:\nfor script in ~/bad-shell/scripts/*.sh do # If the file / folder doesn\u0026#39;t exist, skip it. if ! [ -e \u0026#34;$script\u0026#34; ]; then continue; fi echo \u0026#34;Found: $script\u0026#34; done Here we use the -e (exists) operator in a test command to check whether the file exists. If it does not exist, we run the continue statement.\nThe continue statement \u0026lsquo;skips\u0026rsquo; the current item in the loop and moves to the next one. We will see it a little more later on.\nFor Loops - Files with Find If the files that you are trying to loop through are too complex to match with a shell pattern, you can use the find command to search for files, then loop through the results.\nIf you are not familiar with the find command, check Chapter 11 - Finding Files.\nLet's use the find command to run a loop that prints every symlink in the user's home directory. But before we run the loop we'll create a symlink with a space - this will cause some interesting output in our script:\n# Create a symlink to \u0026#39;effective-shell\u0026#39; that has a space in it... ln -s ~/effective shell ~/effective\\ shell # Find all symlinks and print each one. links=$(find ~ -type l) for link in $links do echo \u0026#34;Found Link: $link\u0026#34; done You will see a few different links shown when you run this script, depending on how your system is set up. But you will also certainly see the results below:\n... Found Link: /home/dwmkerr/effective-shell/effective Found Link: shell ... This is clearly a problem - the shell has taken the path that has a space - /home/dwmkerr/effective-shell/effective shell and performed word splitting and turned it into two separate items.\nThis is a persistent headache for anyone who needs to build shell scripts. There are a large number of ways to solve this problem, and none of them are particularly intuitive. I am going to demonstrate one common solution, which is not perfect but should cover most cases. I'll then suggest a better work-around.\nThe solution that we will use is to temporarily change the values that the shell uses to split text into words. We will set it to only split on newlines. The find command puts each file it finds on its own line. This means we will not split up files with spaces or other whitespace in the name:\n# Save the current value of IFS - so we can restore it later. Split on newlines. old_ifs=$IFS IFS=$\u0026#39;\\n\u0026#39; # Find all symlinks and print each one. links=$(find ~ -type l) for link in $links do echo \u0026#34;Found Link: $link\u0026#34; done # Restore the original value of IFS. IFS=$old_ifs If you run this command now you will see the correct output:\n... Found Link: /home/dwmkerr/effective-shell/effective shell ... This will cover you in most cases. However, this method is not ideal for a number of reasons:\n It is quite verbose - we have to store the current value of $IFS and then reset it later It is not quite foolproof - filenames on some systems can have a newline character and this script would fail for those files We have to use the complex looking \u0026lsquo;ANSI C Quoting\u0026rsquo; syntax to set $IFS to a newline2 If the reader doesn't know what $IFS is then the entire script will be difficult to follow  The $IFS variable can be complex to work with and discussed at the end of the chapter.\nI believe that in this case it is probably best to not use a shell script. There is no solution that is particularly clean or simple. In this case I think you might be better off using a programming language. Check the How to avoid scripting! Chapter for more details on this.\nFor Loops - C Style Loops If you have used programming languages like C, C++, Python, Java and others, you may well be familiar with the \u0026lsquo;C style loop\u0026rsquo; structure that is shown below:\nfor (( expression1 ; expression2 ; expression3 )) do \u0026lt;command 1\u0026gt; \u0026lt;command 2\u0026gt; \u0026lt;command n\u0026gt; done This loop structure uses three arithmetic expressions to run the loop. The first is in \u0026lsquo;initialise\u0026rsquo; expression, this is typically used to setup the initial state of the loop. The second is the \u0026lsquo;conditional\u0026rsquo; expression, this is used to check whether the loop is complex. The third is the \u0026lsquo;iterate\u0026rsquo; expression, this is evaluated after the loop commands are completed.\nHere's how we can use a C style for loop to iterate through five numbers:\nfor (( i = 1; i \u0026lt;= 5; i++ )) do echo \u0026#34;Loop ${i}\u0026#34; done The output of this script is:\nLoop 1 Loop 2 Loop 3 Loop 4 Loop 5 For Loops - Looping over Sequences Another common way to use a for loop is with brace expansion. Brace expansion we have already seen a number of times so far - we can use it to generate a sequence of values. Here is how we might create three files using brace expansion:\ntouch {coffee,tea,milkshake}-menu.txt This will create three files:\n$ ls -1 *-menu.txt coffee-menu.txt milkshake-menu.txt tea-menu.txt Brace expansion can be use in for loops, and brace expansion can be used to create sequences. For example, the loop below could be used as a way to loop through the numbers from one to ten:\nfor i in {1..10} do echo \u0026#34;Loop ${i}\u0026#34; done Brace expansion can be used to loop through a sequence of values or a range of numbers. You can even specify the \u0026lsquo;increment\u0026rsquo; used in a sequence. For example, this loop iterates through a sequence of numbers adding five each time:\nfor i in {0..25..5} do echo \u0026#34;Loop ${i}\u0026#34; done The output of this loop would be:\nLoop 0 Loop 5 Loop 10 Loop 15 Loop 20 Loop 25 The While Loop The while loop is a loop that executes commands until a certain condition is met.\nThe while loop has the following structure:\nwhile \u0026lt;test-commands\u0026gt; do \u0026lt;conditional-command 1\u0026gt; \u0026lt;conditional-command 2\u0026gt; \u0026lt;conditional-command n\u0026gt; done As long as the test commands return success, the loop will run the conditional commands. After the conditional commands have been run, the loop goes \u0026lsquo;back to the start\u0026rsquo; and evaluates the test commands again.\nHere's an example of how a while loop can be used to generate a list of random numbers:\n# Create an empty array of random numbers. random_numbers=() # As long as the length of the array is less than five, continue to loop. while [ ${#random_numbers[@]} -lt 5 ] do # Get a random number, ask the user if they want to add it to the array. random_number=$RANDOM read -p \u0026#34;Add $random_numberto the list? (y/n): \u0026#34; choice # If the user chose \u0026#39;y\u0026#39; add the random number to the array. if [ \u0026#34;$choice\u0026#34; = \u0026#34;y\u0026#34; ]; then random_numbers+=($random_number); fi done # Show the contents of the array. echo \u0026#34;Random Numbers: ${random_numbers[@]}\u0026#34; When you run this script, you can choose to add a number to the list by typing \u0026lsquo;y\u0026rsquo; - once there are five items in the list the while loop condition fails and the loop ends:\nAdd 14718 to the list? (y/n): y Add 2646 to the list? (y/n): n Add 11898 to the list? (y/n): y Add 31506 to the list? (y/n): y Add 32436 to the list? (y/n): y Add 6803 to the list? (y/n): n Add 25811 to the list? (y/n): y Random Numbers: 14718 11898 31506 32436 25811 The $RANDOM variable is a built-in variable in the shell that returns a random number.\nYou would typically use a while loop when you don't know how many iterations you will perform and you need to re-evaluate at each iteration whether you should continue to loop.\nWhile Loops - Looping through the lines in a file You can use a while loop to iterate through each line in a file, without having to load the entire file into memory.\nHere's an example of how to iterate through the lines of a file:\nwhile read line; do echo \u0026#34;Read: $line\u0026#34; done \u0026lt; ~/effective-shell/data/top100.csv The output will look like this:\nRead: \u0026quot;Rank\u0026quot;,\u0026quot;Rating\u0026quot;,\u0026quot;Title\u0026quot;,\u0026quot;Reviews\u0026quot; Read: \u0026quot;1\u0026quot;,\u0026quot;97\u0026quot;,\u0026quot;Black Panther (2018)\u0026quot;,\u0026quot;515\u0026quot; Read: \u0026quot;2\u0026quot;,\u0026quot;94\u0026quot;,\u0026quot;Avengers: Endgame (2019)\u0026quot;,\u0026quot;531\u0026quot; ... This uses shell redirection to redirect the contents of the ~/effective-shell/data/top100.csv file into the read command in the while loop. The read command will read the file, line by line, until it finds the final line.\nThis script has some issues:\n If the last line is does not end with a newline, then it is not read Backlashes will be treated as escape sequences and lead to broken output Leading whitespace will be removed  It is possible to avoid these issues, but the resulting script is a lot harder to read:\nwhile IFS=\u0026#34;\u0026#34; read -r line || [ -n \u0026#34;$line\u0026#34; ]; do echo \u0026#34;Read: $line\u0026#34; done \u0026lt; ~/effective-shell/data/top100.csv In this case we've had to use some complex tricks to avoid each issue:\n The || [ -n \u0026quot;$line\u0026quot;] test ensures that the loop iterates as long as the line read is not zero-length, ensuring we read the last line even if it doesn't have a newline The -r (do not escape) option for read ensures that backlashes are not interpreted as escape sequences The IFS=\u0026quot;\u0026quot; temporarily disables any word splitting in the loop, meaning that we do not lose leading whitespace  However this still has issues - if commands in the loop read from standard input then the loop will still have errors. For this reason, I would again suggest you follow the advice in the How to avoid scripting! Chapter to see better ways to read files!\nEven though I would recommend using a programming language to read the lines of a file, I have kept this example here because it is something you are likely to come across if you see scripts written by others. And for simple scenarios, where you are fairly sure of structure of a file, it might be useful. But this is definitely a case where you should consider using a programming language if you want to create more maintainable solutions to problems!\nWhile Loops - The Infinite Loop There are times that you may want to loop forever. For example you might be writing a script that reads an option from the user, processes it, and then starts again.\nHere's an example of an infinite loop - we use the true command, which always returns success:\nwhile true do echo \u0026#34;1) Move forwards\u0026#34; echo \u0026#34;2) Move backwards\u0026#34; echo \u0026#34;3) Turn Left\u0026#34; echo \u0026#34;4) Turn Right\u0026#34; echo \u0026#34;5) Explore\u0026#34; echo \u0026#34;0) Quit\u0026#34; read -p \u0026#34;What will you do: \u0026#34; choice if [ $choice -eq 0 ]; then exit fi # The rest of the game logic would go here! # ... done This example shows a common pattern for an infinite loop - offering a menu of options which the user can call repeatedly until they decide to quit.\nThe Until Loop The until loop operates just like the while loop, except that it runs until the test commands return success.\nThe structure of the until loop is just like the while loop:\nuntil \u0026lt;test-commands\u0026gt; do \u0026lt;conditional-command 1\u0026gt; \u0026lt;conditional-command 2\u0026gt; \u0026lt;conditional-command n\u0026gt; done As long as the test commands do not return success, the loop will run the conditional commands. After the conditional commands have been run, the loop goes \u0026lsquo;back to the start\u0026rsquo; and evaluates the test commands again.\nHere's an example of an until loop that builds a random number that is at least 15 characters long:\n# Create an empty random number string - we\u0026#39;re going to build it up in the loop. random_number=\u0026#34;\u0026#34; # Keep on looping until the random number is at least 15 characters long. until [ \u0026#34;${#random_number}\u0026#34; -ge 15 ] do random_number+=$RANDOM done echo \u0026#34;Random Number: ${random_number}\u0026#34; When you run this script you will see something like this:\nRandom Number: 364272371462227929 Note that we've used the string-length parameter expansion function to get the length of the random_number_ variable here. If this is not familiar, check Chapter 19 - Variables, Reading Input, and Mathematics.\nIn general I would recommend using while loops rather than until loops. While loops are going to be more familiar to readers as they exist in many programming languages - until loops are a little more rare. And you can easily turn any until loop into a while loop by simply inverting the test commands you are running.\nFor example, we could re-write the loop created before like so:\nrandom_number=\u0026#34;\u0026#34; while [ \u0026#34;${#random_number}\u0026#34; -lt 15 ] do random_number+=$RANDOM done echo \u0026#34;Random Number: ${random_number}\u0026#34; In this case we've changed the condition from -ge 15 (greater than or equal to fifteen) to -lt 15 (less than fifteen). The while loop version of the script will probably be a little easier for most readers to parse.\nContinue and Break We briefly saw that the continue (resume loop) statement can be used to \u0026lsquo;skip\u0026rsquo; an iteration in a loop. break (exit loop) statement that can be used to stop running the loop.\nWhen we use the continue statement, we are telling the shell that we want to stop processing the current \u0026lsquo;iteration\u0026rsquo; of the loop and move onto the next item. You can use as many continue statements as you like in a loop.\nHere's an example of a script that let's the users show the contents of a directory. If the directory is empty it uses the continue statement to skip to the next directory. If the users chooses to cancel the operation, it uses the break statement to stop iterating:\necho \u0026#34;For each folder, choose y/n to show contents, or c to cancel.\u0026#34; for file in ~/* do # If the file is not a directory, or it cannot be searched, skip it. if ! [ -d \u0026#34;$file\u0026#34; ] || ! [ -x \u0026#34;$file\u0026#34; ]; then continue; fi # Ask the user if they want to see the contents. read -p \u0026#34;Show: $file? [y/n/c]: \u0026#34; choice # If the user chose \u0026#39;c\u0026#39; for cancel, break. if [ \u0026#34;$choice\u0026#34; = \u0026#34;c\u0026#34; ]; then break; fi # If the user choice \u0026#39;y\u0026#39; to show contents, list them. if [ \u0026#34;$choice\u0026#34; = \u0026#34;y\u0026#34; ]; then ls \u0026#34;$file\u0026#34;; fi done Using break and continue can simplify our loops - if we it would be much harder to write the loop above.\nCreating Compact Loops In each example in this chapter we have split the loop constructs so that there is one statement per line. But just as with the if statement, we can combine any of these lines, as long as we use a semi-colon to let the shell know where each statement ends.\nA common pattern you will see is the do keyword on the same line as the for or while statement:\nnumbers=(0 1 1 2 3 5) for num in ${numbers[@]}; do echo \u0026#34;$num\u0026#34; done If you are simply typing in the shell in a terminal, rather than writing a script, you might write the loop on a single line:\nfor script in *.sh; do touch \u0026#34;$script\u0026#34;; done This one-liner updates the last access and modified of all files that end with *.sh in the current folder.\nJust like with the if statement I would recommend that you keep each statement on its own line until you are 100% familiar with the syntax. Then when it is second-nature to be able to write a loop, you can use the more compact syntax if it is appropriate.\nWhen you are running the shell interactively, i.e. actually typing in the shell rather than writing a shell script, you can still use multiple lines. If you type for script in *.sh and press enter, the shell will let you type the next line. You can keep on adding lines until you type done and press enter.\nIf you want to make a really compact for loop, you can actually skip the in \u0026lt;words\u0026gt; part. If in \u0026lt;words\u0026gt; is omitted then the special \u0026lsquo;all parameters\u0026rsquo; variable $@ is used. We will look at this special parameter in the next chapter. But this will be confusing to readers so I would recommend that you are always explicit with the in \u0026lt;words part of a for loop.\nWord Splitting and IFS At a number of points in this chapter we have touched on the concept of \u0026lsquo;word-splitting' and the $IFS variable. Before we close out the chapter with an update to the common script, let's talk about these concepts in more detail.\nIf you are not expecting to use shell scripts as a regular part of your work you can safely skip this section. If you think that you are likely to come across shell scripts, loops and similar constructs, it might be worth reading this section.\nWord Splitting Word splitting is the process by which the shell splits text up into a set of words.\nWe saw that the shell will split the words in a loop, which we can see with the example below:\n$ sentence=\u0026quot;Here are some words\u0026quot; for word in $sentence; do echo \u0026quot;$word\u0026quot;; done Here are some words But why is it that wrapping the $sentence variable in quotes stops the word splitting from happening?\n$ sentence=\u0026quot;Here are some words\u0026quot; for word in \u0026quot;$sentence\u0026quot;; do echo \u0026quot;$word\u0026quot;; done Here are some words The reason for this has been touched on in Chapter 19 - Variables, Reading Input, and Mathematics and also partly in this chapter.\nIn the first example the loop iterates over the $sentence variable. Note that this variable is not quoted. This means that it follows the standard rules for \u0026lsquo;expansion\u0026rsquo; in the shell. This means that as well as all of the usual features such as wildcard expansion, word expansion will occur.\nIn the second example, the loop iterates over the \u0026quot;$sentence\u0026quot; variable. Note that this variable is quoted. As we saw in Chapter 19 quoting a variable means that it is treated literally, expect for parameter expansion.\nThis means that in most circumstances you probably want to quote your variables - otherwise the shell is going to perform word splitting on them. But if you do want expansion and splitting to occur, then you should not quote text. For example, if we run the following we see invalid output:\n$ for file in \u0026quot;*\u0026quot;; do echo \u0026quot;Found: $file\u0026quot;; done Found: * Because we have quoted the asterisks, the shell does not treat it as a special character and expand it into a list of files.\nWhat about this example, when we use wildcard expansion to list files, but the results do not have word splitting applied?\n$ touch file\\ with\\ spaces.test $ for file in *.test; do echo \u0026quot;Found: $file\u0026quot;; done Found: file with spaces.test The *.test is not surrounded in quotes, so it is expanded. But why does word splitting not happen?\nThe reason is that the shell applies these \u0026lsquo;expansions\u0026rsquo; in a certain order, which is as follows:\n Brace expansion Tilde expansion Parameter and variable expansion Command substitution Arithmetic expansion Word splitting Pathname expansion  Word splitting happens before pathname expansion, and it is pathname expansion that turns the asterisks wildcard into the list of files. At the point that this happens, word splitting has already been applied and won't be applied again.\nEach of these types of expansion we have actually already seen in the book, but we're going to review them in detail in the final section on advanced techniques. You can find the appropriate section of the Bash manual for this topic by searching for EXPANSION.\nThe IFS Variable The $IFS variable is the \u0026lsquo;internal field separator\u0026rsquo; variable. It is what the shell uses to decide what characters should be used to split up text into words. By default, this variable includes the space character, the tab character and the newline character.\nWhenever you see a script or a command that changes the value of the $IFS variable, the operator is modifying the behaviour of subsequent commands so that they do not split words in the same way.\nHere's an example of how we could change the IFS variable to split text using commas:\ntext=\u0026#34;mother,danzig,1988\u0026#34; IFS=\u0026#34;,\u0026#34; for word in $text do echo \u0026#34;Word: $word\u0026#34; done This script will split text using the comma symbol and output:\nWord: mother Word: danzig Word: 1988 Be careful when changing the IFS variable - it could cause subsequent commands to behave in unexpected ways. You should normally first copy current value into a variable, then change it, then set it back, like so:\nold_ifs=\u0026#34;$IFS\u0026#34; IFS=\u0026#34;:\u0026#34; # Do some stuff IFS=\u0026#34;$IFS\u0026#34; In general if you are changing IFS you might be doing something that would be better done with a programming language.\nUpdating the \u0026lsquo;common\u0026rsquo; Command In the previous chapter we created the common.v3.sh command, that shows common commands from the users shell history.\nIf you need a refresher on what is in the script, you can view it in your pager with:\nless ~/effective-shell/scripts/common.v3.sh Let's add a loop to our common, that let's use show a number next to each command so that we can see the order of the commands.\nAs the file is a little larger now, I am not going to show the entire file, only the key changes we will make.\nFirst, in each of the sections that performs the command to get the common commands we will use Shell Parameter Expansion to run a sub-shell and store the results in a variable:\n# Store the most recently used commands in the \u0026#39;commands\u0026#39; variable. commands=$(tail ~/.bash_history -n ${history_lines} \\  | sort \\  | uniq -c \\  | sed \u0026#39;s/^ *//\u0026#39; \\  | sort -n -r \\  | head -n ${command_count}) There are two places we have to make this change - the first is for the Bash Shell and the second is for the Z-Shell. Now that we have stored our commands in a variable, we can loop through it at the end of the script and show a number that gives the order of each command:\n# Print each command, showing what its order is in the list. # Commands are separated by newlines, so temporarily change IFS to loop over # each line of the commands. counter=1 old_ifs=$IFS IFS=$\u0026#39;\\n\u0026#39; for command in $commands do echo \u0026#34;$counter: $comand\u0026#34; counter=$((counter + 1)) done IFS=$old_ifs The updated script is in the samples folder at ~/effective-shell/scripts/common.v4.sh, you can update your link to point to this version by running the ln command:\nln -s ~/effective-shell/scripts/common.v4.sh /usr/local/bin/common Now when we run this command, each of our common commands is printed with its order shown:\n$ common 1: 135 gst 2: 73 vi 3: 47 gc 4: 40 ls 5: 37 ga . 6: 27 gpo 7: 25 gl 8: 24 gpr 9: 21 gcm 10: 17 make dev Summary In this chapter we looked at how to use different types of loops in the shell, to iterate over values in an array, words in a sentence, files and folders or even the results of commands.\nWe also looked in detail at how \u0026lsquo;word-splitting\u0026rsquo; works, as well as the $IFS variables. In the next chapter we'll look at functions and parameters.\nAppendix - Loops and the Z-Shell The Z-Shell does not perform word-splitting on unquoted variables. This is a deliberate choice by the designers, to avoid what can often be confusing behaviour.\nWe can see this behaviour below:\n% sentence=\u0026quot;one two three\u0026quot; % for word in $sentence; do echo \u0026quot;Word: $word\u0026quot;; done Word: one two three If you want to use more Posix-like functionality then you can set the SH_WORD_SPLIT parameter. You can find out more about this parameter by running man zsh and searching for SH_WORD_SPLIT.\n  If we had put quotes around the wildcard text it would not be expanded - check the section on \u0026lsquo;Quoting\u0026rsquo; in Chapter 19 - Variables, Reading Input, and Mathematics if you need a refresher on this. \u0026#x21a9;\u0026#xfe0e;\n ANSI C Quoting is described in the \u0026lsquo;Quoting\u0026rsquo; section in Chapter 19 - Variables, Reading Input, and Mathematics \u0026#x21a9;\u0026#xfe0e;\n   "});index.add({'id':22,'href':'/docs/part-4-shell-scripting/functions-parameters-and-error-handling/','title':"Functions, Parameters and Error Handling",'content':"Chapter 22 - Functions, Parameters and Error Handling The shell allows you to create functions - a set of commands that you can call at any time. In this chapter we'll see how to create functions and how function parameters and script parameters are handled. We will also look at status codes for commands and scripts and error handling.\nCreating a Function  A function has the following structure:\n\u0026lt;function-name\u0026gt; { \u0026lt;function-command 1\u0026gt; \u0026lt;function-command 2\u0026gt; \u0026lt;function-command n\u0026gt; } First we specify the name of the function. Then between a set of opening and closing curly braces, we list the commands that should be executed when we call the function.\nLet's take a look at a very simple function in action:\ntitle() { echo \u0026#34;My Script version 1.0\u0026#34; } This script defines a very simple function called title that prints out a message. We call the function in the same way we would call any command in the shell, by simply writing the name of the command and hitting enter. Here's how we would call the function:\n$ title My Script version 1.0\u0026quot; Easy! Functions let you structure commands into logical blocks and can help make your scripts easier to read and manage.\nVariables in Functions A function can read and write to any variables in the current shell session. Here's an example:\n# Set some variables. title=\u0026#34;My Cool Script\u0026#34; version=\u0026#34;1.2\u0026#34; succeeded=0 # Create a function that writes a message and changes a variable. title() { # Note that we can read variables... title_message=\u0026#34;${title}- version ${version}\u0026#34; echo \u0026#34;${title_message}\u0026#34; # ...and set them as well. succeeded=1 } # Show the value of \u0026#39;succeeded\u0026#39; before and after the function call. echo \u0026#34;Succeeded: ${succeeded}\u0026#34; title echo \u0026#34;Succeeded: ${succeeded}\u0026#34; echo \u0026#34;Title Message: ${title_message}\u0026#34; The output of this script will be:\nSucceeded: 0 My Cool Script - version 1.2 Succeeded: 1 Title Message: My Cool Script - version 1.2 This demonstrates that functions can use the variables that are available in the shell. They can also set variables. We can also create new variables in functions.\nVariable Scoping  If you come from a programming background you might find it odd that you can create a variable in a function and use it outside of the function. This is a feature known as dynamic scoping. Many common programming languages like Python, JavaScript, C, Java and others use an alternative mechanism called lexical scoping.\nLexical scoping is a feature that ensures that you can only use a variable from within the \u0026lsquo;scope\u0026rsquo; that it is defined. This can reduce errors - because it means that if you define a variable in a function you don't accidentally \u0026lsquo;overwrite\u0026rsquo; the value of another variable that is used elsewhere.\nYou can use the \u0026lsquo;local\u0026rsquo; keyword to define a variable that is only available in the \u0026lsquo;local\u0026rsquo; scope, i.e. the function that it is defined in. This allows you to use lexical scoping and can reduce the risk of errors. Here's an example:\nrun_loop() { local count=0 for i in {1..10}; do # Update our counter. count=$((count + 1)) done echo \u0026#34;Count is: ${count}\u0026#34; } Let's see what happens if we run function:\n$ run_loop Count is: 10 $ echo \u0026quot;Count: ${count}\u0026quot; Count: Notice that because we declared the count variable using the \u0026lsquo;local\u0026rsquo; keyword, it is only available inside the run_loop function. If we try and access it outside of the function it is undefined.\nIn general, you should use \u0026lsquo;local\u0026rsquo; variables inside functions. This can help to avoid problems where calling a function can have an unintended side effects:\n# Set a count variable somewhere in our script... count=3 # Call our \u0026#39;run_loop\u0026#39; function. run_loop # Write out the value of \u0026#39;count\u0026#39;. echo \u0026#34;The \u0026#39;count\u0026#39; variable is: ${count}\u0026#34; The output of this script is:\nCount is: 10 The 'count' variable is: 3 Notice that even though we used a variable named count in the run_loop function, we did not overwrite the value that was set outside of the function. If we were to run the same script but not use the \u0026lsquo;local\u0026rsquo; keyword for he count variable, we would get the following output:\nCount is: 10 The 'count' variable is: 10 In this case calling the function changes the \u0026lsquo;count\u0026rsquo; variable that is outside of the function. In most cases this is not going to be what you want and will just lead to unexpected behaviour later on.\nPassing Parameters to Functions You can pass any number of parameters to a shell function. To get the value of a parameter, we can use special built-in variables that represent each parameter. Let's take a look at an example:\nsum() { local value1=$1 local value2=$2 local result=$((value1 + value2)) echo \u0026#34;The sum of ${value1}and ${value2}is ${result}\u0026#34; } Let's see how we can pass parameters to this function:\n$ sum 3 6 The sum of 3 and 6 is 9 $ sum 10 33 The sum of 10 and 33 is 43 In this script we have used the special $1 and $2 built-in variables to get the value of the first and second parameters. At the beginning of the function I have put these variables into local variables that have more descriptive names. This is purely to make the script more readable, I could also have written the function like this:\n# Create a function that calculates the sum of two numbers. sum() { echo \u0026#34;The sum of $1and $2is $(($1 + $2))\u0026#34; } For a short and simple function you might just use the special parameter variables directly like above. However for anything more complex than a one-line script I think that it is generally more readable to create a local variable with a more descriptive name.\nParameter Variables There are a number of special parameter variables that the shell provides. Let's see a few in action:\n# Create a function that sums a set of numbers. sum() { local total=0 for value in $@; do total=$((total + value)) done # Write out the result. echo \u0026#34;Summed $#values for a total of: ${total}\u0026#34; } We can call this function with any number of parameters:\n$ sum 1 2 3 4 5 Summed 5 values for a total of: 15 In this script we've used two special variables. The $@ variable is expanded into a list of all of the function parameters. The $# variable contains the number of parameters provided to the function.\nYou might recognise that these variables look quite similar to the syntax that is used to get the members of an array or the length of an array as described in Chapter 19 - Variables, Reading Input, and Mathematics. You can actually use some of the array-style operators with the special parameters variable:\n# Show the top \u0026#39;n\u0026#39; values of a set. show_top() { local n=$1 local values=${@:2:n} echo \u0026#34;Top ${n}values: ${values}\u0026#34; } We can call this function with any number of parameters. The first parameter specifies how many of the subsequent parameters we will show:\n$ show_top 3 10 20 30 40 50 Top 3 values: 10 20 30 We have used the \u0026lsquo;range\u0026rsquo; operator on the $@ variable to get a subset of the parameters. This script is a little odd to read because when we set the \u0026lsquo;values\u0026rsquo; parameter we need to \u0026lsquo;skip\u0026rsquo; past the first positional parameter, because the first positional parameter is the number of values to show.\nThe table below shows some of the common variables you can use when working with function parameters:\n   Variable Description     $1 The first parameter   $2 The second parameter   ${11} The 11th parameter - if the parameter is more than one digit you must surround it with braces   $# The number of parameters   $@ The full set of parameters as an array   $* The full set of parameters as a string separated by the first value in the $IFS variable   ${@:start:count} A subset of \u0026lsquo;count\u0026rsquo; parameters starting at parameter number \u0026lsquo;start\u0026rsquo;    The $@ and @* parameters look quite similar. The first one is an array, just like we saw in Chapter 19 - Variables, Reading Input, and Mathematics. The second version is the parameters collected together into a single string separated by spaces (actually, separated by the first character in the $IFS variable).\nParameter Shifting We can use the shift (shift positional parameters) to remove a number of parameters from the beginning of the position parameters list and \u0026lsquo;shift\u0026rsquo; the remaining parameters to take their place.\nThis is a little hard to describe so let's see how we can use it to simplify our show_top function:\n# Show the top \u0026#39;n\u0026#39; values of a set. show_top() { # Grab the number of values to show, then shift. local n=$1 shift # Get the set of values to show. Notice that we start in position 1 now. local values=${@:1:n} echo \u0026#34;Top ${n}values: ${values}\u0026#34; } After we get the value of the first parameter, we \u0026lsquo;shift\u0026rsquo;, removing it from the list of positional parameters so that we can deal with the remaining parameters. I would avoid using \u0026lsquo;shift\u0026rsquo; too much - if you find that you are having to write complex code to shift parameters around you might be better using a programming language rather than the shell for the task you are performing!\nReturn Values You can return a value from a shell function in two ways. The first is to simply set the value of a variable, like so:\nis_even() { local number=$1 # A number is even if when we divide it by 2 there is no remainder. # Set \u0026#39;result\u0026#39; to 1 if the parameter is even and 0 otherwise. if [ $((number % 2)) -eq 0 ]; then result=1 else result=0 fi } A function could set any number of variables to provide output. Here's how we could use the is_even function:\n$ number=33 $ is_even $number $ echo \u0026quot;Result is: $0\u0026quot; Result is: 0 In general, this method of returning values from a function should be avoided, for the reasons we've discussed already in this chapter. It overwrites the value of a global variable and that can be confusing for the operator.\nA more common way to return a value from a function is to write its result to stdout - let's look at this in detail.\nWriting Results to Stdout If we write our result to stdout, then we can capture the result of a function in a far more readable way:\nlowercase() { local params=\u0026#34;$@\u0026#34; # Translate all uppercase characters to lowercase characters. echo \u0026#34;$params\u0026#34; | tr \u0026#39;[:upper:]\u0026#39; \u0026#39;[:lower:]\u0026#39; } In this example we write the result of the function to stdout. This means that we can capture the result and put it in another variable by simply executing the command in a subshell:\n$ result=$(lowercase \u0026quot;Don't SHOUT!\u0026quot;) $ echo \u0026quot;$result\u0026quot; don't shout! If you have a programming background it might seem very strange that you write results in a function by writing to stdout. Remember - the shell is a text based interface to the computer system. The majority of commands that we have seen so far that provide output write their output to the screen. This is what ls does, what find does, what cat does and so on. When we echo a result from a function, we are really just following the Unix standard of writing the results of a program to the screen.\nThis is important - if we run our function directly in a shell, we can see the result written to the screen:\n$ lowercase \u0026quot;PLEASE don't SHOUT!\u0026quot; please don't shout! Remember - shell functions are designed to behave in a similar way to shell commands. They write their output to stdout.\nDealing with Output in Commands Although it might feel a bit clunky, writing the results of a command to stdout is a tried and tested method of returning results. However, we need to be careful. Let's take a look at an example to see why!\n# This function creates a temporary folder for today and returns its path. temp_today() { # Get today\u0026#39;s date in the format YYYY-MM-DD. local today=$(date +\u0026#34;%Y-%m-%d\u0026#34;) # Create a temporary directory for today and return it. tmpdir_today=\u0026#34;/tmp/${today}\u0026#34; echo \u0026#34;Creating folder \u0026#39;${tmpdir_today}\u0026#39;...\u0026#34; mkdir -p \u0026#34;${tmpdir_today}\u0026#34; echo \u0026#34;${tmpdir_today}\u0026#34; } This function creates a temporary folder that is based on the current date. If we try and grab the result of the function and change to that folder then the script will fail:\n# Go to today\u0026#39;s temporary folder. folder=$(temp_today) cd \u0026#34;${folder}\u0026#34; This script fails, with the output:\n'Creating folder \\'/tmp/2021-05-28\\'...\\n/tmp/2021-05-28': No such file or directory What's going on here?\nWell in the temp_today function we wrote a message halfway through the function, showing the name of the folder that would be created. This message is part of the output of the function. Even though in the last line we echo the path to the folder, the output of the command is all of the text we have written.\nIt is important to remember that any command you call in a function that might write to stdout could cause problems as it could write text to your output:\ncommand_exists() { if type \u0026#34;$1\u0026#34;; then echo \u0026#34;1\u0026#34; else echo \u0026#34;0\u0026#34; fi } What happens when we try and store the result of the function in a variable?\n$ result=$(command_exists \u0026quot;touch\u0026quot;) $ echo \u0026quot;Result is: ${result}\u0026quot; Result is: touch is hashed (/usr/bin/touch) 1 This is not a well written function, we'll look at a better way to write it next. But it shows an important challenge to be aware of - when type is used to find out whether a command exists it returns success if the command exists but also writes to stdout.\nIn Chapter 7 - Thinking in Pipelines we saw that we can send the output of a command to the \u0026lsquo;null\u0026rsquo; device to silence its output. We can use this trick in our functions to stop commands from \u0026lsquo;polluting\u0026rsquo; our result: T\ncommand_exists() { if type \u0026#34;$1\u0026#34; \u0026gt;\u0026gt; /dev/null; then echo \u0026#34;1\u0026#34; else echo \u0026#34;0\u0026#34; fi } Now if we run this command we will not get the output from the type command in our result - the output was redirected to the null device.\nReturning Status Codes The return (return from shell function) command causes a function to exit with a given status code.\nThis is something that often causes confusion in shell scripts. The reason is that in most programming languages, you would use a \u0026lsquo;return\u0026rsquo; statement to return the result of a function. But in the shell, when we return, we set the status code of the function.\nWhat is a status code? We actually touched on this in Chapter 20 - Mastering the If Statement. When a command runs, we expect it to return a status code of \u0026lsquo;zero\u0026rsquo; to indicate success. Any non-zero status code is used to specify an error code.\nLet's see how we could re-write the command_exists function to set a status code:\ncommand_exists() { if type \u0026#34;$1\u0026#34; \u0026gt;\u0026gt; /dev/null; then return 0 else return 1 fi } Now that our command sets a status code properly, we can use it in an \u0026lsquo;if statement\u0026rsquo; like so:\nif command_exists \u0026#34;common\u0026#34;; then echo \u0026#34;The \u0026#39;common\u0026#39; command is installed on your system\u0026#34; else echo \u0026#34;The \u0026#39;common\u0026#39; command is not installed on your system\u0026#34; fi Remember - only use the \u0026lsquo;return\u0026rsquo; command to set a status code. Many shells will only allow values from 0-255 to be set, and most users will expect that a command should return zero for success and that any non-zero value is an error code. If you need to provide output for a command that is not just a status code, you should write it to stdout or if you must, set the value of a global variable.\nThe result of the last executed command is always available in the special variable $?. Here's how you could use it:\n$ type \u0026quot;test\u0026quot; test is a shell builtin $ echo \u0026quot;Result: $?\u0026quot; Result: 0 Error Handling When you run a shell script, if a command in the script fails, the script will continue to run. Like many other points in this chapter this might seem unintuitive if you come from a programming background, but this makes sense in the shell - if the shell was to terminate whenever a command fails it would be very difficult to use interactively.\nLet's create a script called \u0026lsquo;today\u0026rsquo; that makes a new temporary folder each day, then puts a link to that folder in our home directory:\n#!/usr/bin/env sh  # Get today\u0026#39;s date in the format YYYY-MM-DD. today=$(date +\u0026#34;%Y-%m-%d\u0026#34;) # Create the path to today\u0026#39;s temp folder and then make sure the folder exists. temp_path=\u0026#34;/tmp/${today}\u0026#34; mkdir -p \u0026#34;${temp_path}\u0026#34; # Now that we\u0026#39;ve created the folder, make a symlink to it in our homedir. ln -sf \u0026#34;${temp_path}\u0026#34; \u0026#34;${HOME}/today\u0026#34; # Write out the path we created. echo \u0026#34;${temp_path}\u0026#34; Now we can run the script to create temporary folder for the current day and a link to it in our home directory:\n$ chmod +x ./today.sh $ ./today.sh /tmp/2021-05-28 $ cd ~/today In this example we created a new directory in the tmp folder and created a link to it in our home directory. But what happens if we cause one of the commands to fail?\nFirst, let's clean up the folder we created:\n$ rm -rf $(./today.sh) $ rm ~/today Now we'll create a file where we want to put our \u0026lsquo;today\u0026rsquo; folder:\n$ touch \u0026quot;/tmp/$(date +\u0026quot;%Y-%m-%d\u0026quot;)\u0026quot; If we run our script now, we can see a problem:\n$ ./today.sh mkdir: /tmp/2021-05-28: Not a directory /tmp/2021-05-28 $ cd ~/today bash: cd: /home/dwmkerr/today: Not a directory The mkdir command failed - because there was a file in the location where we wanted to create the folder. But the script kept on running - meaning that it created a symlink to this file. Now when we try to move to the today folder we get another error - it is a link to a file not a folder.\nIn general in your shell scripts if a command fails you probably want the entire script to stop executing. Otherwise you can get this cascading effect as commands continue to return even after there was a failure, which can lead to all sorts of unexpected behaviour.\nYou can use the set (set option) command to set an option in the shell. There is an option that tells the shell to exit when a command fails. Here's how we would use it:\n#!/usr/bin/env sh # Exit if any command fails. set -e # ... The \u0026lsquo;set\u0026rsquo; command allows you to turn on and turn off shell options. The \u0026lsquo;e\u0026rsquo; option means \u0026lsquo;exit if any command exits with a non-zero status\u0026rsquo;.\nNow let's clean up again:\n$ rm -rf $(./today.sh) $ rm ~/today And finally, we'll run the same script after creating the file that will cause a failure:\n$ touch \u0026quot;/tmp/$(date +\u0026quot;%Y-%m-%d\u0026quot;)\u0026quot; $ ./today.sh mkdir: /tmp/2021-05-28: Not a directory In this case the script stopped running as soon as there was a failure - after the mkdir command failed.\nThe Function Keyword In some scripts you might see functions defined using the function keyword, as below:\nfunction title() { echo \u0026#34;My Script version 1.0\u0026#34; } The \u0026lsquo;function\u0026rsquo; keyword is not required. The keyword is available in Bash and similar shells. Using the function keyword has a minor benefit that it does not lead to an error if you have already defined an alias with the same name as the function you are declaring. However, the drawback is that it is less standard and therefore less portable.\nI would recommend that you do not use the \u0026lsquo;function\u0026rsquo; keyword. Firstly, this will make your scripts more portable. Secondly, if your function is going to clash with the name of an alias that has already been defined, I would actually think that it is better that the script fails. Better to fail early and realise there is clash than to silently overwrite the alias which may cause unexpected errors later on when something else tries to call the alias and calls your function instead!\nParameters and Status Codes for Scripts Everything we have learned about parameters applies to scripts themselves. We can pass parameters to scripts and read them with the special variables such as $1, $2 and so on.\nThe only difference is that instead of using the return command when we want to exit a script with a status code, we use the exit (exit the shell) command. The exit command exits the current shell with the provided status code.\nBe careful when using the exit command - if you are running a script then it is fine to use exit, it will simply close the subshell that the script is running in. But if you type exit in your shell that you are using interactively, it will close it.\nUpdating the \u0026lsquo;common\u0026rsquo; Command In the previous chapter we created the common.v4.sh command, that shows common commands from the users shell history.\nIf you need a refresher on what is in the script, you can view it in your pager with:\nless ~/effective-shell/scripts/common.v4.sh The output of the command will look something like:\n1: 280 gst 2: 144 vi 3: 84 gc 4: 72 ga . 5: 62 gl 6: 54 ls 7: 50 gpo 8: 48 gcm 9: 45 make dev 10: 44 gpr Let's make a couple of changes.\nFirst, let's make sure we exit the script if one of the commands fails:\n# Exit if any command fails. set -e Next, we will update the script on line 7 so that we use the first parameter as the command count. If the first parameter is not set, we default to ten:\n# ... command_count=${1:-10} # The number of common commands to show # ... Here we are using the $1 variable. But we are also using Shell Parameter Expansion as described in Chapter 19 - Variables, Reading Input, and Mathematics to provide a default value to use if the parameter is not set.\nNext, let's change the line that writes out the count and the name of the command. At the moment, the count is shown and then the command name. Let's write a function that takes a number and line of text and writes it as a line of text with the number after the text and in brackets:\nwrite_command_then_count() { # Get the command and count, this will be text that looks like: # \u0026#39;43 git commit\u0026#39; # Then write the command and the count afterwards. local line=\u0026#34;$1\u0026#34; local count=$(echo \u0026#34;${line}\u0026#34; | cut -d\u0026#39; \u0026#39; -f1) local command=$(echo \u0026#34;${line}\u0026#34; | cut -d\u0026#39; \u0026#39; -f2-) echo \u0026#34;${command}(${count})\u0026#34; } We can now re-write our loop to make it a little cleaner:\nfor command in $commands do echo \u0026#34;$counter: $(write_command_then_count \u0026#34;$command\u0026#34;)\u0026#34; counter=$((counter + 1)) done The updated script is in the samples folder at ~/effective-shell/scripts/common.v5.sh, you can update your link to point to this version by running the ln command:\nln -s ~/effective-shell/scripts/common.v5.sh /usr/local/bin/common Now when we run this command we can optionally provide the number of commands to show as a parameter. The output also is shown with the number of times the command has been called after the command text itself:\n$ common 5 common commands: 1: gst (139) 2: vi (74) 3: gc (42) 4: ga . (36) 5: gl (31) Summary In this chapter we looked at how to use functions to provide more structure to our shell scripts, and also how to use parameters, return values and status codes.\nIn the next and final chapter of this section, we'll look at some more advanced techniques that can be useful when writing shell scripts.\n"});index.add({'id':23,'href':'/docs/part-4-shell-scripting/useful-patterns-for-shell-scripts/','title':"Useful Patterns for Shell Scripts",'content':"Chapter 23 - Useful Patterns for Shell Scripts To close this the section on shell scripting we're going to look at some common patterns you will see in shell scripts. These are an assortment of techniques you may find useful when building your scripts - you may come across them in scripts others have written as well.\nRemember that although this chapter focuses on patterns that are useful in scripts, you can apply these patterns in any shell session. This means you might find this chapter useful even if you are not expecting to write scripts, just as a way to understand some more advanced shell techniques.\nDebugging Shell Scripts You can use the set (set option) command to set the _trace option_. This option is incredibly useful for debugging shell scripts. When the trace option is set, the shell will write out each statement before it is evaluated.\nLet's see just how useful this is with an example!\n# today.sh - creates a \u0026#39;today\u0026#39; symlink in the home directory folder to a fresh # temporary folder each day. # Enable tracing in the script. set -x # Get today\u0026#39;s date in the format YYYY-MM-DD. today=$(date +\u0026#34;%Y-%m-%d\u0026#34;) # Create the path to today\u0026#39;s temp folder and then make sure the folder exists. temp_path=\u0026#34;/tmp/${today}\u0026#34; mkdir -p \u0026#34;${temp_path}\u0026#34; # Now that we\u0026#39;ve created the folder, make a symlink to it in our homedir. ln -sf \u0026#34;${temp_path}\u0026#34; \u0026#34;${HOME}/today\u0026#34; # Disable tracing now that we are done with the work. set +x # Write out the path we created. echo \u0026#34;${temp_path}\u0026#34; Notice that we use set -x to enable tracing early on in the script, and set +x to disable tracing towards the end. If we run this script, we'll see the following output:\n$ ~/effective-shell/scripts/today.sh ++ date +%Y-%m-%d + today=2021-05-29 + temp_path=/tmp/2021-05-29 + mkdir -p /tmp/2021-05-29 + ln -sf /tmp/2021-05-29 /home/dwmkerr/today + set +x /tmp/2021-05-29 Each command that the shell executes is written to stdout before it is executed. The parameters are expanded, which can make it far easier to see what is going on and troubleshoot issues.\nThe + symbol is written at the start of each trace line, so that you can differentiate it from normal output that you write in your script1. The final line of output in the example above does not have a + in front of it - because it is actual output from an echo command, rather than a trace line.\nThe number of + symbols indicates the \u0026lsquo;level of indirection\u0026rsquo; - this is how many sub-shells you are in. Each subshell is traced on its own line. This makes tracing complex commands far easier:\nset -x echo \u0026#34;Name of home folder is $(basename $(echo ~) )\u0026#34; The output of this command is:\n+++ echo /home/dwmkerr ++ basename /home/dwmkerr + echo \u0026#39;Name of home folder is dwmkerr\u0026#39; Name of home folder is dwmkerr Notice that each subshell command is shown with an additional plus as it gets more nested. The nested commands are shown in the order that they are evaluated.\nChecking for Existing Variables or Functions The declare (set variable values and attributes) command can be used to explicitly declare that we are creating a variable. We saw in Chapter 19 - Variables, Reading Input, and Mathematics that sometimes this command is required - if we want to create an associative array for example.\nThere are a number of options for the \u0026lsquo;declare\u0026rsquo; command, but one that is particularly useful is the -p (display attributes and value) option. This can be used to show all of the variables of a certain type.\nHere's an example to show all associative arrays that have been created:\n$ declare -p -A declare -A BASH_ALIASES=() declare -A BASH_CMDS=() You can also use this command to validate whether a variable has been set or not:\nif declare -p -A my_options 2\u0026gt;1 /dev/null; then echo \u0026#34;\u0026#39;my_options\u0026#39; exists\u0026#34; else echo \u0026#34;\u0026#39;my_options\u0026#39; does not exist\u0026#34; fi We have to silence the error output of the declare command unless we want it to print an message if the variable doesn't exist. This technique can be useful to use before setting variables to ensure that they are not already in use, or check that the variable exists.\nFunctions are also variables - so we can use this trick to show all functions that are declared, the value of a function, or check if a function exists.\nUnsetting Values If you are writing a script that should clean up after itself, you might want to use the unset (unset values and attributes) command. This can be useful if you want to create a script that leaves behind no variables or functions that could cause issues for later users:\n# Remove the \u0026#39;is_even\u0026#39; function from the shell session. unset -f is_even Traps You can use the trap (trap signals and events) command to specify a set of commands to run when the shell receives signals, or at certain points such as when the script exits or a function returns.\nA very common use for traps is to create a \u0026lsquo;cleanup\u0026rsquo; function that is executed when the script exits or if the user aborts execution by pressing Ctrl+C (which sends the SIGINT signal).\nHere's an example of how a trap can be set to cleanup a temporary folder when a script exits or is interrupted:\n# Create a temporary folder for the effective shell download. source=\u0026#34;https://effective-shell.com/downloads/effective-shell-playground.tar.gz\u0026#34; tmp_dir=$(mktemp -d 2\u0026gt;/dev/null || mktemp -d -t \u0026#39;effective-shell\u0026#39;) tmp_tar=\u0026#34;${tmp_dir}/effective-shell.tar.gz\u0026#34; # Define a cleanup function that we will call when the script exits or if # it is aborted. cleanup () { if [ -e \u0026#34;${tmp_tar}\u0026#34; ]; then rm \u0026#34;$tmp_tar}\u0026#34;; fi if [ -d \u0026#34;${tmp_dir}\u0026#34; ]; then rm -rf \u0026#34;${tmp_dir}\u0026#34;; fi } # Cleanup on interrupt or terminate signals and on exit. trap \u0026#34;cleanup\u0026#34; SIGINT SIGTERM EXIT # Download the samples. curl --fail --compressed -q -s \u0026#34;${source}\u0026#34; -o \u0026#34;${tmp_tar}\u0026#34; # Extract the samples. tar -xzf \u0026#34;${tmp_tar}\u0026#34; -C \u0026#34;${tmp_dir}\u0026#34; In this script we have defined a function called \u0026lsquo;cleanup\u0026rsquo;. We then use the trap command to ensure that we call the function if SIGINT is sent, SIGTERM is sent or when the script exits. This is very useful in scripts that can take a while. This script downloads the effective shell samples from the internet. If the user is having connectivity issues then this might take a while and they may end up aborting the script. If they do so in this case we will still clean up the temporary folder we created.\nTraps provide a very convenient way to handle things like cleanup, provide more diagnostic information or even disable a user from interrupting your script. In the example below we force the user to press Ctrl+C twice if they want to interrupt the script:\ninterrupt_count=0 on_interrupt() { if [ $interrupt_count -lt 1 ]; then echo \u0026#34;Aborting this operation can cause errors.\u0026#34; echo \u0026#34;Press Ctrl+C again if you are sure you want to cancel.\u0026#34; interrupt_count=$((interrupt_count + 1)) else # Convention is to use the status code 130 for interrupted scripts. echo \u0026#34;Aborting long operation\u0026#34; exit 130 fi } trap on_interrupt SIGINT total_time=0 while true; do echo \u0026#34;Long operation: ${total_time}seconds elapsed\u0026#34; sleep 3 total_time=$((total_time + 3)) done If we run this script we can see that the user must press Ctrl+C twice to abort the operation:\n$ ~/effective-shell/scripts/long-operation.sh Long operation: 0 seconds elapsed Long operation: 3 seconds elapsed Long operation: 6 seconds elapsed ^CAborting this operation can cause errors. Press Ctrl+C again if you are sure you want to cancel. Long operation: 9 seconds elapsed Long operation: 12 seconds elapsed ^CAborting long operation Some other things that you might want to be aware of for the trap command are:\n The SIG at the beginning of the name of a signal is optional, and a signal number can also be used - this means that SIGINT, INT and 2 are all equivalent options for trap You can list the signals available with trap -l or kill -l - but remember that special conditions such as EXIT and RETURN are not listed, you can find these with help trap You can stop a signal from being processed with trap \u0026quot;\u0026quot; SIGINT - this means that no command will be executed when we receive a SIGINT You can reset a trap by running trap - SIGINT, this will remove any trap handler You can test your traps by sending a signal explicitly to your script with kill -s SIGINT, providing the name of the signal  Handling Options You can use the getopts (parse option arguments) command to process the arguments for a script or function\nLet's imagine we wanted to update our \u0026lsquo;common\u0026rsquo; command to support the following options:\n -h for \u0026lsquo;help\u0026rsquo;, which shows command help -e for \u0026lsquo;execute\u0026rsquo;, which takes the number of a command from the list which will be executed  The \u0026lsquo;getopts\u0026rsquo; command takes two parameters. The first is an \u0026lsquo;option string\u0026rsquo;, which is a list of the parameter letters that are allowed. This string starts with a colon, and any letter which is followed by a colon is expected to have a value provided. The second parameter is the name of the variable to set when we are processing options.\nTypically this command is used in a while loop, as it will return \u0026lsquo;success\u0026rsquo; until the final option has been processed. A case statement is typically used to process the option:\n# Helper function to show how the command should be invoked. show_help() { echo \u0026#34;usage:\u0026#34; echo \u0026#34; common [-h] [-e \u0026lt;command_number\u0026gt;] count\u0026#34; } # Process the options. while getopts \u0026#34;:he:\u0026#34; option; do case ${option} in # Handle the \u0026#39;help\u0026#39; option. h ) show_help exit 0 ;; # Handle the \u0026#39;execute command\u0026#39; option by storing the value provided # for the option. e ) execute_command=${OPTARG} ;; # If we have an invalid argument, warn and fail. \\? ) echo \u0026#34;The value \u0026#39;${OPTARG}\u0026#39; is not a valid option\u0026#34; exit 1 ;; # If we are missing a required argument, warn and exit. : ) echo \u0026#34;The option \u0026#39;${OPTARG}\u0026#39; requires an argument\u0026#34; ;; esac done There are a few things to point out from this script:\n The option string starts with a colon - any option letter that is followed by a colon expects an argument If an invalid option letter is set, the value of the option variable is set to \\? - we can then handle this in our case statement If a letter is provided without an argument that is required, the value of the option variable is set to : - we can then handle this in our case statement  For complex option processing you might see scripts where multiple loops are used to process sets of options. It is common to end option processing with the following line:\nshift $((OPTIND - 1)) The ${OPTIND} variable stores the index of the last option processed. By shifting by this value minus one, we remove the processed options from the $@ (all parameters) array. This means we don't try to process the same options again.\nThe ~/effective-shell/scripts/common.sh script processes parameters using the getopts command. You can use this as an example to help you with your own scripts.\nColouring Output There are special escape sequences that can be used in the shell to colour the output of the text shown. For example, in many terminals the following text will be shown in green:\ngreen=\u0026#39;\\e[0;32m\u0026#39; reset=\u0026#39;\\e[0m\u0026#39; echo -e \u0026#34;Do you like ${green}apples${reset}?\u0026#34; On most terminals you will see the text below, with the word \u0026lsquo;apples\u0026rsquo; rendered in green:\nDo you like apples? Note that it is important to provide the -e flag to the \u0026lsquo;echo\u0026rsquo; command so that it correctly processes the colour codes. In fact, a better option is to use the printf (format and print arguments) command, as it is more portable and behaves more consistently across different versions of Unix and Linux.\nThe colour codes are ANSI escape sequences that have been defined to control the formatting of content in a terminal. There are number of formatting options - such as foreground and background colours, bold, underline and so on. These codes can be quickly found online if you search for \u0026ldquo;ANSI color codes\u0026rdquo;.\nIt is important to be careful when using colour codes - you don't want them in all circumstances. Let\u0026rsquo; see an example. The \u0026lsquo;rainbow\u0026rsquo; function below writes out a message in a number of colours:\nrainbow () { local message=\u0026#34;$1\u0026#34; local reset=\u0026#39;\\e[0m\u0026#39; for ((colour=31; colour\u0026lt;=37; colour++)) do colour_code=\u0026#34;\\\\e[0;${colour}m\u0026#34; printf \u0026#34;${colour}- ${colour_code}${message}${reset}\\n\u0026#34; done } If we run this function in most terminals, we'll see the provided message with the colour number in seven different colours:\n$ rainbow hello 31 - test 32 - test 33 - test 34 - test 35 - test 36 - test 37 - test We have to be careful when formatting output. It can be helpful for a user in an interactive shell (on many systems for example even the ls command is actually an alias for ls --color=auto meaning that the ls command uses colours in its output). But there are circumstance when we don't want to use colour codes. Let's see what we get when we write the rainbow output to a file:\n$ rainbow hello \u0026gt;\u0026gt; text.txt $ cat -v text.txt 31 - ^[[0;31mhello^[[0m 32 - ^[[0;32mhello^[[0m 33 - ^[[0;33mhello^[[0m 34 - ^[[0;34mhello^[[0m 35 - ^[[0;35mhello^[[0m 36 - ^[[0;36mhello^[[0m 37 - ^[[0;37mhello^[[0m The \u0026lsquo;-v\u0026rsquo; parameter tells cat to make escape characters visible. If you open the in a text editor you will see the same escape characters written in the file.\nThis shows the problem with the rainbow function - it adds the colour escape sequences even when we are writing the results to a file. In most cases this is not going to be what we want. Commands like ls do not include colour codes when writing to a file.\nThere is not an entirely fool-proof way to avoid this issue, but the most common pattern I have seen is to check whether the standard output file descriptor is associated with a terminal. We can do this using the -t expression of the test command:\nif [ -t 1 ]; then echo \u0026#34;We are writing to a terminal\u0026#34; else echo \u0026#34;We are not writing to a terminal\u0026#34; fi You will see -t 1 in a number of scripts as a way to check whether the output is going to a terminal device. The -t test returns success if the provided file descriptor is associated with a terminal device. The file descriptor \u0026lsquo;1\u0026rsquo; is the descriptor for the stdout stream (if this is unfamiliar, check Chapter 7 - Thinking in Pipelines).\nHere's how we could use the test in our rainbow function:\nrainbow () { local message=\u0026#34;$1\u0026#34; local reset=\u0026#39;\\e[0m\u0026#39; for ((colour=31; colour\u0026lt;=37; colour++)) do colour_code=\u0026#34;\\\\e[0;${colour}m\u0026#34; if [ -t 1 ]; then printf \u0026#34;${colour}- ${colour_code}${message}${reset}\\n\u0026#34; else printf \u0026#34;${colour}- ${message}\\n\u0026#34; fi done } This version of the function will not write the ANSI escape sequences if the output device is not a terminal, meaning that if we run:\n$ rainbow test \u0026gt; text.txt Then the output file will not contain escape sequences. You can find out more about the -t test by running man test.\nAs a final tip - if you are formatting output you should consider using the tput (query terminfo database) command to make your code more readable and portable:\ngreen=$(tput setaf 2) # set ansi foreground to \u0026#39;2\u0026#39; (green) reset=$(tput sgr0) # reset the colours echo -e \u0026#34;Do you like ${green}apples${reset}?\u0026#34; The \u0026lsquo;tput\u0026rsquo; command is quite advanced, but you can search online for more details (the manual pages for the command are hard to decipher as it can be used for many operations and is complex).\nThe ~/effective-shell/scripts/common.sh script includes colourised output and also checks to see whether colour codes should be printed based - you can use this as a reference for your own scripts.\nChecking the Operating System Different flavours of Unix and Linux can behave quite differently. A common requirement is to write scripts that are portable and can be used across systems. However, this is not always possible. There are times when we need to check to see whether we are on a specific operating system and take a specific action.\nYou will often see the uname (show operating system name) command used to check the operating system:\ncase \u0026#34;$(uname)\u0026#34; in Darwin) os=\u0026#34;OSX\u0026#34; ;; Linux) os=\u0026#34;Linux\u0026#34; ;; CYGWIN*|MINGW32*|MSYS*|MINGW*) os=\u0026#34;Windows\u0026#34; ;; SunOS) os=\u0026#34;Solaris\u0026#34; ;; *) echo \u0026#34;Unsupported operating system\u0026#34; exit 1 ;; esac echo \u0026#34;Your OS is: ${os}\u0026#34; The ~/effective-shell/scripts/common.sh script checks to see whether the operating system is OSX and if so, temporarily aliases the text commands such as sed to their GNU equivalent, as the OSX versions of the commands are based on BSD so have slightly different parameters. You can use this script as an example of how to deal with OSX in shell scripts that are designed to be used on Linux as well as OSX.\nChecking for Installed Programs As we saw in Chapter 10 - Understanding Commands there are many different ways to determine whether a command is available. The most correct and portable way to test to see whether a command is available is to use the command -v command as shown below:\nif ! command -v \u0026#34;curl\u0026#34; \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then echo \u0026#34;\u0026#39;curl\u0026#39; is not installed, please install and try again\u0026#34; fi Note that when we're using the command command, we silence error output and standard output. This is required because otherwise we would see an error message written to the screen if the command doesn't exit or would see the details of the command if it does exist.\nThe ~/effective-shell/scripts/common.sh script checks to see whether certain GNU versions of tools are installed when running on OSX. You can refer to this script for an example of checking for the presence of commands.\nUsing \u0026lsquo;Select\u0026rsquo; to Show a Menu The select compound command prints a menu and allows the user to make a selection. It is not part of the Posix standard, but is available in Bash and most Bash-like shells.\nHere's how we could ask a user to select their favourite fruit from a list:\nselect fruit in Apple Banana Cherry Durian do echo \u0026#34;You chose: $fruit\u0026#34; echo \u0026#34;This is item number: $REPLY\u0026#34; done If we run these commands we will see output like the below:\n1) Apple 2) Banana 3) Cherry 4) Durian #? 1 You chose: Apple This is item number: 1 #? 3 You chose: Cherry This is item number: 3 #? 4 You chose: Durian This is item number: 4 #? ^D Notice that select will run just like an infinite loop - after the statements in the select body are run, the selection is offered again. The user can either end transmission with the ^D character or press ^C to quit.\nYou will normally see the select used with a case statement to process the selection. This is something you may come across in scripts so is useful to be aware of.\nRunning Commands in Subshells You will often see a nice little trick that allows you to change the current directory for a specific command, without affecting the current directory for the shell.\nHere's how this trick will look:\n(mkdir -p ~/new-project; cd ~/new-project; touch README.md) The brackets around the statements mean that these commands are run in a sub-shell. Because they run in a sub-shell, they change the directory in the sub-shell only, not the current shell. This means we don't need to change back to the previous directory after the commands have completed.\nThis sequence of commands would create a new folder (we use mkdir -p so that if the folder exists the command does not fail), then change to the folder, then create a new file called README.md.\nAnti-Patterns Anti-patterns are techniques that you may see but should be avoided. I have noted a few here as you will likely see them in your travels and should know why they are problematic.\nConfiguring Options in Shebangs You will sometimes see shebangs in shell scripts that contain options, like so:\n#!/usr/bin/bash -ex  # Script contents below... It is possible to specify the arguments to the program that is used to execute the script in the shebang. In the case above, the -ex flags are passed to the bash program, enabling the \u0026lsquo;exit on error\u0026rsquo; and \u0026lsquo;trace\u0026rsquo; options.\nI include this pattern because it is possible you will see it in other scripts, but please do not do this. There are a two particular reasons that it is risky.\nThe first is that pattern requires that you know the path to the shell. As we saw in Chapter 18 - Shell Script Essentials, we should use the #!/usr/bin/env program so that we search the $PATH for the shell rather than assuming that we know the location of the shell program.\nThe second reason is that multiple parameters are not handled consistently across operating systems. For example, on some Unix systems the following shebang will run bash with the -e parameter:\n#!/usr/bin/env bash -e However, on many Unix distributions only one parameter is passed. This would mean that the -e parameter would be silently ignored, which would be very confusing for the reader.\nSummary In this chapter we saw an assortment of common patterns that can be useful when building shell scripts. In the next part of the book we're going to look at how you can customise your shell and environment to build your own toolkit!\n  The value shown before each trace line can be configured by setting the $PS4 variable. \u0026#x21a9;\u0026#xfe0e;\n   "});index.add({'id':24,'href':'/docs/section6/reading-list/','title':"Reading List",'content':""});index.add({'id':25,'href':'/docs/part-1-transitioning-to-the-shell/clipboard-gymnastics/examples/formatting/','title':"Formatting",'content':" Formatting   Apples Oranges Pears Apples    "});index.add({'id':26,'href':'/docs/part-2-core-skills/what-is-a-shell/hack-on/','title':"Hack On",'content':"Hack On! See those system calls! I mentioned earlier on that if you make a call like fopen, the Kernel is going to provide access to a file. It's quite easy to see this in action. Check the code below:\n#include \u0026lt;stdio.h\u0026gt; void main() { void* handle = fopen(\u0026#34;/tmp/some-file\u0026#34;); fwrite(handle, \u0026#34;some text\u0026#34;); fclose(handle); } If you compile this program, then run XXX you will see the actual calls made to the Kernel. It can be very useful to use this technique to see what is going on with programs under the hood, particularly when diagnosing issues.\nHack On! Ahah! So that's TTY? Consider a command like:\ndocker -it Consider a command like:\nssh user@remote.com my-script no tty Once you understand the concept of shells and terminals in a bit more detail, more obscure messages like this start to make sense.\nIn the first instance, we are telling Docker we are interactive - i.e. we are going to use an interface to send commands. The second parameter, -t says use a TTY - which is short for teletype terminal, old fashioned lingo for the screen.\nTODO TTY picture\nTODO Summary; technical For technical readers, there's quite a lot of terms which get thrown about almost interchangeably; shell, command-line, terminal, tty, command-prompt, CLI and so on. But each of these have a very specific meaning. It's important to understand exactly what each of these terms really means, where they came from, and how these different types of system or concept relate to each other.\nSummary; non-tech A computer in a nutshell.\n \u0026ndash; Could link to other layers of abstractions - such as sandboxes in webpages? \u0026ndash; Could link to other layers of abstraction, such as containers?\n"});index.add({'id':27,'href':'/docs/part-5-building-your-toolkit/configuring-the-shell/','title':"Configuring the Shell",'content':"TODO interactive vs non interactive shells - running a shell script is a good example because it is non interactive (i.e. no aliases, no history, etc)\n Why we don't echo in config - imagine shell scripts like common echoing stuff out  Useful reading  https://learnbyexample.gitbooks.io/linux-command-line/content/Shell_Customization.html https://www.freecodecamp.org/news/bashrc-customization-guide/  "});index.add({'id':28,'href':'/docs/part-x-appendices/essential-manpages/','title':"Essential Manpages",'content':"The Most Important Manpages As an appendix, or printed reference, list of the top ten manpages?\n man re_pattern - basic and extended regex patterns man test is an excellent way to quickly check common tests (existence of a file etc) man set is super useful when checking options like set -ex in scripts man re_format man getopt man XXX show signal commands (Ctrl+V etc) man bash search for ARITHMETIC\\ EVALUATION to find how arithmetic operators work in bash man bash search for EXPANSION to see all shell expansion operators  "});index.add({'id':29,'href':'/docs/part-x-appendices/index-of-commands/','title':"Index of Commands",'content':"   Command Description     cd    ls    pwd    mkdir    rm    rmdir    cd    cd       ls\n  grep\n  mkdir\n  rm\n  rmdir\n  touch\n  cat\n  watch\n  tail\n  head\n  less\n  more\n  most\n  echo\n  timeout\n  until\n  pwd -P (physical, shows symlinks)\n  "});index.add({'id':30,'href':'/docs/part-x-appendices/shell-parameter-expansion/','title':"Shell Parameter Expansion",'content':"   Variable Description     $0 The first parameter to the shell, which is typically the path of the shell itself.   $1 The first parameter   $2 The second parameter   ${11} The 11th parameter - if the parameter is more than one digit you must surround it with braces   $# The number of parameters   $@ The full set of parameters as an array   $* The full set of parameters as a string separated by the first value in the $IFS variable   ${@:start:count} A subset of \u0026lsquo;count\u0026rsquo; parameters starting at parameter number \u0026lsquo;start\u0026rsquo;   $? The status code of the most recently called command.    TODO Need to update Chapter 19 to link to this.\n"});index.add({'id':31,'href':'/docs/part-x-linux-fundamentals/processes/','title':"Processes",'content':"Useful information:\nRun:\npstree -p $$ In a shell script, without a shebang. Now take a look and see at what runs the script. This actually varies from shell to shell, see:\nhttps://unix.stackexchange.com/questions/373223/which-shell-interpreter-runs-a-script-with-no-shebang\nFor details.\n"});index.add({'id':32,'href':'/docs/work-in-progress/_how-to-avoid-scripting/','title':"How to Avoid Scripting",'content':"Scripts we need to show how to avoid:\n# Save the current value of IFS - so we can restore it later. Split on newlines. old_ifs=$IFS IFS=$'\\n' # Find all symlinks and print each one. links=$(find ~ -type l) for link in $links do echo \u0026quot;Found Link: $link\u0026quot; done # Restore the original value of IFS. IFS=$old_ifs The while loop over file contents:\nwhile IFS=\u0026quot;\u0026quot; read -r line || [ -n \u0026quot;$line\u0026quot; ]; do echo \u0026quot;Read: $line\u0026quot; done \u0026lt; ~/effective-shell/data/top100.csv "});index.add({'id':33,'href':'/docs/work-in-progress/_posix/','title':"Posix",'content':"Posix Posix Shells Really good reading here:\nhttps://stackoverflow.com/questions/5725296/difference-between-sh-and-bash\nNote that on modern Distros you are often using dash:\n$ file -h /bin/sh /bin/sh: symbolic link to dash "});index.add({'id':34,'href':'/docs/part-5-building-your-toolkit/configuring-the-shell/configuring-the-shell/','title':"Configuring the Shell",'content':"todo: on startup, make a \u0026lsquo;today\u0026rsquo; folder?: pwd -P shows the physical path, great if you are in a symlink - maybe we use the \u0026lsquo;create temporary folder for today\u0026rsquo; as part of shell startup, then show that today is a link to the temp folder? todo: history control https://linux.101hacks.com/command-line-history/histcontrol-erasedups/ todo: https://linux.101hacks.com/bash-scripting/execution-sequence-of-bash-files/\n todo: using grep in an if statement (with the -q flag). example: my dotfiles project uses this trick to decide whether to add the profile file  "});index.add({'id':35,'href':'/docs/','title':"Docs",'content':""});index.add({'id':36,'href':'/','title':"Effective Shell",'content':"Effective Shell This book is for anyone who is interested in computing, and wants to learn more about the exciting, but sometimes daunting world of \u0026ldquo;The Shell\u0026rdquo;!\nFor the newcomer, you'll learn what a shell is, how to use it on your system, and then how to become more effective everyday by integrating the shell into your work.\nFor the experienced professional, there are chapters which go into advanced topics and introduce real-world ways to be more effective with your usage.\nTo get started, jump over to the Introduction page from the menu or the left, whichever chapter sounds the most appealing!\nTo setup your computer to follow along with the chapters, check the Getting Started guide.\nIf you would like to get email updates when new chapters are published, please do provide your email below. I won't be using it for anything beyond updates to the book.\n#mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; width:100%;} /* Add your own Mailchimp form style overrides in your site stylesheet or in this style block. We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */  Subscribe for Updates      This book is also open source! The code is on github.com/dwmkerr/effective-shell. Feel free to use the \u0026lsquo;Comments\u0026rsquo; section at the bottom of each page to discuss the content, or even open a pull request to propose changes.\n"});index.add({'id':37,'href':'/docs/introduction/','title':"Introduction",'content':"Introduction This book is for anyone who is interested in computing, and wants to learn more about the exciting, but sometimes daunting world of \u0026ldquo;The Shell\u0026rdquo;!\nThe shell is the basic interface for controlling a computer with just a keyboard. People use it for managing computers, writing software, doing data science, or even writing books like I am doing! Here's what the shell looks like:\nIf you are already thinking \u0026ldquo;no, this looks too geeky for me!\u0026rdquo; then think again, for almost anyone who uses a computer, or is interested in computing, this book will introduce what the shell is, how it works, and how to use it effectively.\nIf you are already an experienced user, this book also introduces many advanced topics, tips and tricks, and ways you can increase your productivity in the shell.\nThe goal of this book is to be a collection of small, contained tutorials on how to use the shell more effectively, you can go through it sequentially or just pick and choose the sections which seem the most interesting.\nIf you are already comfortable with running a shell, know what bash is, and know how to run basic commands like ls and cd, are familiar with terms like command and parameter then you can skip through some of the earlier sections and pick up the chapters which sound most interesting to you.\nAll of the content of this book is designed to be suitable to work with Microsoft Windows, Mac OS and Linux. So no matter what system you are running, you should be able to follow along. We will focus primarily on \u0026lsquo;Linux Like\u0026rsquo; environments, which hopefully will give you the skills which you can apply most widely. For Windows, we'll look into how to tweak your system to be able to run all of the samples.\n"});index.add({'id':38,'href':'/docs/part-1-transitioning-to-the-shell/','title':"Part 1 - Transitioning to the Shell",'content':"Part 1 - Transitioning to the Shell These are the key skills which everyone should know. Without them, you might struggle to perform certain tasks at all. Experienced users can probably skip this section, or just review the summary. But if you are new to the shell, this is the best place to start! This section focuses on helping you quickly get up to speed with how to perform the same kind of tasks you might have performed in a GUI (Graphical User Interface) with the shell.\n"});index.add({'id':39,'href':'/docs/part-2-core-skills/','title':"Part 2 - Core Skill",'content':"Part 2 - Core Skill In the first part of this book we look at the shell from the perspective of someone who is familiar with a graphical user interface. We studied how to transition from a GUI to the shell, introducing the \u0026lsquo;shell way\u0026rsquo; of performing tasks which you might have previously performed using a GUI.\nIn this section, we'll look at core shell skills. These skills are fundamental to how the shell works, and fundamental to using it effectively. Even if you are familiar with the concepts in each chapter, I would still recommend skimming these sections to make sure there is nothing you have missed.\nA solid understanding of these core skills will be useful for later sections. So even though this book is designed to be something you can dip and dip out of, in any order you choose, it may be worth focusing on this section before moving to later sections.\n"});index.add({'id':40,'href':'/docs/part-3-manipulating-text/','title':"Part 2 - Manipulating Text and Streams",'content':"Part 3 - Manipulating Text and Streams A key part of how Linux and Unix systems work is that almost everything is represented as a text file in the system. Almost everything can be configured with simple text file.\nThis means that you may find yourself regularly manipulating text, searching through text and working with text files. There are a lot of options for how to do this! In fact, there are so many options that it can be a bit overwhelming to know what is the right tool for the job.\nIn this section we'll look at some of the key techniques which can be used to work with text, and demonstrate this with practical examples. In each chapter I'll try to show lots of real world use cases to keep things as applicable to usual tasks as possible.\nHopefully, by the time you have completed this section, you will have a great understanding of the tools available to you and how to apply them to the task at hand.\n"});index.add({'id':41,'href':'/docs/part-4-shell-scripting/','title':"Part 4 - Shell Scripting",'content':"Part 4 - Shell Scripting Now that we've looked at the core skills will help you be more effective, as well as the fundamentals of managing text, it is time to look at shell scripting.\nShell scripting is the process of writing re-usable scripts, which you can use to automate repetitive work, create new programs and manage your environment.\nIn this section we'll look at the fundamentals of how shell scripts work and how to write and structure shell scripts. We'll also look at some more advanced techniques to deal with more complex scenarios, tricks for debugging shell scripts and more.\n"});index.add({'id':42,'href':'/docs/work-in-progress/','title':"Work in Progress!",'content':"Work in Progress! If you have landed here, then most likely you have clicked a link to a chapter which has not yet been completed. Sorry about that!\nI'm trying to get a few chapters published each week, check back regularly to see if the section you are looking for is completed.\nYou can also sign up with the form below to be notified when I publish new chapters (I don't use this for anything beyond notifications of updates to the book and don't share any details).\n#mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; width:100%;} /* Add your own Mailchimp form style overrides in your site stylesheet or in this style block. We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */  Subscribe for Updates      If you have comments on content you'd like to see, feel free to use the comments section below.\nThis book is also open source! That means you can also raise issues for suggestions, or even propose changes to the content yourself. The code is on github.com/dwmkerr/effective-shell. Feel free to use the \u0026lsquo;Comments\u0026rsquo; section at the bottom of each page to discuss the content, or even open a pull request to propose changes.\n"});})();